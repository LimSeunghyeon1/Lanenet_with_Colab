{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lanenet_for_test",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LimSeunghyeon1/Lanenet_with_Colab/blob/master/Lanenet_for_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YythAK3OeWd",
        "outputId": "8edd3891-47d9-4eb4-db57-026e05bdcd54"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js9CD3nMrxij"
      },
      "source": [
        "#! unzip -d /content/drive/Shareddrives/colab/ /content/drive/MyDrive/data_splits.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp3dn1CctRzs"
      },
      "source": [
        "#! cd /content/drive/Shareddrives/colab/data_splits/illus_chg/Gen_LaneNet_ext/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdXOEXYJugPN"
      },
      "source": [
        "#! tar -xvf /content/drive/Shareddrives/colab/data_splits/illus_chg/Gen_LaneNet_ext/model_best_epoch_29.pth.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3NQ6wBRP2Bk"
      },
      "source": [
        "! #unzip -d /content/drive/Shareddrives/colab/ /content/drive/MyDrive/Apollo_Sim_3D_Lane_Release.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_VXWTs-eeyb"
      },
      "source": [
        "\"\"\"\n",
        "Utility functions and default settings\n",
        "Author: Yuliang Guo (33yuliangguo@gmail.com)\n",
        "Date: March, 2020\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import errno\n",
        "import os\n",
        "import sys\n",
        "import easydict\n",
        "\n",
        "import cv2\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.init as init\n",
        "import torch.optim\n",
        "from torch.optim import lr_scheduler\n",
        "import os.path as ops\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import interp1d\n",
        "plt.rcParams['figure.figsize'] = (35, 30)\n",
        "\n",
        "\n",
        "def define_args():\n",
        "    parser=easydict.EasyDict({\n",
        "       \"dataset_name\":str(),\n",
        "       \"data_dir\":str(),\n",
        "       \"dataset_dir\":str(),\n",
        "       \"save_path\":'/content/drive/Shareddrives/colab/data_splits',\n",
        "       \"org_h\":1080,\n",
        "       \"org_w\":1920,\n",
        "       \"crop_y\":0,\n",
        "       \"cam_height\":1.55,\n",
        "       \"pitch\":float(3),\n",
        "       \"fix_cam\":False,\n",
        "       \"no_3d\": False,\n",
        "       \"no_centerline\":False,\n",
        "       \"mod\":'3DLaneNet',\n",
        "       \"pretrained\":True,\n",
        "       \"batch_norm\":True,\n",
        "       \"pred_cam\":False,\n",
        "       \"ipm_h\":208,\n",
        "       \"ipm_w\":128,\n",
        "       \"resize_h\":360,\n",
        "       \"resize_w\":480,\n",
        "       \"y_ref\":20.0,\n",
        "       \"prob_th\":0.5,\n",
        "       \"batch_size\":8,\n",
        "       \"nepochs\":30,\n",
        "       \"learning_rate\":5*1e-4,\n",
        "       \"no_cuda\":False,\n",
        "       \"nworkers\":0,\n",
        "       \"no_dropout\":False,\n",
        "       \"pretrain_epochs\":20,\n",
        "       \"channels_in\":3,\n",
        "       \"flip_on\":False,\n",
        "       \"test_mode\":False,\n",
        "       \"start_epoch\":0,\n",
        "       \"evaluate\":True,  ##only show evaluation?\n",
        "       \"resume\":str(),\n",
        "       \"vgg_mean\":[0.485,0.456,0.406],\n",
        "       \"vgg_std\":[0.229,0.224,0.225],\n",
        "       \"optimizer\":'adam',\n",
        "       \"weight_init\":\"normal\",\n",
        "       \"weight_decay\":float(0),\n",
        "       \"lr_decay\":False,\n",
        "       \"niter\":50,\n",
        "       \"niter_decay\":400,\n",
        "       \"lr_policy\":None,\n",
        "       \"lr_decay_iters\":30,\n",
        "       \"clip_grad_norm\":0,\n",
        "       \"cudnn\":True,\n",
        "       \"no_tb\":False,\n",
        "       \"print_freq\":500,\n",
        "       \"save_freq\":500,\n",
        "       \"list\":[954,2789]\n",
        "    })\n",
        "    '''\n",
        "    parser = argparse.ArgumentParser(description='Lane_detection_all_objectives')\n",
        "    # Paths settings\n",
        "    parser.add_argument('--dataset_name', type=str, help='the dataset name to be used in saving model names')\n",
        "    parser.add_argument('--data_dir', type=str, help='The path saving train.json and val.json files')\n",
        "    parser.add_argument('--dataset_dir', type=str, help='The path saving actual data')\n",
        "    parser.add_argument('--save_path', type=str, default='data_splits/', help='directory to save output')\n",
        "    # Dataset settings\n",
        "    parser.add_argument('--org_h', type=int, default=1080, help='height of the original image')\n",
        "    parser.add_argument('--org_w', type=int, default=1920, help='width of the original image')\n",
        "    parser.add_argument('--crop_y', type=int, default=0, help='crop from image')\n",
        "    parser.add_argument('--cam_height', type=float, default=1.55, help='height of camera in meters')\n",
        "    parser.add_argument('--pitch', type=float, default=3, help='pitch angle of camera to ground in centi degree')\n",
        "    parser.add_argument('--fix_cam', type=str2bool, nargs='?', const=True, default=False, help='if to use fix camera')\n",
        "    parser.add_argument('--no_3d', action='store_true', help='if a dataset include laneline 3D attributes')\n",
        "    parser.add_argument('--no_centerline', action='store_true', help='if a dataset include centerline')\n",
        "    # 3DLaneNet settings\n",
        "    parser.add_argument('--mod', type=str, default='3DLaneNet', help='model to train')\n",
        "    parser.add_argument(\"--pretrained\", type=str2bool, nargs='?', const=True, default=True, help=\"use pretrained vgg model\")\n",
        "    parser.add_argument(\"--batch_norm\", type=str2bool, nargs='?', const=True, default=True, help=\"apply batch norm\")\n",
        "    parser.add_argument(\"--pred_cam\", type=str2bool, nargs='?', const=True, default=False, help=\"use network to predict camera online?\")\n",
        "    parser.add_argument('--ipm_h', type=int, default=208, help='height of inverse projective map (IPM)')\n",
        "    parser.add_argument('--ipm_w', type=int, default=128, help='width of inverse projective map (IPM)')\n",
        "    parser.add_argument('--resize_h', type=int, default=360, help='height of the original image')\n",
        "    parser.add_argument('--resize_w', type=int, default=480, help='width of the original image')\n",
        "    parser.add_argument('--y_ref', type=float, default=20.0, help='the reference Y distance in meters from where lane association is determined')\n",
        "    parser.add_argument('--prob_th', type=float, default=0.5, help='probability threshold for selecting output lanes')\n",
        "    # General model settings\n",
        "    parser.add_argument('--batch_size', type=int, default=8, help='batch size')\n",
        "    parser.add_argument('--nepochs', type=int, default=30, help='total numbers of epochs')\n",
        "    parser.add_argument('--learning_rate', type=float, default=5*1e-4, help='learning rate')\n",
        "    parser.add_argument('--no_cuda', action='store_true', help='if gpu available')\n",
        "    parser.add_argument('--nworkers', type=int, default=0, help='num of threads')\n",
        "    parser.add_argument('--no_dropout', action='store_true', help='no dropout in network')\n",
        "    parser.add_argument('--pretrain_epochs', type=int, default=20, help='Number of epochs to perform segmentation pretraining')\n",
        "    parser.add_argument('--channels_in', type=int, default=3, help='num channels of input image')\n",
        "    parser.add_argument('--flip_on', action='store_true', help='Random flip input images on?')\n",
        "    parser.add_argument('--test_mode', action='store_true', help='prevents loading latest saved model')\n",
        "    parser.add_argument('--start_epoch', type=int, default=0, help='prevents loading latest saved model')\n",
        "    parser.add_argument('--evaluate', action='store_true', help='only perform evaluation')\n",
        "    parser.add_argument('--resume', type=str, default='', help='resume latest saved run')\n",
        "    parser.add_argument('--vgg_mean', type=float, default=[0.485, 0.456, 0.406], help='Mean of rgb used in pretrained model on ImageNet')\n",
        "    parser.add_argument('--vgg_std', type=float, default=[0.229, 0.224, 0.225], help='Std of rgb used in pretrained model on ImageNet')\n",
        "    # Optimizer settings\n",
        "    parser.add_argument('--optimizer', type=str, default='adam', help='adam or sgd')\n",
        "    parser.add_argument('--weight_init', type=str, default='normal', help='normal, xavier, kaiming, orhtogonal weights initialisation')\n",
        "    parser.add_argument('--weight_decay', type=float, default=0, help='L2 weight decay/regularisation on?')\n",
        "    parser.add_argument('--lr_decay', action='store_true', help='decay learning rate with rule')\n",
        "    parser.add_argument('--niter', type=int, default=50, help='# of iter at starting learning rate')\n",
        "    parser.add_argument('--niter_decay', type=int, default=400, help='# of iter to linearly decay learning rate to zero')\n",
        "    parser.add_argument('--lr_policy', default=None, help='learning rate policy: lambda|step|plateau')\n",
        "    parser.add_argument('--lr_decay_iters', type=int, default=30, help='multiply by a gamma every lr_decay_iters iterations')\n",
        "    parser.add_argument('--clip_grad_norm', type=int, default=0, help='performs gradient clipping')\n",
        "    # CUDNN usage\n",
        "    parser.add_argument(\"--cudnn\", type=str2bool, nargs='?', const=True, default=True, help=\"cudnn optimization active\")\n",
        "    # Tensorboard settings\n",
        "    parser.add_argument(\"--no_tb\", type=str2bool, nargs='?', const=True, default=False, help=\"Use tensorboard logging by tensorflow\")\n",
        "    # Print settings\n",
        "    parser.add_argument('--print_freq', type=int, default=500, help='padding')\n",
        "    parser.add_argument('--save_freq', type=int, default=500, help='padding')\n",
        "    # Skip batch\n",
        "    parser.add_argument('--list', type=int, nargs='+', default=[954, 2789], help='Images you want to skip')\n",
        "'''\n",
        "    return parser\n",
        "\n",
        "\n",
        "def tusimple_config(args):\n",
        "\n",
        "    # set dataset parameters\n",
        "    args.org_h = 720\n",
        "    args.org_w = 1280\n",
        "    args.crop_y = 80\n",
        "    args.no_centerline = True\n",
        "    args.no_3d = True\n",
        "    args.fix_cam = True\n",
        "    args.pred_cam = False\n",
        "\n",
        "    # set camera parameters for the test dataset\n",
        "    args.K = np.array([[1000, 0, 640],\n",
        "                       [0, 1000, 400],\n",
        "                       [0, 0, 1]])\n",
        "    args.cam_height = 1.6\n",
        "    args.pitch = 9\n",
        "\n",
        "    # specify model settings\n",
        "    \"\"\"\n",
        "    paper presented params:\n",
        "        args.top_view_region = np.array([[-10, 85], [10, 85], [-10, 5], [10, 5]])\n",
        "        args.anchor_y_steps = np.array([5, 20, 40, 60, 80, 100])\n",
        "    \"\"\"\n",
        "    # args.top_view_region = np.array([[-10, 82], [10, 82], [-10, 2], [10, 2]])\n",
        "    # args.anchor_y_steps = np.array([2, 3, 5, 10, 15, 20, 30, 40, 60, 80])\n",
        "    args.top_view_region = np.array([[-10, 103], [10, 103], [-10, 3], [10, 3]])\n",
        "    args.anchor_y_steps = np.array([5, 10, 15, 20, 30, 40, 50, 60, 80, 100])\n",
        "    args.num_y_steps = len(args.anchor_y_steps)\n",
        "\n",
        "    # initialize with pre-trained vgg weights\n",
        "    args.pretrained = False\n",
        "    # apply batch norm in network\n",
        "    args.batch_norm = True\n",
        "\n",
        "\n",
        "def sim3d_config(args):\n",
        "\n",
        "    # set dataset parameters\n",
        "    args.org_h = 1080\n",
        "    args.org_w = 1920\n",
        "    args.crop_y = 0\n",
        "    args.no_centerline = False\n",
        "    args.no_3d = False\n",
        "    args.fix_cam = False\n",
        "    args.pred_cam = False\n",
        "\n",
        "    # set camera parameters for the test datasets\n",
        "    args.K = np.array([[2015., 0., 960.],\n",
        "                       [0., 2015., 540.],\n",
        "                       [0., 0., 1.]])\n",
        "\n",
        "    # specify model settings\n",
        "    \"\"\"\n",
        "    paper presented params:\n",
        "        args.top_view_region = np.array([[-10, 85], [10, 85], [-10, 5], [10, 5]])\n",
        "        args.anchor_y_steps = np.array([5, 20, 40, 60, 80, 100])\n",
        "    \"\"\"\n",
        "    # args.top_view_region = np.array([[-10, 83], [10, 83], [-10, 3], [10, 3]])\n",
        "    # args.anchor_y_steps = np.array([3, 5, 10, 20, 40, 60, 80, 100])\n",
        "    args.top_view_region = np.array([[-10, 103], [10, 103], [-10, 3], [10, 3]])\n",
        "    args.anchor_y_steps = np.array([5, 10, 15, 20, 30, 40, 50, 60, 80, 100])\n",
        "    args.num_y_steps = len(args.anchor_y_steps)\n",
        "\n",
        "    # initialize with pre-trained vgg weights\n",
        "    args.pretrained = False\n",
        "    # apply batch norm in network\n",
        "    args.batch_norm = True\n",
        "\n",
        "\n",
        "class Visualizer:\n",
        "    def __init__(self, args, vis_folder='val_vis'):\n",
        "        self.save_path = args.save_path\n",
        "        self.vis_folder = vis_folder\n",
        "        self.no_3d = args.no_3d\n",
        "        self.no_centerline = args.no_centerline\n",
        "        self.vgg_mean = args.vgg_mean\n",
        "        self.vgg_std = args.vgg_std\n",
        "        self.ipm_w = args.ipm_w\n",
        "        self.ipm_h = args.ipm_h\n",
        "        self.num_y_steps = args.num_y_steps\n",
        "\n",
        "        if args.no_3d:\n",
        "            self.anchor_dim = args.num_y_steps + 1\n",
        "        else:\n",
        "            if 'ext' in args.mod:\n",
        "                self.anchor_dim = 3 * args.num_y_steps + 1\n",
        "            else:\n",
        "                self.anchor_dim = 2 * args.num_y_steps + 1\n",
        "\n",
        "        x_min = args.top_view_region[0, 0]\n",
        "        x_max = args.top_view_region[1, 0]\n",
        "        self.anchor_x_steps = np.linspace(x_min, x_max, np.int(args.ipm_w / 8), endpoint=True)\n",
        "        self.anchor_y_steps = args.anchor_y_steps\n",
        "\n",
        "        # transformation from ipm to ground region\n",
        "        H_ipm2g = cv2.getPerspectiveTransform(np.float32([[0, 0],\n",
        "                                                          [self.ipm_w-1, 0],\n",
        "                                                          [0, self.ipm_h-1],\n",
        "                                                          [self.ipm_w-1, self.ipm_h-1]]),\n",
        "                                              np.float32(args.top_view_region))\n",
        "        self.H_g2ipm = np.linalg.inv(H_ipm2g)\n",
        "\n",
        "        # probability threshold for choosing visualize lanes\n",
        "        self.prob_th = args.prob_th\n",
        "\n",
        "    def draw_on_img(self, img, lane_anchor, P_g2im, draw_type='laneline', color=[0, 0, 1]):\n",
        "        \"\"\"\n",
        "        :param img: image in numpy array, each pixel in [0, 1] range\n",
        "        :param lane_anchor: lane anchor in N X C numpy ndarray, dimension in agree with dataloader\n",
        "        :param P_g2im: projection from ground 3D coordinates to image 2D coordinates\n",
        "        :param draw_type: 'laneline' or 'centerline' deciding which to draw\n",
        "        :param color: [r, g, b] color for line,  each range in [0, 1]\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        for j in range(lane_anchor.shape[0]):\n",
        "            # draw laneline\n",
        "            if draw_type is 'laneline' and lane_anchor[j, self.anchor_dim - 1] > self.prob_th:\n",
        "                x_offsets = lane_anchor[j, :self.num_y_steps]\n",
        "                x_3d = x_offsets + self.anchor_x_steps[j]\n",
        "                if P_g2im.shape[1] is 3:\n",
        "                    x_2d, y_2d = homographic_transformation(P_g2im, x_3d, self.anchor_y_steps)\n",
        "                else:\n",
        "                    z_3d = lane_anchor[j, self.num_y_steps:self.anchor_dim - 1]\n",
        "                    x_2d, y_2d = projective_transformation(P_g2im, x_3d, self.anchor_y_steps, z_3d)\n",
        "                x_2d = x_2d.astype(np.int)\n",
        "                y_2d = y_2d.astype(np.int)\n",
        "                for k in range(1, x_2d.shape[0]):\n",
        "                    img = cv2.line(img, (x_2d[k - 1], y_2d[k - 1]), (x_2d[k], y_2d[k]), color, 2)\n",
        "\n",
        "            # draw centerline\n",
        "            if draw_type is 'centerline' and lane_anchor[j, 2 * self.anchor_dim - 1] > self.prob_th:\n",
        "                x_offsets = lane_anchor[j, self.anchor_dim:self.anchor_dim + self.num_y_steps]\n",
        "                x_3d = x_offsets + self.anchor_x_steps[j]\n",
        "                if P_g2im.shape[1] is 3:\n",
        "                    x_2d, y_2d = homographic_transformation(P_g2im, x_3d, self.anchor_y_steps)\n",
        "                else:\n",
        "                    z_3d = lane_anchor[j, self.anchor_dim + self.num_y_steps:2 * self.anchor_dim - 1]\n",
        "                    x_2d, y_2d = projective_transformation(P_g2im, x_3d, self.anchor_y_steps, z_3d)\n",
        "                x_2d = x_2d.astype(np.int)\n",
        "                y_2d = y_2d.astype(np.int)\n",
        "                for k in range(1, x_2d.shape[0]):\n",
        "                    img = cv2.line(img, (x_2d[k - 1], y_2d[k - 1]), (x_2d[k], y_2d[k]), color, 2)\n",
        "\n",
        "            # draw the additional centerline for the merging case\n",
        "            if draw_type is 'centerline' and lane_anchor[j, 3 * self.anchor_dim - 1] > self.prob_th:\n",
        "                x_offsets = lane_anchor[j, 2 * self.anchor_dim:2 * self.anchor_dim + self.num_y_steps]\n",
        "                x_3d = x_offsets + self.anchor_x_steps[j]\n",
        "                if P_g2im.shape[1] is 3:\n",
        "                    x_2d, y_2d = homographic_transformation(P_g2im, x_3d, self.anchor_y_steps)\n",
        "                else:\n",
        "                    z_3d = lane_anchor[j, 2 * self.anchor_dim + self.num_y_steps:3 * self.anchor_dim - 1]\n",
        "                    x_2d, y_2d = projective_transformation(P_g2im, x_3d, self.anchor_y_steps, z_3d)\n",
        "                x_2d = x_2d.astype(np.int)\n",
        "                y_2d = y_2d.astype(np.int)\n",
        "                for k in range(1, x_2d.shape[0]):\n",
        "                    img = cv2.line(img, (x_2d[k - 1], y_2d[k - 1]), (x_2d[k], y_2d[k]), color, 2)\n",
        "        return img\n",
        "\n",
        "    def draw_on_img_new(self, img, lane_anchor, P_g2im, draw_type='laneline', color=[0, 0, 1]):\n",
        "        \"\"\"\n",
        "        :param img: image in numpy array, each pixel in [0, 1] range\n",
        "        :param lane_anchor: lane anchor in N X C numpy ndarray, dimension in agree with dataloader\n",
        "        :param P_g2im: projection from ground 3D coordinates to image 2D coordinates\n",
        "        :param draw_type: 'laneline' or 'centerline' deciding which to draw\n",
        "        :param color: [r, g, b] color for line,  each range in [0, 1]\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        for j in range(lane_anchor.shape[0]):\n",
        "            # draw laneline\n",
        "            if draw_type is 'laneline' and lane_anchor[j, self.anchor_dim - 1] > self.prob_th:\n",
        "                x_offsets = lane_anchor[j, :self.num_y_steps]\n",
        "                x_3d = x_offsets + self.anchor_x_steps[j]\n",
        "                if P_g2im.shape[1] is 3:\n",
        "                    x_2d, y_2d = homographic_transformation(P_g2im, x_3d, self.anchor_y_steps)\n",
        "                    visibility = np.ones_like(x_2d)\n",
        "                else:\n",
        "                    z_3d = lane_anchor[j, self.num_y_steps:2*self.num_y_steps]\n",
        "                    x_2d, y_2d = projective_transformation(P_g2im, x_3d, self.anchor_y_steps, z_3d)\n",
        "                    visibility = lane_anchor[j, 2 * self.num_y_steps:3 * self.num_y_steps]\n",
        "                x_2d = x_2d.astype(np.int)\n",
        "                y_2d = y_2d.astype(np.int)\n",
        "                for k in range(1, x_2d.shape[0]):\n",
        "                    if visibility[k] > self.prob_th:\n",
        "                        img = cv2.line(img, (x_2d[k - 1], y_2d[k - 1]), (x_2d[k], y_2d[k]), color, 2)\n",
        "                    else:\n",
        "                        img = cv2.line(img, (x_2d[k - 1], y_2d[k - 1]), (x_2d[k], y_2d[k]), [0, 0, 0], 2)\n",
        "\n",
        "            # draw centerline\n",
        "            if draw_type is 'centerline' and lane_anchor[j, 2 * self.anchor_dim - 1] > self.prob_th:\n",
        "                x_offsets = lane_anchor[j, self.anchor_dim:self.anchor_dim + self.num_y_steps]\n",
        "                x_3d = x_offsets + self.anchor_x_steps[j]\n",
        "                if P_g2im.shape[1] is 3:\n",
        "                    x_2d, y_2d = homographic_transformation(P_g2im, x_3d, self.anchor_y_steps)\n",
        "                    visibility = np.ones_like(x_2d)\n",
        "                else:\n",
        "                    z_3d = lane_anchor[j, self.anchor_dim + self.num_y_steps:self.anchor_dim + 2*self.num_y_steps]\n",
        "                    x_2d, y_2d = projective_transformation(P_g2im, x_3d, self.anchor_y_steps, z_3d)\n",
        "                    visibility = lane_anchor[j, self.anchor_dim + 2*self.num_y_steps:self.anchor_dim + 3*self.num_y_steps]\n",
        "                x_2d = x_2d.astype(np.int)\n",
        "                y_2d = y_2d.astype(np.int)\n",
        "                for k in range(1, x_2d.shape[0]):\n",
        "                    if visibility[k] > self.prob_th:\n",
        "                        img = cv2.line(img, (x_2d[k - 1], y_2d[k - 1]), (x_2d[k], y_2d[k]), color, 2)\n",
        "                    else:\n",
        "                        img = cv2.line(img, (x_2d[k - 1], y_2d[k - 1]), (x_2d[k], y_2d[k]), [0, 0, 0], 2)\n",
        "\n",
        "            # draw the additional centerline for the merging case\n",
        "            if draw_type is 'centerline' and lane_anchor[j, 3*self.anchor_dim - 1] > self.prob_th:\n",
        "                x_offsets = lane_anchor[j, 2*self.anchor_dim:2*self.anchor_dim + self.num_y_steps]\n",
        "                x_3d = x_offsets + self.anchor_x_steps[j]\n",
        "                if P_g2im.shape[1] is 3:\n",
        "                    x_2d, y_2d = homographic_transformation(P_g2im, x_3d, self.anchor_y_steps)\n",
        "                    visibility = np.ones_like(x_2d)\n",
        "                else:\n",
        "                    z_3d = lane_anchor[j, 2*self.anchor_dim + self.num_y_steps:2*self.anchor_dim + 2*self.num_y_steps]\n",
        "                    x_2d, y_2d = projective_transformation(P_g2im, x_3d, self.anchor_y_steps, z_3d)\n",
        "                    visibility = lane_anchor[j,\n",
        "                                 2 * self.anchor_dim + 2 * self.num_y_steps:2 * self.anchor_dim + 3 * self.num_y_steps]\n",
        "                x_2d = x_2d.astype(np.int)\n",
        "                y_2d = y_2d.astype(np.int)\n",
        "                for k in range(1, x_2d.shape[0]):\n",
        "                    if visibility[k] > self.prob_th:\n",
        "                        img = cv2.line(img, (x_2d[k - 1], y_2d[k - 1]), (x_2d[k], y_2d[k]), color, 2)\n",
        "                    else:\n",
        "                        img = cv2.line(img, (x_2d[k - 1], y_2d[k - 1]), (x_2d[k], y_2d[k]), [0, 0, 0], 2)\n",
        "        return img\n",
        "\n",
        "    def draw_on_ipm(self, im_ipm, lane_anchor, draw_type='laneline', color=[0, 0, 1]):\n",
        "        for j in range(lane_anchor.shape[0]):\n",
        "            # draw laneline\n",
        "            if draw_type is 'laneline' and lane_anchor[j, self.anchor_dim - 1] > self.prob_th:\n",
        "                x_offsets = lane_anchor[j, :self.num_y_steps]\n",
        "                x_g = x_offsets + self.anchor_x_steps[j]\n",
        "\n",
        "                # compute lanelines in ipm view\n",
        "                x_ipm, y_ipm = homographic_transformation(self.H_g2ipm, x_g, self.anchor_y_steps)\n",
        "                x_ipm = x_ipm.astype(np.int)\n",
        "                y_ipm = y_ipm.astype(np.int)\n",
        "                for k in range(1, x_g.shape[0]):\n",
        "                    im_ipm = cv2.line(im_ipm, (x_ipm[k - 1], y_ipm[k - 1]),\n",
        "                                      (x_ipm[k], y_ipm[k]), color, 1)\n",
        "\n",
        "            # draw centerline\n",
        "            if draw_type is 'centerline' and lane_anchor[j, 2 * self.anchor_dim - 1] > self.prob_th:\n",
        "                x_offsets = lane_anchor[j, self.anchor_dim:self.anchor_dim + self.num_y_steps]\n",
        "                x_g = x_offsets + self.anchor_x_steps[j]\n",
        "\n",
        "                # compute lanelines in ipm view\n",
        "                x_ipm, y_ipm = homographic_transformation(self.H_g2ipm, x_g, self.anchor_y_steps)\n",
        "                x_ipm = x_ipm.astype(np.int)\n",
        "                y_ipm = y_ipm.astype(np.int)\n",
        "                for k in range(1, x_g.shape[0]):\n",
        "                    im_ipm = cv2.line(im_ipm, (x_ipm[k - 1], y_ipm[k - 1]),\n",
        "                                      (x_ipm[k], y_ipm[k]), color, 1)\n",
        "\n",
        "            # draw the additional centerline for the merging case\n",
        "            if draw_type is 'centerline' and lane_anchor[j, 3 * self.anchor_dim - 1] > self.prob_th:\n",
        "                x_offsets = lane_anchor[j, 2 * self.anchor_dim:2 * self.anchor_dim + self.num_y_steps]\n",
        "                x_g = x_offsets + self.anchor_x_steps[j]\n",
        "\n",
        "                # compute lanelines in ipm view\n",
        "                x_ipm, y_ipm = homographic_transformation(self.H_g2ipm, x_g, self.anchor_y_steps)\n",
        "                x_ipm = x_ipm.astype(np.int)\n",
        "                y_ipm = y_ipm.astype(np.int)\n",
        "                for k in range(1, x_g.shape[0]):\n",
        "                    im_ipm = cv2.line(im_ipm, (x_ipm[k - 1], y_ipm[k - 1]),\n",
        "                                      (x_ipm[k], y_ipm[k]), color, 1)\n",
        "        return im_ipm\n",
        "\n",
        "    def draw_on_ipm_new(self, im_ipm, lane_anchor, draw_type='laneline', color=[0, 0, 1], width=1):\n",
        "        for j in range(lane_anchor.shape[0]):\n",
        "            # draw laneline\n",
        "            if draw_type is 'laneline' and lane_anchor[j, self.anchor_dim - 1] > self.prob_th:\n",
        "                x_offsets = lane_anchor[j, :self.num_y_steps]\n",
        "                x_g = x_offsets + self.anchor_x_steps[j]\n",
        "                if self.no_3d:\n",
        "                    visibility = np.ones_like(x_g)\n",
        "                else:\n",
        "                    visibility = lane_anchor[j, 2*self.num_y_steps:3*self.num_y_steps]\n",
        "\n",
        "                # compute lanelines in ipm view\n",
        "                x_ipm, y_ipm = homographic_transformation(self.H_g2ipm, x_g, self.anchor_y_steps)\n",
        "                x_ipm = x_ipm.astype(np.int)\n",
        "                y_ipm = y_ipm.astype(np.int)\n",
        "                for k in range(1, x_g.shape[0]):\n",
        "                    if visibility[k] > self.prob_th:\n",
        "                        im_ipm = cv2.line(im_ipm, (x_ipm[k - 1], y_ipm[k - 1]),\n",
        "                                          (x_ipm[k], y_ipm[k]), color, width)\n",
        "                    else:\n",
        "                        im_ipm = cv2.line(im_ipm, (x_ipm[k - 1], y_ipm[k - 1]),\n",
        "                                          (x_ipm[k], y_ipm[k]), [0, 0, 0], width)\n",
        "\n",
        "            # draw centerline\n",
        "            if draw_type is 'centerline' and lane_anchor[j, 2*self.anchor_dim - 1] > self.prob_th:\n",
        "                x_offsets = lane_anchor[j, self.anchor_dim:self.anchor_dim + self.num_y_steps]\n",
        "                x_g = x_offsets + self.anchor_x_steps[j]\n",
        "                if self.no_3d:\n",
        "                    visibility = np.ones_like(x_g)\n",
        "                else:\n",
        "                    visibility = lane_anchor[j, self.anchor_dim + 2*self.num_y_steps:self.anchor_dim + 3*self.num_y_steps]\n",
        "\n",
        "                # compute lanelines in ipm view\n",
        "                x_ipm, y_ipm = homographic_transformation(self.H_g2ipm, x_g, self.anchor_y_steps)\n",
        "                x_ipm = x_ipm.astype(np.int)\n",
        "                y_ipm = y_ipm.astype(np.int)\n",
        "                for k in range(1, x_g.shape[0]):\n",
        "                    if visibility[k] > self.prob_th:\n",
        "                        im_ipm = cv2.line(im_ipm, (x_ipm[k - 1], y_ipm[k - 1]),\n",
        "                                          (x_ipm[k], y_ipm[k]), color, width)\n",
        "                    else:\n",
        "                        im_ipm = cv2.line(im_ipm, (x_ipm[k - 1], y_ipm[k - 1]),\n",
        "                                          (x_ipm[k], y_ipm[k]), [0, 0, 0], width)\n",
        "\n",
        "            # draw the additional centerline for the merging case\n",
        "            if draw_type is 'centerline' and lane_anchor[j, 3*self.anchor_dim - 1] > self.prob_th:\n",
        "                x_offsets = lane_anchor[j, 2*self.anchor_dim:2*self.anchor_dim + self.num_y_steps]\n",
        "                x_g = x_offsets + self.anchor_x_steps[j]\n",
        "                if self.no_3d:\n",
        "                    visibility = np.ones_like(x_g)\n",
        "                else:\n",
        "                    visibility = lane_anchor[j, 2*self.anchor_dim + 2*self.num_y_steps:2*self.anchor_dim + 3*self.num_y_steps]\n",
        "\n",
        "                # compute lanelines in ipm view\n",
        "                x_ipm, y_ipm = homographic_transformation(self.H_g2ipm, x_g, self.anchor_y_steps)\n",
        "                x_ipm = x_ipm.astype(np.int)\n",
        "                y_ipm = y_ipm.astype(np.int)\n",
        "                for k in range(1, x_g.shape[0]):\n",
        "                    if visibility[k] > self.prob_th:\n",
        "                        im_ipm = cv2.line(im_ipm, (x_ipm[k - 1], y_ipm[k - 1]),\n",
        "                                          (x_ipm[k], y_ipm[k]), color, width)\n",
        "                    else:\n",
        "                        im_ipm = cv2.line(im_ipm, (x_ipm[k - 1], y_ipm[k - 1]),\n",
        "                                          (x_ipm[k], y_ipm[k]), [0, 0, 0], width)\n",
        "        return im_ipm\n",
        "\n",
        "    def draw_3d_curves(self, ax, lane_anchor, draw_type='laneline', color=[0, 0, 1]):\n",
        "        for j in range(lane_anchor.shape[0]):\n",
        "            # draw laneline\n",
        "            if draw_type is 'laneline' and lane_anchor[j, self.anchor_dim - 1] > self.prob_th:\n",
        "                x_offsets = lane_anchor[j, :self.num_y_steps]\n",
        "                x_g = x_offsets + self.anchor_x_steps[j]\n",
        "                if self.no_3d:\n",
        "                    z_g = np.zeros_like(x_g)\n",
        "                else:\n",
        "                    z_g = lane_anchor[j, self.num_y_steps:2*self.num_y_steps]\n",
        "                ax.plot(x_g, self.anchor_y_steps, z_g, color=color)\n",
        "\n",
        "            # draw centerline\n",
        "            if draw_type is 'centerline' and lane_anchor[j, 2*self.anchor_dim - 1] > self.prob_th:\n",
        "                x_offsets = lane_anchor[j, self.anchor_dim:self.anchor_dim + self.num_y_steps]\n",
        "                x_g = x_offsets + self.anchor_x_steps[j]\n",
        "                if self.no_3d:\n",
        "                    z_g = np.zeros_like(x_g)\n",
        "                else:\n",
        "                    z_g = lane_anchor[j, self.anchor_dim + self.num_y_steps:self.anchor_dim + 2*self.num_y_steps]\n",
        "                ax.plot(x_g, self.anchor_y_steps, z_g, color=color)\n",
        "\n",
        "            # draw the additional centerline for the merging case\n",
        "            if draw_type is 'centerline' and lane_anchor[j, 3*self.anchor_dim - 1] > self.prob_th:\n",
        "                x_offsets = lane_anchor[j, 2*self.anchor_dim:2*self.anchor_dim + self.num_y_steps]\n",
        "                x_g = x_offsets + self.anchor_x_steps[j]\n",
        "                if self.no_3d:\n",
        "                    z_g = np.zeros_like(x_g)\n",
        "                else:\n",
        "                    z_g = lane_anchor[j, 2*self.anchor_dim + self.num_y_steps:2*self.anchor_dim + 2*self.num_y_steps]\n",
        "                ax.plot(x_g, self.anchor_y_steps, z_g, color=color)\n",
        "\n",
        "    def draw_3d_curves_new(self, ax, lane_anchor, h_cam, draw_type='laneline', color=[0, 0, 1]):\n",
        "        for j in range(lane_anchor.shape[0]):\n",
        "            # draw laneline\n",
        "            if draw_type is 'laneline' and lane_anchor[j, self.anchor_dim - 1] > self.prob_th:\n",
        "                x_offsets = lane_anchor[j, :self.num_y_steps]\n",
        "                x_gflat = x_offsets + self.anchor_x_steps[j]\n",
        "                if self.no_3d:\n",
        "                    z_g = np.zeros_like(x_gflat)\n",
        "                    visibility = np.ones_like(x_gflat)\n",
        "                else:\n",
        "                    z_g = lane_anchor[j, self.num_y_steps:2*self.num_y_steps]\n",
        "                    visibility = lane_anchor[j, 2*self.num_y_steps:3*self.num_y_steps]\n",
        "                x_gflat = x_gflat[np.where(visibility > self.prob_th)]\n",
        "                z_g = z_g[np.where(visibility > self.prob_th)]\n",
        "                if len(x_gflat) > 0:\n",
        "                    # transform lane detected in flat ground space to 3d ground space\n",
        "                    x_g, y_g = transform_lane_gflat2g(h_cam,\n",
        "                                                      x_gflat,\n",
        "                                                      self.anchor_y_steps[np.where(visibility > self.prob_th)],\n",
        "                                                      z_g)\n",
        "                    ax.plot(x_g, y_g, z_g, color=color)\n",
        "\n",
        "            # draw centerline\n",
        "            if draw_type is 'centerline' and lane_anchor[j, 2*self.anchor_dim - 1] > self.prob_th:\n",
        "                x_offsets = lane_anchor[j, self.anchor_dim:self.anchor_dim + self.num_y_steps]\n",
        "                x_gflat = x_offsets + self.anchor_x_steps[j]\n",
        "                if self.no_3d:\n",
        "                    z_g = np.zeros_like(x_gflat)\n",
        "                    visibility = np.ones_like(x_gflat)\n",
        "                else:\n",
        "                    z_g = lane_anchor[j, self.anchor_dim + self.num_y_steps:self.anchor_dim + 2*self.num_y_steps]\n",
        "                    visibility = lane_anchor[j, self.anchor_dim + 2*self.num_y_steps:self.anchor_dim + 3*self.num_y_steps]\n",
        "                x_gflat = x_gflat[np.where(visibility > self.prob_th)]\n",
        "                z_g = z_g[np.where(visibility > self.prob_th)]\n",
        "                if len(x_gflat) > 0:\n",
        "                    # transform lane detected in flat ground space to 3d ground space\n",
        "                    x_g, y_g = transform_lane_gflat2g(h_cam,\n",
        "                                                      x_gflat,\n",
        "                                                      self.anchor_y_steps[np.where(visibility > self.prob_th)],\n",
        "                                                      z_g)\n",
        "                    ax.plot(x_g, y_g, z_g, color=color)\n",
        "\n",
        "            # draw the additional centerline for the merging case\n",
        "            if draw_type is 'centerline' and lane_anchor[j, 3*self.anchor_dim - 1] > self.prob_th:\n",
        "                x_offsets = lane_anchor[j, 2*self.anchor_dim:2*self.anchor_dim + self.num_y_steps]\n",
        "                x_gflat = x_offsets + self.anchor_x_steps[j]\n",
        "                if self.no_3d:\n",
        "                    z_g = np.zeros_like(x_gflat)\n",
        "                    visibility = np.ones_like(x_gflat)\n",
        "                else:\n",
        "                    z_g = lane_anchor[j, 2*self.anchor_dim + self.num_y_steps:2*self.anchor_dim + 2*self.num_y_steps]\n",
        "                    visibility = lane_anchor[j, 2*self.anchor_dim + 2*self.num_y_steps:2*self.anchor_dim + 3*self.num_y_steps]\n",
        "                x_gflat = x_gflat[np.where(visibility > self.prob_th)]\n",
        "                z_g = z_g[np.where(visibility > self.prob_th)]\n",
        "                if len(x_gflat) > 0:\n",
        "                    # transform lane detected in flat ground space to 3d ground space\n",
        "                    x_g, y_g = transform_lane_gflat2g(h_cam,\n",
        "                                                      x_gflat,\n",
        "                                                      self.anchor_y_steps[np.where(visibility > self.prob_th)],\n",
        "                                                      z_g)\n",
        "                    ax.plot(x_g, y_g, z_g, color=color)\n",
        "\n",
        "    def save_result(self, dataset, train_or_val, epoch, batch_i, idx, images, gt, pred, pred_cam_pitch, pred_cam_height, aug_mat=np.identity(3, dtype=np.float), evaluate=False):\n",
        "        if not dataset.data_aug:\n",
        "            aug_mat = np.repeat(np.expand_dims(aug_mat, axis=0), idx.shape[0], axis=0)\n",
        "\n",
        "        for i in range(idx.shape[0]):\n",
        "            # during training, only visualize the first sample of this batch\n",
        "            if i > 0 and not evaluate:\n",
        "                break\n",
        "            im = images.permute(0, 2, 3, 1).data.cpu().numpy()[i]\n",
        "            # the vgg_std and vgg_mean are for images in [0, 1] range\n",
        "            im = im * np.array(self.vgg_std)\n",
        "            im = im + np.array(self.vgg_mean)\n",
        "            im = np.clip(im, 0, 1)\n",
        "\n",
        "            gt_anchors = gt[i]\n",
        "            pred_anchors = pred[i]\n",
        "\n",
        "            # apply nms to avoid output directly neighbored lanes\n",
        "            # consider w/o centerline cases\n",
        "            if self.no_centerline:\n",
        "                pred_anchors[:, -1] = nms_1d(pred_anchors[:, -1])\n",
        "            else:\n",
        "                pred_anchors[:, self.anchor_dim - 1] = nms_1d(pred_anchors[:, self.anchor_dim - 1])\n",
        "                pred_anchors[:, 2 * self.anchor_dim - 1] = nms_1d(pred_anchors[:, 2 * self.anchor_dim - 1])\n",
        "                pred_anchors[:, 3 * self.anchor_dim - 1] = nms_1d(pred_anchors[:, 3 * self.anchor_dim - 1])\n",
        "\n",
        "            H_g2im, P_g2im, H_crop, H_im2ipm = dataset.transform_mats(idx[i])\n",
        "            if self.no_3d:\n",
        "                P_gt = np.matmul(H_crop, H_g2im)\n",
        "                H_g2im_pred = homograpthy_g2im(pred_cam_pitch[i],\n",
        "                                               pred_cam_height[i], dataset.K)\n",
        "                P_pred = np.matmul(H_crop, H_g2im_pred)\n",
        "\n",
        "                # consider data augmentation\n",
        "                P_gt = np.matmul(aug_mat[i, :, :], P_gt)\n",
        "                P_pred = np.matmul(aug_mat[i, :, :], P_pred)\n",
        "            else:\n",
        "                P_gt = np.matmul(H_crop, P_g2im)\n",
        "                P_g2im_pred = projection_g2im(pred_cam_pitch[i],\n",
        "                                              pred_cam_height[i], dataset.K)\n",
        "                P_pred = np.matmul(H_crop, P_g2im_pred)\n",
        "\n",
        "                # consider data augmentation\n",
        "                P_gt = np.matmul(aug_mat[i, :, :], P_gt)\n",
        "                P_pred = np.matmul(aug_mat[i, :, :], P_pred)\n",
        "\n",
        "            # update transformation with image augmentation\n",
        "            H_im2ipm = np.matmul(H_im2ipm, np.linalg.inv(aug_mat[i, :, :]))\n",
        "            im_ipm = cv2.warpPerspective(im, H_im2ipm, (self.ipm_w, self.ipm_h))\n",
        "            im_ipm = np.clip(im_ipm, 0, 1)\n",
        "\n",
        "            # draw lanes on image\n",
        "            im_laneline = im.copy()\n",
        "            im_laneline = self.draw_on_img(im_laneline, gt_anchors, P_gt, 'laneline', [0, 0, 1])\n",
        "            im_laneline = self.draw_on_img(im_laneline, pred_anchors, P_pred, 'laneline', [1, 0, 0])\n",
        "            if not self.no_centerline:\n",
        "                im_centerline = im.copy()\n",
        "                im_centerline = self.draw_on_img(im_centerline, gt_anchors, P_gt, 'centerline', [0, 0, 1])\n",
        "                im_centerline = self.draw_on_img(im_centerline, pred_anchors, P_pred, 'centerline', [1, 0, 0])\n",
        "\n",
        "            # draw lanes on ipm\n",
        "            ipm_laneline = im_ipm.copy()\n",
        "            ipm_laneline = self.draw_on_ipm(ipm_laneline, gt_anchors, 'laneline', [0, 0, 1])\n",
        "            ipm_laneline = self.draw_on_ipm(ipm_laneline, pred_anchors, 'laneline', [1, 0, 0])\n",
        "            if not self.no_centerline:\n",
        "                ipm_centerline = im_ipm.copy()\n",
        "                ipm_centerline = self.draw_on_ipm(ipm_centerline, gt_anchors, 'centerline', [0, 0, 1])\n",
        "                ipm_centerline = self.draw_on_ipm(ipm_centerline, pred_anchors, 'centerline', [1, 0, 0])\n",
        "\n",
        "            # plot on a single figure\n",
        "            if self.no_centerline and self.no_3d:\n",
        "                fig = plt.figure()\n",
        "                ax1 = fig.add_subplot(121)\n",
        "                ax2 = fig.add_subplot(122)\n",
        "                ax1.imshow(im_laneline)\n",
        "                ax2.imshow(ipm_laneline)\n",
        "            elif not self.no_centerline and self.no_3d:\n",
        "                fig = plt.figure()\n",
        "                ax1 = fig.add_subplot(221)\n",
        "                ax2 = fig.add_subplot(222)\n",
        "                ax3 = fig.add_subplot(223)\n",
        "                ax4 = fig.add_subplot(224)\n",
        "                ax1.imshow(im_laneline)\n",
        "                ax2.imshow(ipm_laneline)\n",
        "                ax3.imshow(im_centerline)\n",
        "                ax4.imshow(ipm_centerline)\n",
        "            elif not self.no_centerline and not self.no_3d:\n",
        "                fig = plt.figure()\n",
        "                ax1 = fig.add_subplot(231)\n",
        "                ax2 = fig.add_subplot(232)\n",
        "                ax3 = fig.add_subplot(233, projection='3d')\n",
        "                ax4 = fig.add_subplot(234)\n",
        "                ax5 = fig.add_subplot(235)\n",
        "                ax6 = fig.add_subplot(236, projection='3d')\n",
        "                ax1.imshow(im_laneline)\n",
        "                ax2.imshow(ipm_laneline)\n",
        "                self.draw_3d_curves(ax3, gt_anchors, 'laneline', [0, 0, 1])\n",
        "                self.draw_3d_curves(ax3, pred_anchors, 'laneline', [1, 0, 0])\n",
        "                ax3.set_xlabel('x axis')\n",
        "                ax3.set_ylabel('y axis')\n",
        "                ax3.set_zlabel('z axis')\n",
        "                bottom, top = ax3.get_zlim()\n",
        "                ax3.set_zlim(min(bottom, -1), max(top, 1))\n",
        "                ax3.set_xlim(-20, 20)\n",
        "                ax3.set_ylim(0, 100)\n",
        "                ax4.imshow(im_centerline)\n",
        "                ax5.imshow(ipm_centerline)\n",
        "                self.draw_3d_curves(ax6, gt_anchors, 'centerline', [0, 0, 1])\n",
        "                self.draw_3d_curves(ax6, pred_anchors, 'centerline', [1, 0, 0])\n",
        "                ax6.set_xlabel('x axis')\n",
        "                ax6.set_ylabel('y axis')\n",
        "                ax6.set_zlabel('z axis')\n",
        "                bottom, top = ax6.get_zlim()\n",
        "                ax6.set_zlim(min(bottom, -1), max(top, 1))\n",
        "                ax6.set_xlim(-20, 20)\n",
        "                ax6.set_ylim(0, 100)\n",
        "\n",
        "            if evaluate:\n",
        "                fig.savefig(self.save_path + '/example/' + self.vis_folder + '/infer_{}'.format(idx[i]))\n",
        "            else:\n",
        "                fig.savefig(self.save_path + '/example/{}/epoch-{}_batch-{}_idx-{}'.format(train_or_val,\n",
        "                                                                                           epoch, batch_i, idx[i]))\n",
        "            plt.clf()\n",
        "            plt.close(fig)\n",
        "\n",
        "    def save_result_new(self, dataset, train_or_val, epoch, batch_i, idx, images, gt, pred, pred_cam_pitch, pred_cam_height, aug_mat=np.identity(3, dtype=np.float), evaluate=False):\n",
        "        if not dataset.data_aug:\n",
        "            aug_mat = np.repeat(np.expand_dims(aug_mat, axis=0), idx.shape[0], axis=0)\n",
        "\n",
        "        for i in range(idx.shape[0]):\n",
        "            # during training, only visualize the first sample of this batch\n",
        "            if i > 0 and not evaluate:\n",
        "                break\n",
        "            im = images.permute(0, 2, 3, 1).data.cpu().numpy()[i]\n",
        "            # the vgg_std and vgg_mean are for images in [0, 1] range\n",
        "            im = im * np.array(self.vgg_std)\n",
        "            im = im + np.array(self.vgg_mean)\n",
        "            im = np.clip(im, 0, 1)\n",
        "\n",
        "            gt_anchors = gt[i]\n",
        "            pred_anchors = pred[i]\n",
        "\n",
        "            # apply nms to avoid output directly neighbored lanes\n",
        "            # consider w/o centerline cases\n",
        "            if self.no_centerline:\n",
        "                pred_anchors[:, -1] = nms_1d(pred_anchors[:, -1])\n",
        "            else:\n",
        "                pred_anchors[:, self.anchor_dim - 1] = nms_1d(pred_anchors[:, self.anchor_dim - 1])\n",
        "                pred_anchors[:, 2 * self.anchor_dim - 1] = nms_1d(pred_anchors[:, 2 * self.anchor_dim - 1])\n",
        "                pred_anchors[:, 3 * self.anchor_dim - 1] = nms_1d(pred_anchors[:, 3 * self.anchor_dim - 1])\n",
        "\n",
        "            H_g2im, P_g2im, H_crop, H_im2ipm = dataset.transform_mats(idx[i])\n",
        "            P_gt = np.matmul(H_crop, H_g2im)\n",
        "            H_g2im_pred = homograpthy_g2im(pred_cam_pitch[i],\n",
        "                                           pred_cam_height[i], dataset.K)\n",
        "            P_pred = np.matmul(H_crop, H_g2im_pred)\n",
        "\n",
        "            # consider data augmentation\n",
        "            P_gt = np.matmul(aug_mat[i, :, :], P_gt)\n",
        "            P_pred = np.matmul(aug_mat[i, :, :], P_pred)\n",
        "\n",
        "            # update transformation with image augmentation\n",
        "            H_im2ipm = np.matmul(H_im2ipm, np.linalg.inv(aug_mat[i, :, :]))\n",
        "            im_ipm = cv2.warpPerspective(im, H_im2ipm, (self.ipm_w, self.ipm_h))\n",
        "            im_ipm = np.clip(im_ipm, 0, 1)\n",
        "\n",
        "            # draw lanes on image\n",
        "            im_laneline = im.copy()\n",
        "            im_laneline = self.draw_on_img_new(im_laneline, gt_anchors, P_gt, 'laneline', [0, 0, 1])\n",
        "            im_laneline = self.draw_on_img_new(im_laneline, pred_anchors, P_pred, 'laneline', [1, 0, 0])\n",
        "            if not self.no_centerline:\n",
        "                im_centerline = im.copy()\n",
        "                im_centerline = self.draw_on_img_new(im_centerline, gt_anchors, P_gt, 'centerline', [0, 0, 1])\n",
        "                im_centerline = self.draw_on_img_new(im_centerline, pred_anchors, P_pred, 'centerline', [1, 0, 0])\n",
        "\n",
        "            # draw lanes on ipm\n",
        "            ipm_laneline = im_ipm.copy()\n",
        "            ipm_laneline = self.draw_on_ipm_new(ipm_laneline, gt_anchors, 'laneline', [0, 0, 1])\n",
        "            ipm_laneline = self.draw_on_ipm_new(ipm_laneline, pred_anchors, 'laneline', [1, 0, 0])\n",
        "            if not self.no_centerline:\n",
        "                ipm_centerline = im_ipm.copy()\n",
        "                ipm_centerline = self.draw_on_ipm_new(ipm_centerline, gt_anchors, 'centerline', [0, 0, 1])\n",
        "                ipm_centerline = self.draw_on_ipm_new(ipm_centerline, pred_anchors, 'centerline', [1, 0, 0])\n",
        "\n",
        "            # plot on a single figure\n",
        "            if self.no_centerline and self.no_3d:\n",
        "                fig = plt.figure()\n",
        "                ax1 = fig.add_subplot(121)\n",
        "                ax2 = fig.add_subplot(122)\n",
        "                ax1.imshow(im_laneline)\n",
        "                ax2.imshow(ipm_laneline)\n",
        "            elif not self.no_centerline and self.no_3d:\n",
        "                fig = plt.figure()\n",
        "                ax1 = fig.add_subplot(221)\n",
        "                ax2 = fig.add_subplot(222)\n",
        "                ax3 = fig.add_subplot(223)\n",
        "                ax4 = fig.add_subplot(224)\n",
        "                ax1.imshow(im_laneline)\n",
        "                ax2.imshow(ipm_laneline)\n",
        "                ax3.imshow(im_centerline)\n",
        "                ax4.imshow(ipm_centerline)\n",
        "            elif not self.no_centerline and not self.no_3d:\n",
        "                fig = plt.figure()\n",
        "                ax1 = fig.add_subplot(231)\n",
        "                ax2 = fig.add_subplot(232)\n",
        "                ax3 = fig.add_subplot(233, projection='3d')\n",
        "                ax4 = fig.add_subplot(234)\n",
        "                ax5 = fig.add_subplot(235)\n",
        "                ax6 = fig.add_subplot(236, projection='3d')\n",
        "                ax1.imshow(im_laneline)\n",
        "                ax2.imshow(ipm_laneline)\n",
        "                # TODO:use separate gt_cam_height when ready\n",
        "                self.draw_3d_curves_new(ax3, gt_anchors, pred_cam_height[i], 'laneline', [0, 0, 1])\n",
        "                self.draw_3d_curves_new(ax3, pred_anchors, pred_cam_height[i], 'laneline', [1, 0, 0])\n",
        "                ax3.set_xlabel('x axis')\n",
        "                ax3.set_ylabel('y axis')\n",
        "                ax3.set_zlabel('z axis')\n",
        "                bottom, top = ax3.get_zlim()\n",
        "                ax3.set_xlim(-20, 20)\n",
        "                ax3.set_ylim(0, 100)\n",
        "                ax3.set_zlim(min(bottom, -1), max(top, 1))\n",
        "                ax4.imshow(im_centerline)\n",
        "                ax5.imshow(ipm_centerline)\n",
        "                # TODO:use separate gt_cam_height when ready\n",
        "                self.draw_3d_curves_new(ax6, gt_anchors, pred_cam_height[i], 'centerline', [0, 0, 1])\n",
        "                self.draw_3d_curves_new(ax6, pred_anchors, pred_cam_height[i], 'centerline', [1, 0, 0])\n",
        "                ax6.set_xlabel('x axis')\n",
        "                ax6.set_ylabel('y axis')\n",
        "                ax6.set_zlabel('z axis')\n",
        "                bottom, top = ax6.get_zlim()\n",
        "                ax6.set_xlim(-20, 20)\n",
        "                ax6.set_ylim(0, 100)\n",
        "                ax6.set_zlim(min(bottom, -1), max(top, 1))\n",
        "\n",
        "            if evaluate:\n",
        "                fig.savefig(self.save_path + '/example/' + self.vis_folder + '/infer_{}'.format(idx[i]))\n",
        "            else:\n",
        "                fig.savefig(self.save_path + '/example/{}/epoch-{}_batch-{}_idx-{}'.format(train_or_val,\n",
        "                                                                                           epoch, batch_i, idx[i]))\n",
        "            plt.clf()\n",
        "            plt.close(fig)\n",
        "\n",
        "\n",
        "def prune_3d_lane_by_visibility(lane_3d, visibility):\n",
        "    lane_3d = lane_3d[visibility > 0, ...]\n",
        "    return lane_3d\n",
        "\n",
        "\n",
        "def prune_3d_lane_by_range(lane_3d, x_min, x_max):\n",
        "    # TODO: solve hard coded range later\n",
        "    # remove points with y out of range\n",
        "    # 3D label may miss super long straight-line with only two points: Not have to be 200, gt need a min-step\n",
        "    # 2D dataset requires this to rule out those points projected to ground, but out of meaningful range\n",
        "    lane_3d = lane_3d[np.logical_and(lane_3d[:, 1] > 0, lane_3d[:, 1] < 200), ...]\n",
        "\n",
        "    # remove lane points out of x range\n",
        "    lane_3d = lane_3d[np.logical_and(lane_3d[:, 0] > x_min,\n",
        "                                     lane_3d[:, 0] < x_max), ...]\n",
        "    return lane_3d\n",
        "\n",
        "\n",
        "def resample_laneline_in_y(input_lane, y_steps, out_vis=False):\n",
        "    \"\"\"\n",
        "        Interpolate x, z values at each anchor grid, including those beyond the range of input lnae y range\n",
        "    :param input_lane: N x 2 or N x 3 ndarray, one row for a point (x, y, z-optional).\n",
        "                       It requires y values of input lane in ascending order\n",
        "    :param y_steps: a vector of steps in y\n",
        "    :param out_vis: whether to output visibility indicator which only depends on input y range\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    # at least two points are included\n",
        "    assert(input_lane.shape[0] >= 2)\n",
        "\n",
        "    y_min = np.min(input_lane[:, 1])-5\n",
        "    y_max = np.max(input_lane[:, 1])+5\n",
        "\n",
        "    if input_lane.shape[1] < 3:\n",
        "        input_lane = np.concatenate([input_lane, np.zeros([input_lane.shape[0], 1], dtype=np.float32)], axis=1)\n",
        "\n",
        "    f_x = interp1d(input_lane[:, 1], input_lane[:, 0], fill_value=\"extrapolate\")\n",
        "    f_z = interp1d(input_lane[:, 1], input_lane[:, 2], fill_value=\"extrapolate\")\n",
        "\n",
        "    x_values = f_x(y_steps)\n",
        "    z_values = f_z(y_steps)\n",
        "\n",
        "    if out_vis:\n",
        "        output_visibility = np.logical_and(y_steps >= y_min, y_steps <= y_max)\n",
        "        return x_values, z_values, output_visibility.astype(np.float32) + 1e-9\n",
        "    return x_values, z_values\n",
        "\n",
        "\n",
        "def resample_laneline_in_y_with_vis(input_lane, y_steps, vis_vec):\n",
        "    \"\"\"\n",
        "        Interpolate x, z values at each anchor grid, including those beyond the range of input lnae y range\n",
        "    :param input_lane: N x 2 or N x 3 ndarray, one row for a point (x, y, z-optional).\n",
        "                       It requires y values of input lane in ascending order\n",
        "    :param y_steps: a vector of steps in y\n",
        "    :param out_vis: whether to output visibility indicator which only depends on input y range\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    # at least two points are included\n",
        "    assert(input_lane.shape[0] >= 2)\n",
        "\n",
        "    if input_lane.shape[1] < 3:\n",
        "        input_lane = np.concatenate([input_lane, np.zeros([input_lane.shape[0], 1], dtype=np.float32)], axis=1)\n",
        "\n",
        "    f_x = interp1d(input_lane[:, 1], input_lane[:, 0], fill_value=\"extrapolate\")\n",
        "    f_z = interp1d(input_lane[:, 1], input_lane[:, 2], fill_value=\"extrapolate\")\n",
        "    f_vis = interp1d(input_lane[:, 1], vis_vec, fill_value=\"extrapolate\")\n",
        "\n",
        "    x_values = f_x(y_steps)\n",
        "    z_values = f_z(y_steps)\n",
        "    vis_values = f_vis(y_steps)\n",
        "\n",
        "    x_values = x_values[vis_values > 0.5]\n",
        "    y_values = y_steps[vis_values > 0.5]\n",
        "    z_values = z_values[vis_values > 0.5]\n",
        "    return np.array([x_values, y_values, z_values]).T\n",
        "\n",
        "\n",
        "def homography_im2ipm_norm(top_view_region, org_img_size, crop_y, resize_img_size, cam_pitch, cam_height, K):\n",
        "    \"\"\"\n",
        "        Compute the normalized transformation such that image region are mapped to top_view region maps to\n",
        "        the top view image's 4 corners\n",
        "        Ground coordinates: x-right, y-forward, z-up\n",
        "        The purpose of applying normalized transformation: 1. invariance in scale change\n",
        "                                                           2.Torch grid sample is based on normalized grids\n",
        "    :param top_view_region: a 4 X 2 list of (X, Y) indicating the top-view region corners in order:\n",
        "                            top-left, top-right, bottom-left, bottom-right\n",
        "    :param org_img_size: the size of original image size: [h, w]\n",
        "    :param crop_y: pixels croped from original img\n",
        "    :param resize_img_size: the size of image as network input: [h, w]\n",
        "    :param cam_pitch: camera pitch angle wrt ground plane\n",
        "    :param cam_height: camera height wrt ground plane in meters\n",
        "    :param K: camera intrinsic parameters\n",
        "    :return: H_im2ipm_norm: the normalized transformation from image to IPM image\n",
        "    \"\"\"\n",
        "\n",
        "    # compute homography transformation from ground to image (only this depends on cam_pitch and cam height)\n",
        "    H_g2im = homograpthy_g2im(cam_pitch, cam_height, K)\n",
        "    # transform original image region to network input region\n",
        "    H_c = homography_crop_resize(org_img_size, crop_y, resize_img_size)\n",
        "    H_g2im = np.matmul(H_c, H_g2im)\n",
        "\n",
        "    # compute top-view corners' coordinates in image\n",
        "    x_2d, y_2d = homographic_transformation(H_g2im, top_view_region[:, 0], top_view_region[:, 1])\n",
        "    border_im = np.concatenate([x_2d.reshape(-1, 1), y_2d.reshape(-1, 1)], axis=1)\n",
        "\n",
        "    # compute the normalized transformation\n",
        "    border_im[:, 0] = border_im[:, 0] / resize_img_size[1]\n",
        "    border_im[:, 1] = border_im[:, 1] / resize_img_size[0]\n",
        "    border_im = np.float32(border_im)\n",
        "    dst = np.float32([[0, 0], [1, 0], [0, 1], [1, 1]])\n",
        "    # img to ipm\n",
        "    H_im2ipm_norm = cv2.getPerspectiveTransform(border_im, dst)\n",
        "    # ipm to im\n",
        "    H_ipm2im_norm = cv2.getPerspectiveTransform(dst, border_im)\n",
        "    return H_im2ipm_norm, H_ipm2im_norm\n",
        "\n",
        "\n",
        "def homography_ipmnorm2g(top_view_region):\n",
        "    src = np.float32([[0, 0], [1, 0], [0, 1], [1, 1]])\n",
        "    H_ipmnorm2g = cv2.getPerspectiveTransform(src, np.float32(top_view_region))\n",
        "    return H_ipmnorm2g\n",
        "\n",
        "\n",
        "def homograpthy_g2im(cam_pitch, cam_height, K):\n",
        "    # transform top-view region to original image region\n",
        "    R_g2c = np.array([[1, 0, 0],\n",
        "                      [0, np.cos(np.pi / 2 + cam_pitch), -np.sin(np.pi / 2 + cam_pitch)],\n",
        "                      [0, np.sin(np.pi / 2 + cam_pitch), np.cos(np.pi / 2 + cam_pitch)]])\n",
        "    H_g2im = np.matmul(K, np.concatenate([R_g2c[:, 0:2], [[0], [cam_height], [0]]], 1))\n",
        "    return H_g2im\n",
        "\n",
        "\n",
        "def projection_g2im(cam_pitch, cam_height, K):\n",
        "    P_g2c = np.array([[1,                             0,                              0,          0],\n",
        "                      [0, np.cos(np.pi / 2 + cam_pitch), -np.sin(np.pi / 2 + cam_pitch), cam_height],\n",
        "                      [0, np.sin(np.pi / 2 + cam_pitch),  np.cos(np.pi / 2 + cam_pitch),          0]])\n",
        "    P_g2im = np.matmul(K, P_g2c)\n",
        "    return P_g2im\n",
        "\n",
        "\n",
        "def homography_crop_resize(org_img_size, crop_y, resize_img_size):\n",
        "    \"\"\"\n",
        "        compute the homography matrix transform original image to cropped and resized image\n",
        "    :param org_img_size: [org_h, org_w]\n",
        "    :param crop_y:\n",
        "    :param resize_img_size: [resize_h, resize_w]\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # transform original image region to network input region\n",
        "    ratio_x = resize_img_size[1] / org_img_size[1]\n",
        "    ratio_y = resize_img_size[0] / (org_img_size[0] - crop_y)\n",
        "    H_c = np.array([[ratio_x, 0, 0],\n",
        "                    [0, ratio_y, -ratio_y*crop_y],\n",
        "                    [0, 0, 1]])\n",
        "    return H_c\n",
        "\n",
        "\n",
        "def homographic_transformation(Matrix, x, y):\n",
        "    \"\"\"\n",
        "    Helper function to transform coordinates defined by transformation matrix\n",
        "    Args:\n",
        "            Matrix (multi dim - array): 3x3 homography matrix\n",
        "            x (array): original x coordinates\n",
        "            y (array): original y coordinates\n",
        "    \"\"\"\n",
        "    ones = np.ones((1, len(y)))\n",
        "    coordinates = np.vstack((x, y, ones))\n",
        "    trans = np.matmul(Matrix, coordinates)\n",
        "\n",
        "    x_vals = trans[0, :]/trans[2, :]\n",
        "    y_vals = trans[1, :]/trans[2, :]\n",
        "    return x_vals, y_vals\n",
        "\n",
        "\n",
        "def projective_transformation(Matrix, x, y, z):\n",
        "    \"\"\"\n",
        "    Helper function to transform coordinates defined by transformation matrix\n",
        "    Args:\n",
        "            Matrix (multi dim - array): 3x4 projection matrix\n",
        "            x (array): original x coordinates\n",
        "            y (array): original y coordinates\n",
        "            z (array): original z coordinates\n",
        "    \"\"\"\n",
        "    ones = np.ones((1, len(z)))\n",
        "    coordinates = np.vstack((x, y, z, ones))\n",
        "    trans = np.matmul(Matrix, coordinates)\n",
        "\n",
        "    x_vals = trans[0, :]/trans[2, :]\n",
        "    y_vals = trans[1, :]/trans[2, :]\n",
        "    return x_vals, y_vals\n",
        "\n",
        "\n",
        "def transform_lane_gflat2g(h_cam, X_gflat, Y_gflat, Z_g):\n",
        "    \"\"\"\n",
        "        Given X coordinates in flat ground space, Y coordinates in flat ground space, and Z coordinates in real 3D ground space\n",
        "        with projection matrix from 3D ground to flat ground, compute real 3D coordinates X, Y in 3D ground space.\n",
        "    :param P_g2gflat: a 3 X 4 matrix transforms lane form 3d ground x,y,z to flat ground x, y\n",
        "    :param X_gflat: X coordinates in flat ground space\n",
        "    :param Y_gflat: Y coordinates in flat ground space\n",
        "    :param Z_g: Z coordinates in real 3D ground space\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    X_g = X_gflat - X_gflat * Z_g / h_cam\n",
        "    Y_g = Y_gflat - Y_gflat * Z_g / h_cam\n",
        "\n",
        "    return X_g, Y_g\n",
        "\n",
        "\n",
        "def transform_lane_g2gflat(h_cam, X_g, Y_g, Z_g):\n",
        "    \"\"\"\n",
        "        Given X coordinates in flat ground space, Y coordinates in flat ground space, and Z coordinates in real 3D ground space\n",
        "        with projection matrix from 3D ground to flat ground, compute real 3D coordinates X, Y in 3D ground space.\n",
        "    :param P_g2gflat: a 3 X 4 matrix transforms lane form 3d ground x,y,z to flat ground x, y\n",
        "    :param X_gflat: X coordinates in flat ground space\n",
        "    :param Y_gflat: Y coordinates in flat ground space\n",
        "    :param Z_g: Z coordinates in real 3D ground space\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    X_gflat = X_g * h_cam / (h_cam - Z_g)\n",
        "    Y_gflat = Y_g * h_cam / (h_cam - Z_g)\n",
        "\n",
        "    return X_gflat, Y_gflat\n",
        "\n",
        "\n",
        "def nms_1d(v):\n",
        "    \"\"\"\n",
        "    :param v: a 1D numpy array\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    v_out = v.copy()\n",
        "    len = v.shape[0]\n",
        "    if len < 2:\n",
        "        return v\n",
        "    for i in range(len):\n",
        "        if i is not 0 and v[i - 1] > v[i]:\n",
        "            v_out[i] = 0.\n",
        "        elif i is not len-1 and v[i+1] > v[i]:\n",
        "            v_out[i] = 0.\n",
        "    return v_out\n",
        "\n",
        "\n",
        "def first_run(save_path):\n",
        "    txt_file = os.path.join(save_path,'first_run.txt')\n",
        "    if not os.path.exists(txt_file):\n",
        "        open(txt_file, 'w').close()\n",
        "    else:\n",
        "        saved_epoch = open(txt_file).read()\n",
        "        if saved_epoch is None:\n",
        "            print('You forgot to delete [first run file]')\n",
        "            return '' \n",
        "        return saved_epoch\n",
        "    return ''\n",
        "\n",
        "\n",
        "def mkdir_if_missing(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        try:\n",
        "            os.makedirs(directory)\n",
        "        except OSError as e:\n",
        "            if e.errno != errno.EEXIST:\n",
        "                raise\n",
        "\n",
        "\n",
        "# trick from stackoverflow\n",
        "def str2bool(argument):\n",
        "    if argument.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif argument.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Wrong argument in argparse, should be a boolean')\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "    \"\"\"\n",
        "    Source https://github.com/Cysu/open-reid/blob/master/reid/utils/logging.py.\n",
        "    \"\"\"\n",
        "    def __init__(self, fpath=None):\n",
        "        self.console = sys.stdout\n",
        "        self.file = None\n",
        "        self.fpath = fpath\n",
        "        if fpath is not None:\n",
        "            mkdir_if_missing(os.path.dirname(fpath))\n",
        "            self.file = open(fpath, 'w')\n",
        "\n",
        "    def __del__(self):\n",
        "        self.close()\n",
        "\n",
        "    def __enter__(self):\n",
        "        pass\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        self.close()\n",
        "\n",
        "    def write(self, msg):\n",
        "        self.console.write(msg)\n",
        "        if self.file is not None:\n",
        "            self.file.write(msg)\n",
        "\n",
        "    def flush(self):\n",
        "        self.console.flush()\n",
        "        if self.file is not None:\n",
        "            self.file.flush()\n",
        "            os.fsync(self.file.fileno())\n",
        "\n",
        "    def close(self):\n",
        "        self.console.close()\n",
        "        if self.file is not None:\n",
        "            self.file.close()\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def define_optim(optim, params, lr, weight_decay):\n",
        "    if optim == 'adam':\n",
        "        optimizer = torch.optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
        "    elif optim == 'sgd':\n",
        "        optimizer = torch.optim.SGD(params, lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
        "    elif optim == 'rmsprop':\n",
        "        optimizer = torch.optim.RMSprop(params, lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
        "    else:\n",
        "        raise KeyError(\"The requested optimizer: {} is not implemented\".format(optim))\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def define_scheduler(optimizer, args):\n",
        "    if args.lr_policy == 'lambda':\n",
        "        def lambda_rule(epoch):\n",
        "            lr_l = 1.0 - max(0, epoch + 1 - args.niter) / float(args.niter_decay + 1)\n",
        "            return lr_l\n",
        "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
        "    elif args.lr_policy == 'step':\n",
        "        scheduler = lr_scheduler.StepLR(optimizer,\n",
        "                                        step_size=args.lr_decay_iters, gamma=args.gamma)\n",
        "    elif args.lr_policy == 'plateau':\n",
        "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                   factor=args.gamma,\n",
        "                                                   threshold=0.0001,\n",
        "                                                   patience=args.lr_decay_iters)\n",
        "    elif args.lr_policy == 'none':\n",
        "        scheduler = None\n",
        "    else:\n",
        "        return NotImplementedError('learning rate policy [%s] is not implemented', args.lr_policy)\n",
        "    return scheduler\n",
        "\n",
        "\n",
        "def define_init_weights(model, init_w='normal', activation='relu'):\n",
        "    print('Init weights in network with [{}]'.format(init_w))\n",
        "    if init_w == 'normal':\n",
        "        model.apply(weights_init_normal)\n",
        "    elif init_w == 'xavier':\n",
        "        model.apply(weights_init_xavier)\n",
        "    elif init_w == 'kaiming':\n",
        "        model.apply(weights_init_kaiming)\n",
        "    elif init_w == 'orthogonal':\n",
        "        model.apply(weights_init_orthogonal)\n",
        "    else:\n",
        "        raise NotImplementedError('initialization method [{}] is not implemented'.format(init_w))\n",
        "\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "#    print(classname)\n",
        "    if classname.find('Conv') != -1 or classname.find('ConvTranspose') != -1:\n",
        "        init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.zero_()\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.zero_()\n",
        "    elif classname.find('BatchNorm2d') != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def weights_init_xavier(m):\n",
        "    classname = m.__class__.__name__\n",
        "    # print(classname)\n",
        "    if classname.find('Conv') != -1 or classname.find('ConvTranspose') != -1:\n",
        "        init.xavier_normal_(m.weight.data, gain=0.02)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.zero_()\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.xavier_normal_(m.weight.data, gain=0.02)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.zero_()\n",
        "    elif classname.find('BatchNorm2d') != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def weights_init_kaiming(m):\n",
        "    classname = m.__class__.__name__\n",
        "    # print(classname)\n",
        "    if classname.find('Conv') != -1 or classname.find('ConvTranspose') != -1:\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in', nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.zero_()\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in', nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.zero_()\n",
        "    elif classname.find('BatchNorm2d') != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def weights_init_orthogonal(m):\n",
        "    classname = m.__class__.__name__\n",
        "#    print(classname)\n",
        "    if classname.find('Conv') != -1 or classname.find('ConvTranspose') != -1:\n",
        "        init.orthogonal(m.weight.data, gain=1)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.zero_()\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.orthogonal(m.weight.data, gain=1)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.zero_()\n",
        "    elif classname.find('BatchNorm2d') != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0.0)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "k683hhouo8sZ",
        "outputId": "06eb7892-75e4-4906-c96a-068925f815b2"
      },
      "source": [
        "! pip install ortools"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ortools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/bd/75277072925d687aa35a6ea9e23e81a7f6b7c980b2a80949c5b9a3f98c79/ortools-9.0.9048-cp37-cp37m-manylinux1_x86_64.whl (14.4MB)\n",
            "\u001b[K     |████████████████████████████████| 14.4MB 196kB/s \n",
            "\u001b[?25hCollecting protobuf>=3.15.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/53/ddcef00219f2a3c863b24288e24a20c3070bd086a1e77706f22994a7f6db/protobuf-3.17.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 45.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.11 in /usr/local/lib/python3.7/dist-packages (from ortools) (0.12.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.15.8->ortools) (1.15.0)\n",
            "Installing collected packages: protobuf, ortools\n",
            "  Found existing installation: protobuf 3.12.4\n",
            "    Uninstalling protobuf-3.12.4:\n",
            "      Successfully uninstalled protobuf-3.12.4\n",
            "Successfully installed ortools-9.0.9048 protobuf-3.17.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "wKtvRoHwhwE3",
        "outputId": "0cee2951-1755-4495-a093-70aab9231c96"
      },
      "source": [
        "\"\"\"\n",
        "MinCostFow solver adapted for matching two set of contours. The implementation is based on google-ortools.\n",
        "Author: Yuliang Guo (33yuliangguo@gmail.com)\n",
        "Date: March, 2020\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "from ortools.graph import pywrapgraph\n",
        "import time\n",
        "\n",
        "\n",
        "def SolveMinCostFlow(adj_mat, cost_mat):\n",
        "    \"\"\"\n",
        "        Solving an Assignment Problem with MinCostFlow\"\n",
        "    :param adj_mat: adjacency matrix with binary values indicating possible matchings between two sets\n",
        "    :param cost_mat: cost matrix recording the matching cost of every possible pair of items from two sets\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    # Instantiate a SimpleMinCostFlow solver.\n",
        "    min_cost_flow = pywrapgraph.SimpleMinCostFlow()\n",
        "    # Define the directed graph for the flow.\n",
        "\n",
        "    cnt_1, cnt_2 = adj_mat.shape\n",
        "    cnt_nonzero_row = int(np.sum(np.sum(adj_mat, axis=1) > 0))\n",
        "    cnt_nonzero_col = int(np.sum(np.sum(adj_mat, axis=0) > 0))\n",
        "\n",
        "    # prepare directed graph for the flow\n",
        "    start_nodes = np.zeros(cnt_1, dtype=np.int).tolist() +\\\n",
        "                  np.repeat(np.array(range(1, cnt_1+1)), cnt_2).tolist() + \\\n",
        "                  [i for i in range(cnt_1+1, cnt_1 + cnt_2 + 1)]\n",
        "    end_nodes = [i for i in range(1, cnt_1+1)] + \\\n",
        "                np.repeat(np.array([i for i in range(cnt_1+1, cnt_1 + cnt_2 + 1)]).reshape([1, -1]), cnt_1, axis=0).flatten().tolist() + \\\n",
        "                [cnt_1 + cnt_2 + 1 for i in range(cnt_2)]\n",
        "    capacities = np.ones(cnt_1, dtype=np.int).tolist() + adj_mat.flatten().astype(np.int).tolist() + np.ones(cnt_2, dtype=np.int).tolist()\n",
        "    costs = (np.zeros(cnt_1, dtype=np.int).tolist() + cost_mat.flatten().astype(np.int).tolist() + np.zeros(cnt_2, dtype=np.int).tolist())\n",
        "    # Define an array of supplies at each node.\n",
        "    supplies = [min(cnt_nonzero_row, cnt_nonzero_col)] + np.zeros(cnt_1 + cnt_2, dtype=np.int).tolist() + [-min(cnt_nonzero_row, cnt_nonzero_col)]\n",
        "    # supplies = [min(cnt_1, cnt_2)] + np.zeros(cnt_1 + cnt_2, dtype=np.int).tolist() + [-min(cnt_1, cnt_2)]\n",
        "    source = 0\n",
        "    sink = cnt_1 + cnt_2 + 1\n",
        "\n",
        "    # Add each arc.\n",
        "    for i in range(len(start_nodes)):\n",
        "        min_cost_flow.AddArcWithCapacityAndUnitCost(start_nodes[i], end_nodes[i],\n",
        "                                                    capacities[i], costs[i])\n",
        "\n",
        "    # Add node supplies.\n",
        "    for i in range(len(supplies)):\n",
        "        min_cost_flow.SetNodeSupply(i, supplies[i])\n",
        "\n",
        "    match_results = []\n",
        "    # Find the minimum cost flow between node 0 and node 10.\n",
        "    if min_cost_flow.Solve() == min_cost_flow.OPTIMAL:\n",
        "        # print('Total cost = ', min_cost_flow.OptimalCost())\n",
        "        # print()\n",
        "        for arc in range(min_cost_flow.NumArcs()):\n",
        "\n",
        "            # Can ignore arcs leading out of source or into sink.\n",
        "            if min_cost_flow.Tail(arc)!=source and min_cost_flow.Head(arc)!=sink:\n",
        "\n",
        "                # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
        "                # give an assignment of worker to task.\n",
        "\n",
        "                if min_cost_flow.Flow(arc) > 0:\n",
        "                    # print('set A item %d assigned to set B item %d.  Cost = %d' % (\n",
        "                    #     min_cost_flow.Tail(arc)-1,\n",
        "                    #     min_cost_flow.Head(arc)-cnt_1-1,\n",
        "                    #     min_cost_flow.UnitCost(arc)))\n",
        "                    match_results.append([min_cost_flow.Tail(arc)-1,\n",
        "                                          min_cost_flow.Head(arc)-cnt_1-1,\n",
        "                                          min_cost_flow.UnitCost(arc)])\n",
        "    else:\n",
        "        print('There was an issue with the min cost flow input.')\n",
        "\n",
        "    return match_results\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Solving an Assignment Problem with MinCostFlow\"\"\"\n",
        "\n",
        "    # Instantiate a SimpleMinCostFlow solver.\n",
        "    min_cost_flow = pywrapgraph.SimpleMinCostFlow()\n",
        "    # Define the directed graph for the flow.\n",
        "\n",
        "    start_nodes = [0, 0, 0, 0] + [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4] + [5, 6, 7, 8]\n",
        "    end_nodes = [1, 2, 3, 4] + [5, 6, 7, 8, 5, 6, 7, 8, 5, 6, 7, 8, 5, 6, 7, 8] + [9, 9, 9, 9]\n",
        "    capacities = [1, 1, 1, 1] + [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] + [1, 1, 1, 1]\n",
        "    costs = ([0, 0, 0, 0] + [90, 76, 75, 70, 35, 85, 55, 65, 125, 95, 90, 105, 45, 110, 95, 115] + [0, 0, 0, 0])\n",
        "    # Define an array of supplies at each node.\n",
        "    supplies = [4, 0, 0, 0, 0, 0, 0, 0, 0, -4]\n",
        "    source = 0\n",
        "    sink = 9\n",
        "    tasks = 4\n",
        "\n",
        "    # Add each arc.\n",
        "    for i in range(len(start_nodes)):\n",
        "        min_cost_flow.AddArcWithCapacityAndUnitCost(start_nodes[i], end_nodes[i],\n",
        "                                                    capacities[i], costs[i])\n",
        "\n",
        "    # Add node supplies.\n",
        "\n",
        "    for i in range(len(supplies)):\n",
        "        min_cost_flow.SetNodeSupply(i, supplies[i])\n",
        "    # Find the minimum cost flow between node 0 and node 10.\n",
        "    if min_cost_flow.Solve() == min_cost_flow.OPTIMAL:\n",
        "        print('Total cost = ', min_cost_flow.OptimalCost())\n",
        "        print()\n",
        "        for arc in range(min_cost_flow.NumArcs()):\n",
        "\n",
        "            # Can ignore arcs leading out of source or into sink.\n",
        "            if min_cost_flow.Tail(arc)!=source and min_cost_flow.Head(arc)!=sink:\n",
        "\n",
        "                # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
        "                # give an assignment of worker to task.\n",
        "\n",
        "                if min_cost_flow.Flow(arc) > 0:\n",
        "                    print('Worker %d assigned to task %d.  Cost = %d' % (\n",
        "                        min_cost_flow.Tail(arc),\n",
        "                        min_cost_flow.Head(arc),\n",
        "                        min_cost_flow.UnitCost(arc)))\n",
        "    else:\n",
        "        print('There was an issue with the min cost flow input.')\n",
        "\n",
        "'''\n",
        "if __name__ == '__main__':\n",
        "    start_time = time.clock()\n",
        "    main()\n",
        "    print()\n",
        "    print(\"Time =\", time.clock() - start_time, \"seconds\")\n",
        "    '''"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nif __name__ == \\'__main__\\':\\n    start_time = time.clock()\\n    main()\\n    print()\\n    print(\"Time =\", time.clock() - start_time, \"seconds\")\\n    '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLLjxJBfpSC6",
        "outputId": "d8c33cd6-21ce-476d-a277-6932c9028d2a"
      },
      "source": [
        "! pip install ujson"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ujson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/4e/50e8e4cf5f00b537095711c2c86ac4d7191aed2b4fffd5a19f06898f6929/ujson-4.0.2-cp37-cp37m-manylinux1_x86_64.whl (179kB)\n",
            "\r\u001b[K     |█▉                              | 10kB 24.7MB/s eta 0:00:01\r\u001b[K     |███▋                            | 20kB 17.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 30kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 40kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 51kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 61kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 71kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 81kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 92kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 102kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 112kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 122kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 133kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 143kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 153kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 163kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 174kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 7.4MB/s \n",
            "\u001b[?25hInstalling collected packages: ujson\n",
            "Successfully installed ujson-4.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "SEmtbMDZee_N",
        "outputId": "847f43f9-3d29-4d94-e227-6b4a6e8b2143"
      },
      "source": [
        "\"\"\"\n",
        "eval_3D_Lane.py\n",
        "Description: This code is to evaluate 3D lane detection. The optimal matching between ground-truth set and predicted\n",
        "set of lanes are sought via solving a min cost flow.\n",
        "Evaluation metrics includes:\n",
        "    Average Precision (AP)\n",
        "    Max F-scores\n",
        "    x error close (0 - 40 m)\n",
        "    x error far (0 - 100 m)\n",
        "    z error close (0 - 40 m)\n",
        "    z error far (0 - 100 m)\n",
        "Reference: \"Gen-LaneNet: Generalized and Scalable Approach for 3D Lane Detection\". Y. Guo. etal. 2020\n",
        "Author: Yuliang Guo (33yuliangguo@gmail.com)\n",
        "Date: March, 2020\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import os.path as ops\n",
        "import copy\n",
        "import math\n",
        "import ujson as json\n",
        "from scipy.interpolate import interp1d\n",
        "import matplotlib\n",
        "#from tools.utils import *\n",
        "#from tools.MinCostFlow import SolveMinCostFlow\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (35, 30)\n",
        "plt.rcParams.update({'font.size': 25})\n",
        "plt.rcParams.update({'font.weight': 'semibold'})\n",
        "\n",
        "color = [[0, 0, 255],  # red\n",
        "         [0, 255, 0],  # green\n",
        "         [255, 0, 255],  # purple\n",
        "         [255, 255, 0]]  # cyan\n",
        "\n",
        "vis_min_y = 5\n",
        "vis_max_y = 80\n",
        "\n",
        "\n",
        "class LaneEval(object):\n",
        "    def __init__(self, args):\n",
        "        self.dataset_dir = args.dataset_dir\n",
        "        self.K = args.K\n",
        "        self.no_centerline = args.no_centerline\n",
        "        self.resize_h = args.resize_h\n",
        "        self.resize_w = args.resize_w\n",
        "        self.H_crop = homography_crop_resize([args.org_h, args.org_w], args.crop_y, [args.resize_h, args.resize_w])\n",
        "\n",
        "        self.x_min = args.top_view_region[0, 0]\n",
        "        self.x_max = args.top_view_region[1, 0]\n",
        "        self.y_min = args.top_view_region[2, 1]\n",
        "        self.y_max = args.top_view_region[0, 1]\n",
        "        self.y_samples = np.linspace(self.y_min, self.y_max, num=100, endpoint=False)\n",
        "        # self.y_samples = np.linspace(min_y, max_y, num=100, endpoint=False)\n",
        "        self.dist_th = 1.5\n",
        "        self.ratio_th = 0.75\n",
        "        self.close_range = 40\n",
        "\n",
        "    def bench(self, pred_lanes, gt_lanes, gt_visibility, raw_file, gt_cam_height, gt_cam_pitch, vis, ax1, ax2):\n",
        "        \"\"\"\n",
        "            Matching predicted lanes and ground-truth lanes in their IPM projection, ignoring z attributes.\n",
        "            x error, y_error, and z error are all considered, although the matching does not rely on z\n",
        "            The input of prediction and ground-truth lanes are in ground coordinate, x-right, y-forward, z-up\n",
        "            The fundamental assumption is: 1. there are no two points from different lanes with identical x, y\n",
        "                                              but different z's\n",
        "                                           2. there are no two points from a single lane having identical x, y\n",
        "                                              but different z's\n",
        "            If the interest area is within the current drivable road, the above assumptions are almost always valid.\n",
        "        :param pred_lanes: N X 2 or N X 3 lists depending on 2D or 3D\n",
        "        :param gt_lanes: N X 2 or N X 3 lists depending on 2D or 3D\n",
        "        :param raw_file: file path rooted in dataset folder\n",
        "        :param gt_cam_height: camera height given in ground-truth data\n",
        "        :param gt_cam_pitch: camera pitch given in ground-truth data\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        # change this properly\n",
        "        close_range_idx = np.where(self.y_samples > self.close_range)[0][0]\n",
        "\n",
        "        r_lane, p_lane = 0., 0.\n",
        "        x_error_close = []\n",
        "        x_error_far = []\n",
        "        z_error_close = []\n",
        "        z_error_far = []\n",
        "\n",
        "        # only keep the visible portion\n",
        "        gt_lanes = [prune_3d_lane_by_visibility(np.array(gt_lane), np.array(gt_visibility[k])) for k, gt_lane in\n",
        "                    enumerate(gt_lanes)]\n",
        "        gt_lanes = [lane for lane in gt_lanes if lane.shape[0] > 1]\n",
        "        # only consider those gt lanes overlapping with sampling range\n",
        "        gt_lanes = [lane for lane in gt_lanes if lane[0, 1] < self.y_samples[-1] and lane[-1, 1] > self.y_samples[0]]\n",
        "        gt_lanes = [prune_3d_lane_by_range(np.array(gt_lane), 3 * self.x_min, 3 * self.x_max) for gt_lane in gt_lanes]\n",
        "        gt_lanes = [lane for lane in gt_lanes if lane.shape[0] > 1]\n",
        "        cnt_gt = len(gt_lanes)\n",
        "        cnt_pred = len(pred_lanes)\n",
        "\n",
        "        gt_visibility_mat = np.zeros((cnt_gt, 100))\n",
        "        pred_visibility_mat = np.zeros((cnt_pred, 100))\n",
        "        # resample gt and pred at y_samples\n",
        "        for i in range(cnt_gt):\n",
        "            min_y = np.min(np.array(gt_lanes[i])[:, 1])\n",
        "            max_y = np.max(np.array(gt_lanes[i])[:, 1])\n",
        "            x_values, z_values, visibility_vec = resample_laneline_in_y(np.array(gt_lanes[i]), self.y_samples,\n",
        "                                                                        out_vis=True)\n",
        "            gt_lanes[i] = np.vstack([x_values, z_values]).T\n",
        "            gt_visibility_mat[i, :] = np.logical_and(x_values >= self.x_min,\n",
        "                                                     np.logical_and(x_values <= self.x_max,\n",
        "                                                                    np.logical_and(self.y_samples >= min_y,\n",
        "                                                                                   self.y_samples <= max_y)))\n",
        "            gt_visibility_mat[i, :] = np.logical_and(gt_visibility_mat[i, :], visibility_vec)\n",
        "\n",
        "        for i in range(cnt_pred):\n",
        "            # # ATTENTION: ensure y mono increase before interpolation: but it can reduce size\n",
        "            # pred_lanes[i] = make_lane_y_mono_inc(np.array(pred_lanes[i]))\n",
        "            # pred_lane = prune_3d_lane_by_range(np.array(pred_lanes[i]), self.x_min, self.x_max)\n",
        "            min_y = np.min(np.array(pred_lanes[i])[:, 1])\n",
        "            max_y = np.max(np.array(pred_lanes[i])[:, 1])\n",
        "            x_values, z_values, visibility_vec = resample_laneline_in_y(np.array(pred_lanes[i]), self.y_samples,\n",
        "                                                                        out_vis=True)\n",
        "            pred_lanes[i] = np.vstack([x_values, z_values]).T\n",
        "            pred_visibility_mat[i, :] = np.logical_and(x_values >= self.x_min,\n",
        "                                                       np.logical_and(x_values <= self.x_max,\n",
        "                                                                      np.logical_and(self.y_samples >= min_y,\n",
        "                                                                                     self.y_samples <= max_y)))\n",
        "            pred_visibility_mat[i, :] = np.logical_and(pred_visibility_mat[i, :], visibility_vec)\n",
        "            # pred_visibility_mat[i, :] = np.logical_and(x_values >= self.x_min, x_values <= self.x_max)\n",
        "\n",
        "        adj_mat = np.zeros((cnt_gt, cnt_pred), dtype=np.int)\n",
        "        cost_mat = np.zeros((cnt_gt, cnt_pred), dtype=np.int)\n",
        "        cost_mat.fill(1000)\n",
        "        num_match_mat = np.zeros((cnt_gt, cnt_pred), dtype=np.float)\n",
        "        x_dist_mat_close = np.zeros((cnt_gt, cnt_pred), dtype=np.float)\n",
        "        x_dist_mat_close.fill(1000.)\n",
        "        x_dist_mat_far = np.zeros((cnt_gt, cnt_pred), dtype=np.float)\n",
        "        x_dist_mat_far.fill(1000.)\n",
        "        z_dist_mat_close = np.zeros((cnt_gt, cnt_pred), dtype=np.float)\n",
        "        z_dist_mat_close.fill(1000.)\n",
        "        z_dist_mat_far = np.zeros((cnt_gt, cnt_pred), dtype=np.float)\n",
        "        z_dist_mat_far.fill(1000.)\n",
        "        # compute curve to curve distance\n",
        "        for i in range(cnt_gt):\n",
        "            for j in range(cnt_pred):\n",
        "                x_dist = np.abs(gt_lanes[i][:, 0] - pred_lanes[j][:, 0])\n",
        "                z_dist = np.abs(gt_lanes[i][:, 1] - pred_lanes[j][:, 1])\n",
        "                euclidean_dist = np.sqrt(x_dist ** 2 + z_dist ** 2)\n",
        "\n",
        "                # apply visibility to penalize different partial matching accordingly\n",
        "                euclidean_dist[\n",
        "                    np.logical_or(gt_visibility_mat[i, :] < 0.5, pred_visibility_mat[j, :] < 0.5)] = self.dist_th\n",
        "\n",
        "                # if np.average(euclidean_dist) < 2*self.dist_th: # don't prune here to encourage finding perfect match\n",
        "                num_match_mat[i, j] = np.sum(euclidean_dist < self.dist_th)\n",
        "                adj_mat[i, j] = 1\n",
        "                # ATTENTION: use the sum as int type to meet the requirements of min cost flow optimization (int type)\n",
        "                # using num_match_mat as cost does not work?\n",
        "                cost_mat[i, j] = np.sum(euclidean_dist).astype(np.int)\n",
        "                # cost_mat[i, j] = num_match_mat[i, j]\n",
        "\n",
        "                # use the both visible portion to calculate distance error\n",
        "                both_visible_indices = np.logical_and(gt_visibility_mat[i, :] > 0.5, pred_visibility_mat[j, :] > 0.5)\n",
        "                if np.sum(both_visible_indices[:close_range_idx]) > 0:\n",
        "                    x_dist_mat_close[i, j] = np.sum(\n",
        "                        x_dist[:close_range_idx] * both_visible_indices[:close_range_idx]) / np.sum(\n",
        "                        both_visible_indices[:close_range_idx])\n",
        "                    z_dist_mat_close[i, j] = np.sum(\n",
        "                        z_dist[:close_range_idx] * both_visible_indices[:close_range_idx]) / np.sum(\n",
        "                        both_visible_indices[:close_range_idx])\n",
        "                else:\n",
        "                    x_dist_mat_close[i, j] = self.dist_th\n",
        "                    z_dist_mat_close[i, j] = self.dist_th\n",
        "\n",
        "                if np.sum(both_visible_indices[close_range_idx:]) > 0:\n",
        "                    x_dist_mat_far[i, j] = np.sum(\n",
        "                        x_dist[close_range_idx:] * both_visible_indices[close_range_idx:]) / np.sum(\n",
        "                        both_visible_indices[close_range_idx:])\n",
        "                    z_dist_mat_far[i, j] = np.sum(\n",
        "                        z_dist[close_range_idx:] * both_visible_indices[close_range_idx:]) / np.sum(\n",
        "                        both_visible_indices[close_range_idx:])\n",
        "                else:\n",
        "                    x_dist_mat_far[i, j] = self.dist_th\n",
        "                    z_dist_mat_far[i, j] = self.dist_th\n",
        "\n",
        "        # solve bipartite matching vis min cost flow solver\n",
        "        match_results = SolveMinCostFlow(adj_mat, cost_mat)\n",
        "        match_results = np.array(match_results)\n",
        "\n",
        "        # only a match with avg cost < self.dist_th is consider valid one\n",
        "        match_gt_ids = []\n",
        "        match_pred_ids = []\n",
        "        if match_results.shape[0] > 0:\n",
        "            for i in range(len(match_results)):\n",
        "                if match_results[i, 2] < self.dist_th * self.y_samples.shape[0]:\n",
        "                    gt_i = match_results[i, 0]\n",
        "                    pred_i = match_results[i, 1]\n",
        "                    # consider match when the matched points is above a ratio\n",
        "                    if num_match_mat[gt_i, pred_i] / np.sum(gt_visibility_mat[gt_i, :]) >= self.ratio_th:\n",
        "                        r_lane += 1\n",
        "                        match_gt_ids.append(gt_i)\n",
        "                    if num_match_mat[gt_i, pred_i] / np.sum(pred_visibility_mat[pred_i, :]) >= self.ratio_th:\n",
        "                        p_lane += 1\n",
        "                        match_pred_ids.append(pred_i)\n",
        "                    x_error_close.append(x_dist_mat_close[gt_i, pred_i])\n",
        "                    x_error_far.append(x_dist_mat_far[gt_i, pred_i])\n",
        "                    z_error_close.append(z_dist_mat_close[gt_i, pred_i])\n",
        "                    z_error_far.append(z_dist_mat_far[gt_i, pred_i])\n",
        "\n",
        "        # visualize lanelines and matching results both in image and 3D\n",
        "        if vis:\n",
        "            P_g2im = projection_g2im(gt_cam_pitch, gt_cam_height, self.K)\n",
        "            P_gt = np.matmul(self.H_crop, P_g2im)\n",
        "            img = cv2.imread(ops.join(self.dataset_dir, raw_file))\n",
        "            img = cv2.warpPerspective(img, self.H_crop, (self.resize_w, self.resize_h))\n",
        "            img = img.astype(np.float) / 255\n",
        "\n",
        "            for i in range(cnt_gt):\n",
        "                x_values = gt_lanes[i][:, 0]\n",
        "                z_values = gt_lanes[i][:, 1]\n",
        "                x_2d, y_2d = projective_transformation(P_gt, x_values, self.y_samples, z_values)\n",
        "                x_2d = x_2d.astype(np.int)\n",
        "                y_2d = y_2d.astype(np.int)\n",
        "\n",
        "                if i in match_gt_ids:\n",
        "                    color = [0, 0, 1]\n",
        "                else:\n",
        "                    color = [0, 1, 1]\n",
        "                for k in range(1, x_2d.shape[0]):\n",
        "                    # only draw the visible portion\n",
        "                    if gt_visibility_mat[i, k - 1] and gt_visibility_mat[i, k]:\n",
        "                        img = cv2.line(img, (x_2d[k - 1], y_2d[k - 1]), (x_2d[k], y_2d[k]), color[-1::-1], 3)\n",
        "                ax2.plot(x_values[np.where(gt_visibility_mat[i, :])],\n",
        "                         self.y_samples[np.where(gt_visibility_mat[i, :])],\n",
        "                         z_values[np.where(gt_visibility_mat[i, :])], color=color, linewidth=5)\n",
        "\n",
        "            for i in range(cnt_pred):\n",
        "                x_values = pred_lanes[i][:, 0]\n",
        "                z_values = pred_lanes[i][:, 1]\n",
        "                x_2d, y_2d = projective_transformation(P_gt, x_values, self.y_samples, z_values)\n",
        "                x_2d = x_2d.astype(np.int)\n",
        "                y_2d = y_2d.astype(np.int)\n",
        "\n",
        "                if i in match_pred_ids:\n",
        "                    color = [1, 0, 0]\n",
        "                else:\n",
        "                    color = [1, 0, 1]\n",
        "                for k in range(1, x_2d.shape[0]):\n",
        "                    # only draw the visible portion\n",
        "                    if pred_visibility_mat[i, k - 1] and pred_visibility_mat[i, k]:\n",
        "                        img = cv2.line(img, (x_2d[k - 1], y_2d[k - 1]), (x_2d[k], y_2d[k]), color[-1::-1], 2)\n",
        "                ax2.plot(x_values[np.where(pred_visibility_mat[i, :])],\n",
        "                         self.y_samples[np.where(pred_visibility_mat[i, :])],\n",
        "                         z_values[np.where(pred_visibility_mat[i, :])], color=color, linewidth=5)\n",
        "\n",
        "            cv2.putText(img, 'Recall: {:.3f}'.format(r_lane / (cnt_gt + 1e-6)),\n",
        "                        (5, 30), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.7, color=(0, 0, 1), thickness=2)\n",
        "            cv2.putText(img, 'Precision: {:.3f}'.format(p_lane / (cnt_pred + 1e-6)),\n",
        "                        (5, 60), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.7, color=(0, 0, 1), thickness=2)\n",
        "            ax1.imshow(img[:, :, [2, 1, 0]])\n",
        "\n",
        "        return r_lane, p_lane, cnt_gt, cnt_pred, x_error_close, x_error_far, z_error_close, z_error_far\n",
        "\n",
        "    # compare predicted set and ground-truth set using a fixed lane probability threshold\n",
        "    def bench_one_submit(self, pred_file, gt_file, prob_th=0.5, vis=False):\n",
        "        if vis:\n",
        "            save_path = pred_file[:pred_file.rfind('/')]\n",
        "            save_path += '/vis'\n",
        "            if vis and not os.path.exists(save_path):\n",
        "                try:\n",
        "                    os.makedirs(save_path)\n",
        "                except OSError as e:\n",
        "                    print(e.message)\n",
        "        # try:\n",
        "        pred_lines = open(pred_file).readlines()\n",
        "        json_pred = [json.loads(line) for line in pred_lines]\n",
        "        # except BaseException as e:\n",
        "        #     raise Exception('Fail to load json file of the prediction.')\n",
        "        json_gt = [json.loads(line) for line in open(gt_file).readlines()]\n",
        "        if len(json_gt) != len(json_pred):\n",
        "            raise Exception('We do not get the predictions of all the test tasks')\n",
        "        gts = {l['raw_file']: l for l in json_gt}\n",
        "\n",
        "        laneline_stats = []\n",
        "        laneline_x_error_close = []\n",
        "        laneline_x_error_far = []\n",
        "        laneline_z_error_close = []\n",
        "        laneline_z_error_far = []\n",
        "        centerline_stats = []\n",
        "        centerline_x_error_close = []\n",
        "        centerline_x_error_far = []\n",
        "        centerline_z_error_close = []\n",
        "        centerline_z_error_far = []\n",
        "        for i, pred in enumerate(json_pred):\n",
        "            if 'raw_file' not in pred or 'laneLines' not in pred:\n",
        "                raise Exception('raw_file or lanelines not in some predictions.')\n",
        "            raw_file = pred['raw_file']\n",
        "\n",
        "            # if raw_file != 'images/05/0000347.jpg':\n",
        "            #     continue\n",
        "            pred_lanelines = pred['laneLines']\n",
        "            pred_laneLines_prob = pred['laneLines_prob']\n",
        "            pred_lanelines = [pred_lanelines[ii] for ii in range(len(pred_laneLines_prob)) if\n",
        "                              pred_laneLines_prob[ii] > prob_th]\n",
        "\n",
        "            if raw_file not in gts:\n",
        "                raise Exception('Some raw_file from your predictions do not exist in the test tasks.')\n",
        "            gt = gts[raw_file]\n",
        "            gt_cam_height = gt['cam_height']\n",
        "            gt_cam_pitch = gt['cam_pitch']\n",
        "\n",
        "            if vis:\n",
        "                fig = plt.figure()\n",
        "                ax1 = fig.add_subplot(221)\n",
        "                ax2 = fig.add_subplot(222, projection='3d')\n",
        "                ax3 = fig.add_subplot(223)\n",
        "                ax4 = fig.add_subplot(224, projection='3d')\n",
        "            else:\n",
        "                ax1 = 0\n",
        "                ax2 = 0\n",
        "                ax3 = 0\n",
        "                ax4 = 0\n",
        "\n",
        "            # evaluate lanelines\n",
        "            gt_lanelines = gt['laneLines']\n",
        "            gt_visibility = gt['laneLines_visibility']\n",
        "            # N to N matching of lanelines\n",
        "            r_lane, p_lane, cnt_gt, cnt_pred, \\\n",
        "            x_error_close, x_error_far, \\\n",
        "            z_error_close, z_error_far = self.bench(pred_lanelines,\n",
        "                                                    gt_lanelines,\n",
        "                                                    gt_visibility,\n",
        "                                                    raw_file,\n",
        "                                                    gt_cam_height,\n",
        "                                                    gt_cam_pitch,\n",
        "                                                    vis, ax1, ax2)\n",
        "            laneline_stats.append(np.array([r_lane, p_lane, cnt_gt, cnt_pred]))\n",
        "            # consider x_error z_error only for the matched lanes\n",
        "            # if r_lane > 0 and p_lane > 0:\n",
        "            laneline_x_error_close.extend(x_error_close)\n",
        "            laneline_x_error_far.extend(x_error_far)\n",
        "            laneline_z_error_close.extend(z_error_close)\n",
        "            laneline_z_error_far.extend(z_error_far)\n",
        "\n",
        "            # evaluate centerlines\n",
        "            if not self.no_centerline:\n",
        "                pred_centerlines = pred['centerLines']\n",
        "                pred_centerlines_prob = pred['centerLines_prob']\n",
        "                pred_centerlines = [pred_centerlines[ii] for ii in range(len(pred_centerlines_prob)) if\n",
        "                                    pred_centerlines_prob[ii] > prob_th]\n",
        "\n",
        "                gt_centerlines = gt['centerLines']\n",
        "                gt_visibility = gt['centerLines_visibility']\n",
        "\n",
        "                # N to N matching of lanelines\n",
        "                r_lane, p_lane, cnt_gt, cnt_pred, \\\n",
        "                x_error_close, x_error_far, \\\n",
        "                z_error_close, z_error_far = self.bench(pred_centerlines,\n",
        "                                                        gt_centerlines,\n",
        "                                                        gt_visibility,\n",
        "                                                        raw_file,\n",
        "                                                        gt_cam_height,\n",
        "                                                        gt_cam_pitch,\n",
        "                                                        vis, ax3, ax4)\n",
        "                centerline_stats.append(np.array([r_lane, p_lane, cnt_gt, cnt_pred]))\n",
        "                # consider x_error z_error only for the matched lanes\n",
        "                # if r_lane > 0 and p_lane > 0:\n",
        "                centerline_x_error_close.extend(x_error_close)\n",
        "                centerline_x_error_far.extend(x_error_far)\n",
        "                centerline_z_error_close.extend(z_error_close)\n",
        "                centerline_z_error_far.extend(z_error_far)\n",
        "\n",
        "            if vis:\n",
        "                ax1.set_xticks([])\n",
        "                ax1.set_yticks([])\n",
        "                # ax2.set_xlabel('x axis')\n",
        "                # ax2.set_ylabel('y axis')\n",
        "                # ax2.set_zlabel('z axis')\n",
        "                bottom, top = ax2.get_zlim()\n",
        "                left, right = ax2.get_xlim()\n",
        "                ax2.set_zlim(min(bottom, -0.1), max(top, 0.1))\n",
        "                ax2.set_xlim(left, right)\n",
        "                ax2.set_ylim(vis_min_y, vis_max_y)\n",
        "                ax2.locator_params(nbins=5, axis='x')\n",
        "                ax2.locator_params(nbins=5, axis='z')\n",
        "                ax2.tick_params(pad=18)\n",
        "\n",
        "                ax3.set_xticks([])\n",
        "                ax3.set_yticks([])\n",
        "                # ax4.set_xlabel('x axis')\n",
        "                # ax4.set_ylabel('y axis')\n",
        "                # ax4.set_zlabel('z axis')\n",
        "                bottom, top = ax4.get_zlim()\n",
        "                left, right = ax4.get_xlim()\n",
        "                ax4.set_zlim(min(bottom, -0.1), max(top, 0.1))\n",
        "                ax4.set_xlim(left, right)\n",
        "                ax4.set_ylim(vis_min_y, vis_max_y)\n",
        "                ax4.locator_params(nbins=5, axis='x')\n",
        "                ax4.locator_params(nbins=5, axis='z')\n",
        "                ax4.tick_params(pad=18)\n",
        "\n",
        "                fig.subplots_adjust(wspace=0, hspace=0.01)\n",
        "                fig.savefig(ops.join(save_path, raw_file.replace(\"/\", \"_\")))\n",
        "                plt.close(fig)\n",
        "                print('processed sample: {}  {}'.format(i, raw_file))\n",
        "\n",
        "        output_stats = []\n",
        "        laneline_stats = np.array(laneline_stats)\n",
        "        laneline_x_error_close = np.array(laneline_x_error_close)\n",
        "        laneline_x_error_far = np.array(laneline_x_error_far)\n",
        "        laneline_z_error_close = np.array(laneline_z_error_close)\n",
        "        laneline_z_error_far = np.array(laneline_z_error_far)\n",
        "\n",
        "        R_lane = np.sum(laneline_stats[:, 0]) / (np.sum(laneline_stats[:, 2]) + 1e-6)\n",
        "        P_lane = np.sum(laneline_stats[:, 1]) / (np.sum(laneline_stats[:, 3]) + 1e-6)\n",
        "        F_lane = 2 * R_lane * P_lane / (R_lane + P_lane + 1e-6)\n",
        "        x_error_close_avg = np.average(laneline_x_error_close)\n",
        "        x_error_far_avg = np.average(laneline_x_error_far)\n",
        "        z_error_close_avg = np.average(laneline_z_error_close)\n",
        "        z_error_far_avg = np.average(laneline_z_error_far)\n",
        "\n",
        "        output_stats.append(F_lane)\n",
        "        output_stats.append(R_lane)\n",
        "        output_stats.append(P_lane)\n",
        "        output_stats.append(x_error_close_avg)\n",
        "        output_stats.append(x_error_far_avg)\n",
        "        output_stats.append(z_error_close_avg)\n",
        "        output_stats.append(z_error_far_avg)\n",
        "\n",
        "        if not self.no_centerline:\n",
        "            centerline_stats = np.array(centerline_stats)\n",
        "            centerline_x_error_close = np.array(centerline_x_error_close)\n",
        "            centerline_x_error_far = np.array(centerline_x_error_far)\n",
        "            centerline_z_error_close = np.array(centerline_z_error_close)\n",
        "            centerline_z_error_far = np.array(centerline_z_error_far)\n",
        "\n",
        "            R_lane = np.sum(centerline_stats[:, 0]) / (np.sum(centerline_stats[:, 2]) + 1e-6)\n",
        "            P_lane = np.sum(centerline_stats[:, 1]) / (np.sum(centerline_stats[:, 3]) + 1e-6)\n",
        "            F_lane = 2 * R_lane * P_lane / (R_lane + P_lane + 1e-6)\n",
        "            x_error_close_avg = np.average(centerline_x_error_close)\n",
        "            x_error_far_avg = np.average(centerline_x_error_far)\n",
        "            z_error_close_avg = np.average(centerline_z_error_close)\n",
        "            z_error_far_avg = np.average(centerline_z_error_far)\n",
        "\n",
        "            output_stats.append(F_lane)\n",
        "            output_stats.append(R_lane)\n",
        "            output_stats.append(P_lane)\n",
        "            output_stats.append(x_error_close_avg)\n",
        "            output_stats.append(x_error_far_avg)\n",
        "            output_stats.append(z_error_close_avg)\n",
        "            output_stats.append(z_error_far_avg)\n",
        "\n",
        "        return output_stats\n",
        "\n",
        "    def bench_PR(self, pred_lanes, gt_lanes, gt_visibility):\n",
        "        \"\"\"\n",
        "            Matching predicted lanes and ground-truth lanes in their IPM projection, ignoring z attributes.\n",
        "            x error, y_error, and z error are all considered, although the matching does not rely on z\n",
        "            The input of prediction and ground-truth lanes are in ground coordinate, x-right, y-forward, z-up\n",
        "            The fundamental assumption is: 1. there are no two points from different lanes with identical x, y\n",
        "                                              but different z's\n",
        "                                           2. there are no two points from a single lane having identical x, y\n",
        "                                              but different z's\n",
        "            If the interest area is within the current drivable road, the above assumptions are almost always valid.\n",
        "        :param pred_lanes: N X 2 or N X 3 lists depending on 2D or 3D\n",
        "        :param gt_lanes: N X 2 or N X 3 lists depending on 2D or 3D\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        r_lane, p_lane = 0., 0.\n",
        "\n",
        "        # only keep the visible portion\n",
        "        gt_lanes = [prune_3d_lane_by_visibility(np.array(gt_lane), np.array(gt_visibility[k])) for k, gt_lane in\n",
        "                    enumerate(gt_lanes)]\n",
        "        gt_lanes = [lane for lane in gt_lanes if lane.shape[0] > 1]\n",
        "        # only consider those gt lanes overlapping with sampling range\n",
        "        gt_lanes = [lane for lane in gt_lanes if lane[0, 1] < self.y_samples[-1] and lane[-1, 1] > self.y_samples[0]]\n",
        "        gt_lanes = [prune_3d_lane_by_range(np.array(gt_lane), 3 * self.x_min, 3 * self.x_max) for gt_lane in gt_lanes]\n",
        "        gt_lanes = [lane for lane in gt_lanes if lane.shape[0] > 1]\n",
        "        cnt_gt = len(gt_lanes)\n",
        "        cnt_pred = len(pred_lanes)\n",
        "\n",
        "        gt_visibility_mat = np.zeros((cnt_gt, 100))\n",
        "        pred_visibility_mat = np.zeros((cnt_pred, 100))\n",
        "        # resample gt and pred at y_samples\n",
        "        for i in range(cnt_gt):\n",
        "            min_y = np.min(np.array(gt_lanes[i])[:, 1])\n",
        "            max_y = np.max(np.array(gt_lanes[i])[:, 1])\n",
        "            x_values, z_values, visibility_vec = resample_laneline_in_y(np.array(gt_lanes[i]), self.y_samples,\n",
        "                                                                        out_vis=True)\n",
        "            gt_lanes[i] = np.vstack([x_values, z_values]).T\n",
        "            gt_visibility_mat[i, :] = np.logical_and(x_values >= self.x_min,\n",
        "                                                     np.logical_and(x_values <= self.x_max,\n",
        "                                                                    np.logical_and(self.y_samples >= min_y,\n",
        "                                                                                   self.y_samples <= max_y)))\n",
        "            gt_visibility_mat[i, :] = np.logical_and(gt_visibility_mat[i, :], visibility_vec)\n",
        "\n",
        "        for i in range(cnt_pred):\n",
        "            # # ATTENTION: ensure y mono increase before interpolation: but it can reduce size\n",
        "            # pred_lanes[i] = make_lane_y_mono_inc(np.array(pred_lanes[i]))\n",
        "            # pred_lane = prune_3d_lane_by_range(np.array(pred_lanes[i]), self.x_min, self.x_max)\n",
        "            min_y = np.min(np.array(pred_lanes[i])[:, 1])\n",
        "            max_y = np.max(np.array(pred_lanes[i])[:, 1])\n",
        "            x_values, z_values, visibility_vec = resample_laneline_in_y(np.array(pred_lanes[i]), self.y_samples,\n",
        "                                                                        out_vis=True)\n",
        "            pred_lanes[i] = np.vstack([x_values, z_values]).T\n",
        "            pred_visibility_mat[i, :] = np.logical_and(x_values >= self.x_min,\n",
        "                                                       np.logical_and(x_values <= self.x_max,\n",
        "                                                                      np.logical_and(self.y_samples >= min_y,\n",
        "                                                                                     self.y_samples <= max_y)))\n",
        "            pred_visibility_mat[i, :] = np.logical_and(pred_visibility_mat[i, :], visibility_vec)\n",
        "            # pred_visibility_mat[i, :] = np.logical_and(x_values >= self.x_min, x_values <= self.x_max)\n",
        "\n",
        "        adj_mat = np.zeros((cnt_gt, cnt_pred), dtype=np.int)\n",
        "        cost_mat = np.zeros((cnt_gt, cnt_pred), dtype=np.int)\n",
        "        cost_mat.fill(1000)\n",
        "        num_match_mat = np.zeros((cnt_gt, cnt_pred), dtype=np.float)\n",
        "        # compute curve to curve distance\n",
        "        for i in range(cnt_gt):\n",
        "            for j in range(cnt_pred):\n",
        "                x_dist = np.abs(gt_lanes[i][:, 0] - pred_lanes[j][:, 0])\n",
        "                z_dist = np.abs(gt_lanes[i][:, 1] - pred_lanes[j][:, 1])\n",
        "                euclidean_dist = np.sqrt(x_dist ** 2 + z_dist ** 2)\n",
        "\n",
        "                # apply visibility to penalize different partial matching accordingly\n",
        "                euclidean_dist[\n",
        "                    np.logical_or(gt_visibility_mat[i, :] < 0.5, pred_visibility_mat[j, :] < 0.5)] = self.dist_th\n",
        "\n",
        "                # if np.average(euclidean_dist) < 2*self.dist_th: # don't prune here to encourage finding perfect match\n",
        "                num_match_mat[i, j] = np.sum(euclidean_dist < self.dist_th)\n",
        "                adj_mat[i, j] = 1\n",
        "                # ATTENTION: use the sum as int type to meet the requirements of min cost flow optimization (int type)\n",
        "                # why using num_match_mat as cost does not work?\n",
        "                cost_mat[i, j] = np.sum(euclidean_dist).astype(np.int)\n",
        "                # cost_mat[i, j] = num_match_mat[i, j]\n",
        "\n",
        "        # solve bipartite matching vis min cost flow solver\n",
        "        match_results = SolveMinCostFlow(adj_mat, cost_mat)\n",
        "        match_results = np.array(match_results)\n",
        "\n",
        "        # only a match with avg cost < self.dist_th is consider valid one\n",
        "        match_gt_ids = []\n",
        "        match_pred_ids = []\n",
        "        if match_results.shape[0] > 0:\n",
        "            for i in range(len(match_results)):\n",
        "                if match_results[i, 2] < self.dist_th * self.y_samples.shape[0]:\n",
        "                    gt_i = match_results[i, 0]\n",
        "                    pred_i = match_results[i, 1]\n",
        "                    # consider match when the matched points is above a ratio\n",
        "                    if num_match_mat[gt_i, pred_i] / np.sum(gt_visibility_mat[gt_i, :]) >= self.ratio_th:\n",
        "                        r_lane += 1\n",
        "                        match_gt_ids.append(gt_i)\n",
        "                    if num_match_mat[gt_i, pred_i] / np.sum(pred_visibility_mat[pred_i, :]) >= self.ratio_th:\n",
        "                        p_lane += 1\n",
        "                        match_pred_ids.append(pred_i)\n",
        "\n",
        "        return r_lane, p_lane, cnt_gt, cnt_pred\n",
        "\n",
        "    # evaluate two dataset at varying lane probability threshold to calculate AP\n",
        "    def bench_one_submit_varying_probs(self, pred_file, gt_file, eval_out_file=None, eval_fig_file=None):\n",
        "        varying_th = np.linspace(0.05, 0.95, 19)\n",
        "        # try:\n",
        "        pred_lines = open(pred_file).readlines()\n",
        "        json_pred = [json.loads(line) for line in pred_lines]\n",
        "        # except BaseException as e:\n",
        "        #     raise Exception('Fail to load json file of the prediction.')\n",
        "        json_gt = [json.loads(line) for line in open(gt_file).readlines()]\n",
        "        if len(json_gt) != len(json_pred):\n",
        "            raise Exception('We do not get the predictions of all the test tasks')\n",
        "        gts = {l['raw_file']: l for l in json_gt}\n",
        "\n",
        "        laneline_r_all = []\n",
        "        laneline_p_all = []\n",
        "        laneline_gt_cnt_all = []\n",
        "        laneline_pred_cnt_all = []\n",
        "        centerline_r_all = []\n",
        "        centerline_p_all = []\n",
        "        centerline_gt_cnt_all = []\n",
        "        centerline_pred_cnt_all = []\n",
        "        for i, pred in enumerate(json_pred):\n",
        "            print('Evaluating sample {} / {}'.format(i, len(json_pred)))\n",
        "            if 'raw_file' not in pred or 'laneLines' not in pred:\n",
        "                raise Exception('raw_file or lanelines not in some predictions.')\n",
        "            raw_file = pred['raw_file']\n",
        "\n",
        "            pred_lanelines = pred['laneLines']\n",
        "            pred_laneLines_prob = pred['laneLines_prob']\n",
        "            if raw_file not in gts:\n",
        "                raise Exception('Some raw_file from your predictions do not exist in the test tasks.')\n",
        "            gt = gts[raw_file]\n",
        "            gt_cam_height = gt['cam_height']\n",
        "            gt_cam_pitch = gt['cam_pitch']\n",
        "\n",
        "            # evaluate lanelines\n",
        "            gt_lanelines = gt['laneLines']\n",
        "            gt_visibility = gt['laneLines_visibility']\n",
        "            r_lane_vec = []\n",
        "            p_lane_vec = []\n",
        "            cnt_gt_vec = []\n",
        "            cnt_pred_vec = []\n",
        "\n",
        "            for prob_th in varying_th:\n",
        "                pred_lanelines = [pred_lanelines[ii] for ii in range(len(pred_laneLines_prob)) if\n",
        "                                  pred_laneLines_prob[ii] > prob_th]\n",
        "                pred_laneLines_prob = [prob for prob in pred_laneLines_prob if prob > prob_th]\n",
        "                pred_lanelines_copy = copy.deepcopy(pred_lanelines)\n",
        "                # N to N matching of lanelines\n",
        "                r_lane, p_lane, cnt_gt, cnt_pred = self.bench_PR(pred_lanelines_copy,\n",
        "                                                                 gt_lanelines,\n",
        "                                                                 gt_visibility)\n",
        "                r_lane_vec.append(r_lane)\n",
        "                p_lane_vec.append(p_lane)\n",
        "                cnt_gt_vec.append(cnt_gt)\n",
        "                cnt_pred_vec.append(cnt_pred)\n",
        "\n",
        "            laneline_r_all.append(r_lane_vec)\n",
        "            laneline_p_all.append(p_lane_vec)\n",
        "            laneline_gt_cnt_all.append(cnt_gt_vec)\n",
        "            laneline_pred_cnt_all.append(cnt_pred_vec)\n",
        "\n",
        "            # evaluate centerlines\n",
        "            if not self.no_centerline:\n",
        "                pred_centerlines = pred['centerLines']\n",
        "                pred_centerLines_prob = pred['centerLines_prob']\n",
        "                gt_centerlines = gt['centerLines']\n",
        "                gt_visibility = gt['centerLines_visibility']\n",
        "                r_lane_vec = []\n",
        "                p_lane_vec = []\n",
        "                cnt_gt_vec = []\n",
        "                cnt_pred_vec = []\n",
        "\n",
        "                for prob_th in varying_th:\n",
        "                    pred_centerlines = [pred_centerlines[ii] for ii in range(len(pred_centerLines_prob)) if\n",
        "                                        pred_centerLines_prob[ii] > prob_th]\n",
        "                    pred_centerLines_prob = [prob for prob in pred_centerLines_prob if prob > prob_th]\n",
        "                    pred_centerlines_copy = copy.deepcopy(pred_centerlines)\n",
        "                    # N to N matching of lanelines\n",
        "                    r_lane, p_lane, cnt_gt, cnt_pred = self.bench_PR(pred_centerlines_copy,\n",
        "                                                                     gt_centerlines,\n",
        "                                                                     gt_visibility)\n",
        "                    r_lane_vec.append(r_lane)\n",
        "                    p_lane_vec.append(p_lane)\n",
        "                    cnt_gt_vec.append(cnt_gt)\n",
        "                    cnt_pred_vec.append(cnt_pred)\n",
        "                centerline_r_all.append(r_lane_vec)\n",
        "                centerline_p_all.append(p_lane_vec)\n",
        "                centerline_gt_cnt_all.append(cnt_gt_vec)\n",
        "                centerline_pred_cnt_all.append(cnt_pred_vec)\n",
        "\n",
        "        output_stats = []\n",
        "        # compute precision, recall\n",
        "        laneline_r_all = np.array(laneline_r_all)\n",
        "        laneline_p_all = np.array(laneline_p_all)\n",
        "        laneline_gt_cnt_all = np.array(laneline_gt_cnt_all)\n",
        "        laneline_pred_cnt_all = np.array(laneline_pred_cnt_all)\n",
        "\n",
        "        R_lane = np.sum(laneline_r_all, axis=0) / (np.sum(laneline_gt_cnt_all, axis=0) + 1e-6)\n",
        "        P_lane = np.sum(laneline_p_all, axis=0) / (np.sum(laneline_pred_cnt_all, axis=0) + 1e-6)\n",
        "        F_lane = 2 * R_lane * P_lane / (R_lane + P_lane + 1e-6)\n",
        "\n",
        "        output_stats.append(F_lane)\n",
        "        output_stats.append(R_lane)\n",
        "        output_stats.append(P_lane)\n",
        "\n",
        "        if not self.no_centerline:\n",
        "            centerline_r_all = np.array(centerline_r_all)\n",
        "            centerline_p_all = np.array(centerline_p_all)\n",
        "            centerline_gt_cnt_all = np.array(centerline_gt_cnt_all)\n",
        "            centerline_pred_cnt_all = np.array(centerline_pred_cnt_all)\n",
        "\n",
        "            R_lane = np.sum(centerline_r_all, axis=0) / (np.sum(centerline_gt_cnt_all, axis=0) + 1e-6)\n",
        "            P_lane = np.sum(centerline_p_all, axis=0) / (np.sum(centerline_pred_cnt_all, axis=0) + 1e-6)\n",
        "            F_lane = 2 * R_lane * P_lane / (R_lane + P_lane + 1e-6)\n",
        "\n",
        "            output_stats.append(F_lane)\n",
        "            output_stats.append(R_lane)\n",
        "            output_stats.append(P_lane)\n",
        "\n",
        "        # calculate metrics\n",
        "        laneline_F = output_stats[0]\n",
        "        laneline_F_max = np.max(laneline_F)\n",
        "        laneline_max_i = np.argmax(laneline_F)\n",
        "        laneline_R = output_stats[1]\n",
        "        laneline_P = output_stats[2]\n",
        "        centerline_F = output_stats[3]\n",
        "        centerline_F_max = centerline_F[laneline_max_i]\n",
        "        centerline_max_i = laneline_max_i\n",
        "        centerline_R = output_stats[4]\n",
        "        centerline_P = output_stats[5]\n",
        "\n",
        "        laneline_R = np.array([1.] + laneline_R.tolist() + [0.])\n",
        "        laneline_P = np.array([0.] + laneline_P.tolist() + [1.])\n",
        "        centerline_R = np.array([1.] + centerline_R.tolist() + [0.])\n",
        "        centerline_P = np.array([0.] + centerline_P.tolist() + [1.])\n",
        "        f_laneline = interp1d(laneline_R, laneline_P)\n",
        "        f_centerline = interp1d(centerline_R, centerline_P)\n",
        "        r_range = np.linspace(0.05, 0.95, 19)\n",
        "        laneline_AP = np.mean(f_laneline(r_range))\n",
        "        centerline_AP = np.mean(f_centerline(r_range))\n",
        "\n",
        "        if eval_fig_file is not None:\n",
        "            # plot PR curve\n",
        "            fig = plt.figure()\n",
        "            ax1 = fig.add_subplot(121)\n",
        "            ax2 = fig.add_subplot(122)\n",
        "            ax1.plot(laneline_R, laneline_P, '-s')\n",
        "            ax2.plot(centerline_R, centerline_P, '-s')\n",
        "\n",
        "            ax1.set_xlim(0, 1)\n",
        "            ax1.set_ylim(0, 1)\n",
        "            ax1.set_title('Lane Line')\n",
        "            ax1.set_xlabel('Recall')\n",
        "            ax1.set_ylabel('Precision')\n",
        "            ax1.set_aspect('equal')\n",
        "            ax1.legend('Max F-measure {:.3}'.format(laneline_F_max))\n",
        "\n",
        "            ax2.set_xlim(0, 1)\n",
        "            ax2.set_ylim(0, 1)\n",
        "            ax2.set_title('Center Line')\n",
        "            ax2.set_xlabel('Recall')\n",
        "            ax2.set_ylabel('Precision')\n",
        "            ax2.set_aspect('equal')\n",
        "            ax2.legend('Max F-measure {:.3}'.format(centerline_F_max))\n",
        "\n",
        "            # fig.subplots_adjust(wspace=0.1, hspace=0.01)\n",
        "            fig.savefig(eval_fig_file)\n",
        "            plt.close(fig)\n",
        "\n",
        "        # print(\"===> Evaluation on validation set: \\n\"\n",
        "        #       \"laneline max F-measure {:.3} at Recall {:.3}, Precision {:.3} \\n\"\n",
        "        #       \"laneline AP: {:.3}\\n\"\n",
        "        #       \"centerline max F-measure {:.3} at Recall {:.3}, Precision {:.3} \\n\"\n",
        "        #       \"centerline AP: {:.3} \\n\".format(laneline_F_max,\n",
        "        #                                        laneline_R[laneline_max_i + 1],\n",
        "        #                                        laneline_P[laneline_max_i + 1],\n",
        "        #                                        laneline_AP,\n",
        "        #                                        centerline_F_max,\n",
        "        #                                        centerline_R[centerline_max_i + 1],\n",
        "        #                                        centerline_P[centerline_max_i + 1],\n",
        "        #                                        centerline_AP))\n",
        "\n",
        "        json_out = {}\n",
        "        json_out['laneline_R'] = laneline_R[1:-1].astype(np.float32).tolist()\n",
        "        json_out['laneline_P'] = laneline_P[1:-1].astype(np.float32).tolist()\n",
        "        json_out['laneline_F_max'] = laneline_F_max\n",
        "        json_out['laneline_max_i'] = laneline_max_i.tolist()\n",
        "        json_out['laneline_AP'] = laneline_AP\n",
        "\n",
        "        json_out['centerline_R'] = centerline_R[1:-1].astype(np.float32).tolist()\n",
        "        json_out['centerline_P'] = centerline_P[1:-1].astype(np.float32).tolist()\n",
        "        json_out['centerline_F_max'] = centerline_F_max\n",
        "        json_out['centerline_max_i'] = centerline_max_i.tolist()\n",
        "        json_out['centerline_AP'] = centerline_AP\n",
        "\n",
        "        json_out['max_F_prob_th'] = varying_th[laneline_max_i]\n",
        "\n",
        "        if eval_out_file is not None:\n",
        "            with open(eval_out_file, 'w') as jsonFile:\n",
        "                jsonFile.write(json.dumps(json_out))\n",
        "                jsonFile.write('\\n')\n",
        "                jsonFile.close()\n",
        "        return json_out\n",
        "\n",
        "'''\n",
        "if __name__ == '__main__':\n",
        "    vis = False\n",
        "    args = define_args()\n",
        "    #args = parser.parse_args()\n",
        "\n",
        "    # two method are compared: '3D_LaneNet' and 'Gen_LaneNet'\n",
        "    method_name = 'Gen_LaneNet_ext'\n",
        "\n",
        "    # Three different splits of datasets: 'standard', 'rare_subsit', 'illus_chg'\n",
        "    data_split = 'illus_chg'\n",
        "\n",
        "    # location where the original dataset is saved. Image will be loaded in case of visualization\n",
        "    args.dataset_dir = '/content/drive/Shareddrives/colab/Apollo_Sim_3D_Lane_Release' ##YOU NEED TO EDIT THIS DIRECTORY\n",
        "\n",
        "    # load configuration for certain dataset\n",
        "    sim3d_config(args)\n",
        "\n",
        "    # auto-file in dependent paths\n",
        "    base_dir='/content/drive/Shareddrives/colab/'\n",
        "    gt_file = base_dir+'data_splits/' + data_split + '/test.json'\n",
        "    pred_folder = base_dir+'data_splits/' + data_split + '/' + method_name\n",
        "    pred_file = pred_folder + '/test_pred_file.json'\n",
        "\n",
        "    # Initialize evaluator\n",
        "    evaluator = LaneEval(args)\n",
        "\n",
        "    # evaluation at varying thresholds\n",
        "    eval_stats_pr = evaluator.bench_one_submit_varying_probs(pred_file, gt_file)\n",
        "    max_f_prob = eval_stats_pr['max_F_prob_th']\n",
        "\n",
        "    # evaluate at the point with max F-measure. Additional eval of position error. Option to visualize matching result\n",
        "    eval_stats = evaluator.bench_one_submit(pred_file, gt_file, prob_th=max_f_prob, vis=vis)\n",
        "\n",
        "    print(\"Metrics: AP, F-score, x error (close), x error (far), z error (close), z error (far)\")\n",
        "    print(\n",
        "        \"Laneline:  {:.3}  {:.3}  {:.3}  {:.3}  {:.3}  {:.3}\".format(eval_stats_pr['laneline_AP'], eval_stats[0],\n",
        "                                                                     eval_stats[3], eval_stats[4],\n",
        "                                                                     eval_stats[5], eval_stats[6]))\n",
        "    print(\"Centerline:  {:.3}  {:.3}  {:.3}  {:.3}  {:.3}  {:.3}\".format(eval_stats_pr['centerline_AP'], eval_stats[7],\n",
        "                                                                         eval_stats[10], eval_stats[11],\n",
        "                                                                         eval_stats[12], eval_stats[13]))\n",
        "                                                                         '''"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nif __name__ == \\'__main__\\':\\n    vis = False\\n    args = define_args()\\n    #args = parser.parse_args()\\n\\n    # two method are compared: \\'3D_LaneNet\\' and \\'Gen_LaneNet\\'\\n    method_name = \\'Gen_LaneNet_ext\\'\\n\\n    # Three different splits of datasets: \\'standard\\', \\'rare_subsit\\', \\'illus_chg\\'\\n    data_split = \\'illus_chg\\'\\n\\n    # location where the original dataset is saved. Image will be loaded in case of visualization\\n    args.dataset_dir = \\'/content/drive/Shareddrives/colab/Apollo_Sim_3D_Lane_Release\\' ##YOU NEED TO EDIT THIS DIRECTORY\\n\\n    # load configuration for certain dataset\\n    sim3d_config(args)\\n\\n    # auto-file in dependent paths\\n    base_dir=\\'/content/drive/Shareddrives/colab/\\'\\n    gt_file = base_dir+\\'data_splits/\\' + data_split + \\'/test.json\\'\\n    pred_folder = base_dir+\\'data_splits/\\' + data_split + \\'/\\' + method_name\\n    pred_file = pred_folder + \\'/test_pred_file.json\\'\\n\\n    # Initialize evaluator\\n    evaluator = LaneEval(args)\\n\\n    # evaluation at varying thresholds\\n    eval_stats_pr = evaluator.bench_one_submit_varying_probs(pred_file, gt_file)\\n    max_f_prob = eval_stats_pr[\\'max_F_prob_th\\']\\n\\n    # evaluate at the point with max F-measure. Additional eval of position error. Option to visualize matching result\\n    eval_stats = evaluator.bench_one_submit(pred_file, gt_file, prob_th=max_f_prob, vis=vis)\\n\\n    print(\"Metrics: AP, F-score, x error (close), x error (far), z error (close), z error (far)\")\\n    print(\\n        \"Laneline:  {:.3}  {:.3}  {:.3}  {:.3}  {:.3}  {:.3}\".format(eval_stats_pr[\\'laneline_AP\\'], eval_stats[0],\\n                                                                     eval_stats[3], eval_stats[4],\\n                                                                     eval_stats[5], eval_stats[6]))\\n    print(\"Centerline:  {:.3}  {:.3}  {:.3}  {:.3}  {:.3}  {:.3}\".format(eval_stats_pr[\\'centerline_AP\\'], eval_stats[7],\\n                                                                         eval_stats[10], eval_stats[11],\\n                                                                         eval_stats[12], eval_stats[13]))\\n                                                                         '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "FPauk0l50MWN",
        "outputId": "2f3420a1-59b6-4444-a31d-5910c24cd177"
      },
      "source": [
        "\"\"\"\n",
        "/dataloader/Load_Data_3DLane_ext.py /\n",
        "\n",
        "Dataloader for networks integrated with the new geometry-guided anchor design proposed in Gen-LaneNet:\n",
        "    \"Gen-laneNet: a generalized and scalable approach for 3D lane detection\", Y.Guo, etal., arxiv 2020\n",
        "Author: Yuliang Guo (33yuliangguo@gmail.com)\n",
        "Date: March, 2020\n",
        "\"\"\"\n",
        "\n",
        "import copy\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from PIL import Image, ImageOps\n",
        "import json\n",
        "import random\n",
        "import warnings\n",
        "import torchvision.transforms.functional as Q\n",
        "#from tools.utils import *\n",
        "warnings.simplefilter('ignore', np.RankWarning)\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "\n",
        "class LaneDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset with labeled lanes\n",
        "        This implementation considers:\n",
        "        w/o laneline 3D attributes\n",
        "        w/o centerline annotations\n",
        "        default considers 3D laneline, including centerlines\n",
        "        This new version of data loader prepare ground-truth anchor tensor in flat ground space.\n",
        "        It is assumed the dataset provides accurate visibility labels. Preparing ground-truth tensor depends on it.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset_base_dir, json_file_path, args, data_aug=False, save_std=False):\n",
        "        \"\"\"\n",
        "        :param dataset_info_file: json file list\n",
        "        \"\"\"\n",
        "        # define image pre-processor\n",
        "        self.totensor = transforms.ToTensor()\n",
        "        self.normalize = transforms.Normalize(args.vgg_mean, args.vgg_std)\n",
        "        self.data_aug = data_aug\n",
        "\n",
        "        # dataset parameters\n",
        "        self.dataset_name = args.dataset_name\n",
        "        self.no_3d = args.no_3d\n",
        "        self.no_centerline = args.no_centerline\n",
        "\n",
        "        self.h_org = args.org_h\n",
        "        self.w_org = args.org_w\n",
        "        self.h_crop = args.crop_y\n",
        "\n",
        "        # parameters related to service network\n",
        "        self.h_net = args.resize_h\n",
        "        self.w_net = args.resize_w\n",
        "        self.ipm_h = args.ipm_h\n",
        "        self.ipm_w = args.ipm_w\n",
        "        # self.x_ratio = float(self.w_net) / float(self.w_org)\n",
        "        # self.y_ratio = float(self.h_net) / float(self.h_org - self.h_crop)\n",
        "        self.top_view_region = args.top_view_region\n",
        "\n",
        "        self.K = args.K\n",
        "        self.H_crop = homography_crop_resize([args.org_h, args.org_w], args.crop_y, [args.resize_h, args.resize_w])\n",
        "        # transformation from ipm to ground region\n",
        "        self.H_ipm2g = cv2.getPerspectiveTransform(np.float32([[0, 0],\n",
        "                                                               [self.ipm_w-1, 0],\n",
        "                                                               [0, self.ipm_h-1],\n",
        "                                                               [self.ipm_w-1, self.ipm_h-1]]),\n",
        "                                                   np.float32(args.top_view_region))\n",
        "        # self.H_g2ipm = np.linalg.inv(H_ipm2g)\n",
        "\n",
        "        if args.fix_cam:\n",
        "            self.fix_cam = True\n",
        "            # compute the homography between image and IPM, and crop transformation\n",
        "            self.cam_height = args.cam_height\n",
        "            self.cam_pitch = np.pi / 180 * args.pitch\n",
        "            self.P_g2im = projection_g2im(self.cam_pitch, self.cam_height, args.K)\n",
        "            self.H_g2im = homograpthy_g2im(self.cam_pitch, self.cam_height, args.K)\n",
        "            self.H_im2g = np.linalg.inv(self.H_g2im)\n",
        "            self.H_im2ipm = np.linalg.inv(np.matmul(self.H_crop, np.matmul(self.H_g2im, self.H_ipm2g)))\n",
        "        else:\n",
        "            self.fix_cam = False\n",
        "\n",
        "        # compute anchor steps\n",
        "        x_min = self.top_view_region[0, 0]\n",
        "        x_max = self.top_view_region[1, 0]\n",
        "        self.x_min = x_min\n",
        "        self.x_max = x_max\n",
        "        self.anchor_x_steps = np.linspace(x_min, x_max, np.int(args.ipm_w/8), endpoint=True)\n",
        "        self.anchor_y_steps = args.anchor_y_steps\n",
        "        self.num_y_steps = len(self.anchor_y_steps)\n",
        "\n",
        "        if self.no_centerline:\n",
        "            self.num_types = 1\n",
        "        else:\n",
        "            self.num_types = 3\n",
        "\n",
        "        if self.no_3d:\n",
        "            self.anchor_dim = self.num_y_steps + 1\n",
        "        else:\n",
        "            self.anchor_dim = 3 * args.num_y_steps + 1\n",
        "\n",
        "        self.y_ref = args.y_ref\n",
        "        self.ref_id = np.argmin(np.abs(self.num_y_steps - self.y_ref))\n",
        "\n",
        "        # parse ground-truth file\n",
        "        if 'tusimple' in self.dataset_name:\n",
        "            self._label_image_path,\\\n",
        "                self._label_laneline_all_org, \\\n",
        "                self._label_laneline_all, \\\n",
        "                self._laneline_ass_ids, \\\n",
        "                self._x_off_std,\\\n",
        "                self._gt_laneline_visibility_all = self.init_dataset_tusimple(dataset_base_dir, json_file_path)\n",
        "        else:  # assume loading apollo sim 3D lane\n",
        "            self._label_image_path, \\\n",
        "                self._label_laneline_all_org, \\\n",
        "                self._label_laneline_all, \\\n",
        "                self._label_centerline_all, \\\n",
        "                self._label_cam_height_all, \\\n",
        "                self._label_cam_pitch_all, \\\n",
        "                self._laneline_ass_ids, \\\n",
        "                self._centerline_ass_ids, \\\n",
        "                self._x_off_std, \\\n",
        "                self._y_off_std, \\\n",
        "                self._z_std, \\\n",
        "                self._gt_laneline_visibility_all, \\\n",
        "                self._gt_centerline_visibility_all = self.init_dataset_3D(dataset_base_dir, json_file_path)\n",
        "        self.n_samples = self._label_image_path.shape[0]\n",
        "\n",
        "        if save_std is True:\n",
        "            with open(ops.join(args.data_dir, 'geo_anchor_std.json'), 'w') as jsonFile:\n",
        "                json_out = {}\n",
        "                json_out[\"x_off_std\"] = self._x_off_std.tolist()\n",
        "                json_out[\"z_std\"] = self._z_std.tolist()\n",
        "                json.dump(json_out, jsonFile)\n",
        "                jsonFile.write('\\n')\n",
        "        # # normalize label values: manual execute in main function, in case overwriting stds is needed\n",
        "        # self.normalize_lane_label()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Conventional len method\n",
        "        \"\"\"\n",
        "        return self.n_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Args: idx (int): Index in list to load image\n",
        "        \"\"\"\n",
        "\n",
        "        # fetch camera height and pitch\n",
        "        if not self.fix_cam:\n",
        "            gt_cam_height = self._label_cam_height_all[idx]\n",
        "            gt_cam_pitch = self._label_cam_pitch_all[idx]\n",
        "        else:\n",
        "            gt_cam_height = self.cam_height\n",
        "            gt_cam_pitch = self.cam_pitch\n",
        "\n",
        "        img_name = self._label_image_path[idx]\n",
        "\n",
        "        with open(img_name, 'rb') as f:\n",
        "            image = (Image.open(f).convert('RGB'))\n",
        "\n",
        "        # image preprocess with crop and resize\n",
        "        image = Q.crop(image, self.h_crop, 0, self.h_org-self.h_crop, self.w_org)\n",
        "        image = Q.resize(image, size=(self.h_net, self.w_net), interpolation=Image.BILINEAR)\n",
        "\n",
        "        gt_anchor = np.zeros([np.int32(self.ipm_w / 8), self.num_types, self.anchor_dim], dtype=np.float32)\n",
        "        gt_lanes = self._label_laneline_all[idx]\n",
        "        gt_vis_inds = self._gt_laneline_visibility_all[idx]\n",
        "        for i in range(len(gt_lanes)):\n",
        "\n",
        "            # if ass_id >= 0:\n",
        "            ass_id = self._laneline_ass_ids[idx][i]\n",
        "            x_off_values = gt_lanes[i][:, 0]\n",
        "            z_values = gt_lanes[i][:, 1]\n",
        "            visibility = gt_vis_inds[i]\n",
        "            # assign anchor tensor values\n",
        "            gt_anchor[ass_id, 0, 0: self.num_y_steps] = x_off_values\n",
        "            if not self.no_3d:\n",
        "                gt_anchor[ass_id, 0, self.num_y_steps:2*self.num_y_steps] = z_values\n",
        "                gt_anchor[ass_id, 0, 2*self.num_y_steps:3*self.num_y_steps] = visibility\n",
        "\n",
        "            gt_anchor[ass_id, 0, -1] = 1.0\n",
        "\n",
        "        # fetch centerlines when available\n",
        "        if not self.no_centerline:\n",
        "            gt_lanes = self._label_centerline_all[idx]\n",
        "            gt_vis_inds = self._gt_centerline_visibility_all[idx]\n",
        "            for i in range(len(gt_lanes)):\n",
        "\n",
        "                # if ass_id >= 0:\n",
        "                ass_id = self._centerline_ass_ids[idx][i]\n",
        "                x_off_values = gt_lanes[i][:, 0]\n",
        "                z_values = gt_lanes[i][:, 1]\n",
        "                visibility = gt_vis_inds[i]\n",
        "\n",
        "                # assign anchor tensor values\n",
        "                # if ass_id >= 0:\n",
        "                if gt_anchor[ass_id, 1, -1] > 0:  # the case one splitting lane has been assigned\n",
        "                    gt_anchor[ass_id, 2, 0: self.num_y_steps] = x_off_values\n",
        "                    if not self.no_3d:\n",
        "                        gt_anchor[ass_id, 2, self.num_y_steps:2*self.num_y_steps] = z_values\n",
        "                        gt_anchor[ass_id, 2, 2*self.num_y_steps:3*self.num_y_steps] = visibility\n",
        "                    gt_anchor[ass_id, 2, -1] = 1.0\n",
        "                else:\n",
        "                    gt_anchor[ass_id, 1, 0: self.num_y_steps] = x_off_values\n",
        "                    if not self.no_3d:\n",
        "                        gt_anchor[ass_id, 1, self.num_y_steps:2*self.num_y_steps] = z_values\n",
        "                        gt_anchor[ass_id, 1, 2*self.num_y_steps:3*self.num_y_steps] = visibility\n",
        "                    gt_anchor[ass_id, 1, -1] = 1.0\n",
        "\n",
        "        if self.data_aug:\n",
        "            img_rot, aug_mat = data_aug_rotate(image)\n",
        "            image = Image.fromarray(img_rot)\n",
        "        image = self.totensor(image).float()\n",
        "        image = self.normalize(image)\n",
        "        gt_anchor = gt_anchor.reshape([np.int32(self.ipm_w / 8), -1])\n",
        "        gt_anchor = torch.from_numpy(gt_anchor)\n",
        "        gt_cam_height = torch.tensor(gt_cam_height, dtype=torch.float32)\n",
        "        gt_cam_pitch = torch.tensor(gt_cam_pitch, dtype=torch.float32)\n",
        "\n",
        "        # prepare binary segmentation label map\n",
        "        seg_label = np.zeros((self.h_net, self.w_net), dtype=np.int8)\n",
        "        gt_lanes = self._label_laneline_all_org[idx]\n",
        "        for i, lane in enumerate(gt_lanes):\n",
        "            # project lane3d to image\n",
        "            if self.no_3d:\n",
        "                x_2d = lane[:, 0]\n",
        "                y_2d = lane[:, 1]\n",
        "                # update transformation with image augmentation\n",
        "                if self.data_aug:\n",
        "                    x_2d, y_2d = homographic_transformation(aug_mat, x_2d, y_2d)\n",
        "            else:\n",
        "                H_g2im, P_g2im, H_crop, H_im2ipm = self.transform_mats(idx)\n",
        "                M = np.matmul(H_crop, P_g2im)\n",
        "                # update transformation with image augmentation\n",
        "                if self.data_aug:\n",
        "                    M = np.matmul(aug_mat, M)\n",
        "                x_2d, y_2d = projective_transformation(M, lane[:, 0],\n",
        "                                                       lane[:, 1], lane[:, 2])\n",
        "            for j in range(len(x_2d) - 1):\n",
        "                seg_label = cv2.line(seg_label,\n",
        "                                     (int(x_2d[j]), int(y_2d[j])), (int(x_2d[j+1]), int(y_2d[j+1])),\n",
        "                                     color=np.asscalar(np.array([1])))\n",
        "        seg_label = torch.from_numpy(seg_label.astype(np.float32))\n",
        "        seg_label.unsqueeze_(0)\n",
        "\n",
        "        if self.data_aug:\n",
        "            aug_mat = torch.from_numpy(aug_mat.astype(np.float32))\n",
        "            return image, seg_label, gt_anchor, idx, gt_cam_height, gt_cam_pitch, aug_mat\n",
        "        return image, seg_label, gt_anchor, idx, gt_cam_height, gt_cam_pitch\n",
        "\n",
        "    def init_dataset_3D(self, dataset_base_dir, json_file_path):\n",
        "        \"\"\"\n",
        "        :param dataset_info_file:\n",
        "        :return: image paths, labels in unormalized net input coordinates\n",
        "        data processing:\n",
        "        ground truth labels map are scaled wrt network input sizes\n",
        "        \"\"\"\n",
        "\n",
        "        # load image path, and lane pts\n",
        "        label_image_path = []\n",
        "        gt_laneline_pts_all = []\n",
        "        gt_centerline_pts_all = []\n",
        "        gt_laneline_visibility_all = []\n",
        "        gt_centerline_visibility_all = []\n",
        "        gt_cam_height_all = []\n",
        "        gt_cam_pitch_all = []\n",
        "\n",
        "        assert ops.exists(json_file_path), '{:s} not exist'.format(json_file_path)\n",
        "\n",
        "        with open(json_file_path, 'r') as file:\n",
        "            for line in file:\n",
        "                info_dict = json.loads(line)\n",
        "\n",
        "                image_path = ops.join(dataset_base_dir, info_dict['raw_file'])\n",
        "                assert ops.exists(image_path), '{:s} not exist'.format(image_path)\n",
        "\n",
        "                label_image_path.append(image_path)\n",
        "\n",
        "                gt_lane_pts = info_dict['laneLines']\n",
        "                gt_lane_visibility = info_dict['laneLines_visibility']\n",
        "                for i, lane in enumerate(gt_lane_pts):\n",
        "                    # A GT lane can be either 2D or 3D\n",
        "                    # if a GT lane is 3D, the height is intact from 3D GT, so keep it intact here too\n",
        "                    lane = np.array(lane)\n",
        "                    gt_lane_pts[i] = lane\n",
        "                    gt_lane_visibility[i] = np.array(gt_lane_visibility[i])\n",
        "                gt_laneline_pts_all.append(gt_lane_pts)\n",
        "                gt_laneline_visibility_all.append(gt_lane_visibility)\n",
        "\n",
        "                if not self.no_centerline:\n",
        "                    gt_lane_pts = info_dict['centerLines']\n",
        "                    gt_lane_visibility = info_dict['centerLines_visibility']\n",
        "                    for i, lane in enumerate(gt_lane_pts):\n",
        "                        # A GT lane can be either 2D or 3D\n",
        "                        # if a GT lane is 3D, the height is intact from 3D GT, so keep it intact here too\n",
        "                        lane = np.array(lane)\n",
        "                        gt_lane_pts[i] = lane\n",
        "                        gt_lane_visibility[i] = np.array(gt_lane_visibility[i])\n",
        "                    gt_centerline_pts_all.append(gt_lane_pts)\n",
        "                    gt_centerline_visibility_all.append(gt_lane_visibility)\n",
        "\n",
        "                if not self.fix_cam:\n",
        "                    gt_cam_height = info_dict['cam_height']\n",
        "                    gt_cam_height_all.append(gt_cam_height)\n",
        "                    gt_cam_pitch = info_dict['cam_pitch']\n",
        "                    gt_cam_pitch_all.append(gt_cam_pitch)\n",
        "\n",
        "        label_image_path = np.array(label_image_path)\n",
        "        gt_cam_height_all = np.array(gt_cam_height_all)\n",
        "        gt_cam_pitch_all = np.array(gt_cam_pitch_all)\n",
        "        gt_laneline_pts_all_org = copy.deepcopy(gt_laneline_pts_all)\n",
        "\n",
        "        # convert labeled laneline to anchor format\n",
        "        gt_laneline_ass_ids = []\n",
        "        gt_centerline_ass_ids = []\n",
        "        lane_x_off_all = []\n",
        "        lane_z_all = []\n",
        "        lane_y_off_all = []  # this is the offset of y when transformed back 3 3D\n",
        "        visibility_all_flat = []\n",
        "        for idx in range(len(gt_laneline_pts_all)):\n",
        "            # if idx == 936:\n",
        "            #     print(label_image_path[idx])\n",
        "            # fetch camera height and pitch\n",
        "            gt_cam_height = gt_cam_height_all[idx]\n",
        "            gt_cam_pitch = gt_cam_pitch_all[idx]\n",
        "            if not self.fix_cam:\n",
        "                P_g2im = projection_g2im(gt_cam_pitch, gt_cam_height, self.K)\n",
        "                H_g2im = homograpthy_g2im(gt_cam_pitch, gt_cam_height, self.K)\n",
        "                H_im2g = np.linalg.inv(H_g2im)\n",
        "            else:\n",
        "                P_g2im = self.P_g2im\n",
        "                H_im2g = self.H_im2g\n",
        "            P_g2gflat = np.matmul(H_im2g, P_g2im)\n",
        "\n",
        "            gt_lanes = gt_laneline_pts_all[idx]\n",
        "            gt_visibility = gt_laneline_visibility_all[idx]\n",
        "\n",
        "            # prune gt lanes by visibility labels\n",
        "            gt_lanes = [prune_3d_lane_by_visibility(gt_lane, gt_visibility[k]) for k, gt_lane in enumerate(gt_lanes)]\n",
        "            gt_laneline_pts_all_org[idx] = gt_lanes\n",
        "            # prune out-of-range points are necessary before transformation\n",
        "            gt_lanes = [prune_3d_lane_by_range(gt_lane, 3*self.x_min, 3*self.x_max) for gt_lane in gt_lanes]\n",
        "            gt_lanes = [lane for lane in gt_lanes if lane.shape[0] > 1]\n",
        "\n",
        "            # convert 3d lanes to flat ground space\n",
        "            self.convert_lanes_3d_to_gflat(gt_lanes, P_g2gflat)\n",
        "\n",
        "            gt_anchors = []\n",
        "            ass_ids = []\n",
        "            visibility_vectors = []\n",
        "            for i in range(len(gt_lanes)):\n",
        "\n",
        "                # convert gt label to anchor label\n",
        "                # consider individual out-of-range interpolation still visible\n",
        "                ass_id, x_off_values, z_values, visibility_vec = self.convert_label_to_anchor(gt_lanes[i], H_im2g)\n",
        "                if ass_id >= 0:\n",
        "                    gt_anchors.append(np.vstack([x_off_values, z_values]).T)\n",
        "                    ass_ids.append(ass_id)\n",
        "                    visibility_vectors.append(visibility_vec)\n",
        "\n",
        "            for i in range(len(gt_anchors)):\n",
        "                lane_x_off_all.append(gt_anchors[i][:, 0])\n",
        "                lane_z_all.append(gt_anchors[i][:, 1])\n",
        "                # compute y offset when transformed back to 3D space\n",
        "                lane_y_off_all.append(-gt_anchors[i][:, 1]*self.anchor_y_steps/gt_cam_height)\n",
        "            visibility_all_flat.extend(visibility_vectors)\n",
        "            gt_laneline_ass_ids.append(ass_ids)\n",
        "            gt_laneline_pts_all[idx] = gt_anchors\n",
        "            gt_laneline_visibility_all[idx] = visibility_vectors\n",
        "\n",
        "            if not self.no_centerline:\n",
        "                gt_lanes = gt_centerline_pts_all[idx]\n",
        "                gt_visibility = gt_centerline_visibility_all[idx]\n",
        "\n",
        "                # prune gt lanes by visibility labels\n",
        "                gt_lanes = [prune_3d_lane_by_visibility(gt_lane, gt_visibility[k]) for k, gt_lane in\n",
        "                            enumerate(gt_lanes)]\n",
        "                # prune out-of-range points are necessary before transformation\n",
        "                gt_lanes = [prune_3d_lane_by_range(gt_lane, 3 * self.x_min, 3 * self.x_max) for gt_lane in gt_lanes]\n",
        "                gt_lanes = [lane for lane in gt_lanes if lane.shape[0] > 1]\n",
        "\n",
        "                # convert 3d lanes to flat ground space\n",
        "                self.convert_lanes_3d_to_gflat(gt_lanes, P_g2gflat)\n",
        "\n",
        "                gt_anchors = []\n",
        "                ass_ids = []\n",
        "                visibility_vectors = []\n",
        "                for i in range(len(gt_lanes)):\n",
        "                    # convert gt label to anchor label\n",
        "                    # consider individual out-of-range interpolation still visible\n",
        "                    ass_id, x_off_values, z_values, visibility_vec = self.convert_label_to_anchor(gt_lanes[i], H_im2g)\n",
        "                    if ass_id >= 0:\n",
        "                        gt_anchors.append(np.vstack([x_off_values, z_values]).T)\n",
        "                        ass_ids.append(ass_id)\n",
        "                        visibility_vectors.append(visibility_vec)\n",
        "\n",
        "                for i in range(len(gt_anchors)):\n",
        "                    lane_x_off_all.append(gt_anchors[i][:, 0])\n",
        "                    lane_z_all.append(gt_anchors[i][:, 1])\n",
        "                    # compute y offset when transformed back to 3D space\n",
        "                    lane_y_off_all.append(-gt_anchors[i][:, 1] * self.anchor_y_steps / gt_cam_height)\n",
        "                visibility_all_flat.extend(visibility_vectors)\n",
        "                gt_centerline_ass_ids.append(ass_ids)\n",
        "                gt_centerline_pts_all[idx] = gt_anchors\n",
        "                gt_centerline_visibility_all[idx] = visibility_vectors\n",
        "\n",
        "        lane_x_off_all = np.array(lane_x_off_all)\n",
        "        lane_y_off_all = np.array(lane_y_off_all)\n",
        "        lane_z_all = np.array(lane_z_all)\n",
        "        visibility_all_flat = np.array(visibility_all_flat)\n",
        "\n",
        "        # computed weighted std based on visibility\n",
        "        lane_x_off_std = np.sqrt(np.average(lane_x_off_all**2, weights=visibility_all_flat, axis=0))\n",
        "        lane_y_off_std = np.sqrt(np.average(lane_y_off_all**2, weights=visibility_all_flat, axis=0))\n",
        "        lane_z_std = np.sqrt(np.average(lane_z_all**2, weights=visibility_all_flat, axis=0))\n",
        "        return label_image_path, gt_laneline_pts_all_org,\\\n",
        "               gt_laneline_pts_all, gt_centerline_pts_all, gt_cam_height_all, gt_cam_pitch_all,\\\n",
        "               gt_laneline_ass_ids, gt_centerline_ass_ids, lane_x_off_std, lane_y_off_std, lane_z_std,\\\n",
        "               gt_laneline_visibility_all, gt_centerline_visibility_all\n",
        "\n",
        "    def init_dataset_tusimple(self, dataset_base_dir, json_file_path):\n",
        "        \"\"\"\n",
        "        :param json_file_path:\n",
        "        :return: image paths, labels in unormalized net input coordinates\n",
        "        data processing:\n",
        "        ground truth labels map are scaled wrt network input sizes\n",
        "        \"\"\"\n",
        "\n",
        "        # load image path, and lane pts\n",
        "        label_image_path = []\n",
        "        gt_laneline_pts_all = []\n",
        "        gt_laneline_visibility_all = []\n",
        "\n",
        "        assert ops.exists(json_file_path), '{:s} not exist'.format(json_file_path)\n",
        "\n",
        "        with open(json_file_path, 'r') as file:\n",
        "            for line in file:\n",
        "                info_dict = json.loads(line)\n",
        "\n",
        "                image_path = ops.join(dataset_base_dir, info_dict['raw_file'])\n",
        "                assert ops.exists(image_path), '{:s} not exist'.format(image_path)\n",
        "\n",
        "                label_image_path.append(image_path)\n",
        "\n",
        "                gt_lane_pts_X = info_dict['lanes']\n",
        "                gt_y_steps = np.array(info_dict['h_samples'])\n",
        "                gt_lane_pts = []\n",
        "\n",
        "                for i, lane_x in enumerate(gt_lane_pts_X):\n",
        "                    lane = np.zeros([gt_y_steps.shape[0], 2], dtype=np.float32)\n",
        "\n",
        "                    lane_x = np.array(lane_x)\n",
        "                    lane[:, 0] = lane_x\n",
        "                    lane[:, 1] = gt_y_steps\n",
        "                    # remove invalid samples\n",
        "                    lane = lane[lane_x >= 0, :]\n",
        "\n",
        "                    if lane.shape[0] < 2:\n",
        "                        continue\n",
        "\n",
        "                    gt_lane_pts.append(lane)\n",
        "                gt_laneline_pts_all.append(gt_lane_pts)\n",
        "        label_image_path = np.array(label_image_path)\n",
        "        gt_laneline_pts_all_org = copy.deepcopy(gt_laneline_pts_all)\n",
        "\n",
        "        # convert labeled laneline to anchor format\n",
        "        H_im2g = self.H_im2g\n",
        "        gt_laneline_ass_ids = []\n",
        "        lane_x_off_all = []\n",
        "        for idx in range(len(gt_laneline_pts_all)):\n",
        "            gt_lanes = gt_laneline_pts_all[idx]\n",
        "            gt_anchors = []\n",
        "            ass_ids = []\n",
        "            visibility_vectors = []\n",
        "            for i in range(len(gt_lanes)):\n",
        "                # convert gt label to anchor label\n",
        "                ass_id, x_off_values, z_values, visibility_vec = self.convert_label_to_anchor(gt_lanes[i], H_im2g)\n",
        "                if ass_id >= 0:\n",
        "                    gt_anchors.append(np.vstack([x_off_values, z_values]).T)\n",
        "                    ass_ids.append(ass_id)\n",
        "                    lane_x_off_all.append(x_off_values)\n",
        "                    visibility_vectors.append(visibility_vec)\n",
        "            gt_laneline_ass_ids.append(ass_ids)\n",
        "            gt_laneline_pts_all[idx] = gt_anchors\n",
        "            gt_laneline_visibility_all.append(visibility_vectors)\n",
        "\n",
        "        lane_x_off_all = np.array(lane_x_off_all)\n",
        "        lane_x_off_std = np.std(lane_x_off_all, axis=0)\n",
        "\n",
        "        return label_image_path, gt_laneline_pts_all_org, gt_laneline_pts_all, gt_laneline_ass_ids,\\\n",
        "               lane_x_off_std, gt_laneline_visibility_all\n",
        "\n",
        "    def set_x_off_std(self, x_off_std):\n",
        "        self._x_off_std = x_off_std\n",
        "\n",
        "    def set_y_off_std(self, y_off_std):\n",
        "        self._y_off_std = y_off_std\n",
        "\n",
        "    def set_z_std(self, z_std):\n",
        "        self._z_std = z_std\n",
        "\n",
        "    def normalize_lane_label(self):\n",
        "        for lanes in self._label_laneline_all:\n",
        "            for lane in lanes:\n",
        "                lane[:, 0] = np.divide(lane[:, 0], self._x_off_std)\n",
        "                if not self.no_3d:\n",
        "                    lane[:, 1] = np.divide(lane[:, 1], self._z_std)\n",
        "\n",
        "        if not self.no_centerline:\n",
        "            for lanes in self._label_centerline_all:\n",
        "                for lane in lanes:\n",
        "                    lane[:, 0] = np.divide(lane[:, 0], self._x_off_std)\n",
        "                    if not self.no_3d:\n",
        "                        lane[:, 1] = np.divide(lane[:, 1], self._z_std)\n",
        "\n",
        "    def convert_lanes_3d_to_gflat(self, lanes, P_g2gflat):\n",
        "        \"\"\"\n",
        "            Convert a set of lanes from 3D ground coordinates [X, Y, Z], to IPM-based\n",
        "            flat ground coordinates [x_gflat, y_gflat, Z]\n",
        "        :param lanes: a list of N x 3 numpy arrays recording a set of 3d lanes\n",
        "        :param P_g2gflat: projection matrix from 3D ground coordinates to frat ground coordinates\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # TODO: this function can be simplified with the derived formula\n",
        "        for lane in lanes:\n",
        "            # convert gt label to anchor label\n",
        "            lane_gflat_x, lane_gflat_y = projective_transformation(P_g2gflat, lane[:, 0], lane[:, 1], lane[:, 2])\n",
        "            lane[:, 0] = lane_gflat_x\n",
        "            lane[:, 1] = lane_gflat_y\n",
        "\n",
        "    def compute_visibility_lanes_gflat(self, lane_anchors, ass_ids):\n",
        "        \"\"\"\n",
        "            Compute the visibility of each anchor point in flat ground space. The reasoning requires all the considering\n",
        "            lanes globally.\n",
        "        :param lane_anchors: A list of N x 2 numpy arrays where N equals to number of Y steps in anchor representation\n",
        "                             x offset and z values are recorded for each lane\n",
        "               ass_ids: the associated id determine the base x value\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if len(lane_anchors) is 0:\n",
        "            return [], [], []\n",
        "\n",
        "        vis_inds_lanes = []\n",
        "        # sort the lane_anchors such that lanes are recorded from left to right\n",
        "        # sort the lane_anchors based on the x value at the closed anchor\n",
        "        # do NOT sort the lane_anchors by the order of ass_ids because there could be identical ass_ids\n",
        "\n",
        "        x_refs = [lane_anchors[i][0, 0] + self.anchor_x_steps[ass_ids[i]] for i in range(len(lane_anchors))]\n",
        "        sort_idx = np.argsort(x_refs)\n",
        "        lane_anchors = [lane_anchors[i] for i in sort_idx]\n",
        "        ass_ids = [ass_ids[i] for i in sort_idx]\n",
        "\n",
        "        min_x_vec = lane_anchors[0][:, 0] + self.anchor_x_steps[ass_ids[0]]\n",
        "        max_x_vec = lane_anchors[-1][:, 0] + self.anchor_x_steps[ass_ids[-1]]\n",
        "        for i, lane in enumerate(lane_anchors):\n",
        "            vis_inds = np.ones(lane.shape[0])\n",
        "            for j in range(lane.shape[0]):\n",
        "                x_value = lane[j, 0] + self.anchor_x_steps[ass_ids[i]]\n",
        "                if x_value < 3*self.x_min or x_value > 3*self.x_max:\n",
        "                    vis_inds[j:] = 0\n",
        "                # A point with x < the left most lane's current x is considered invisible\n",
        "                # A point with x > the right most lane's current x is considered invisible\n",
        "                if x_value < min_x_vec[j] - 0.01 or x_value > max_x_vec[j] + 0.01:\n",
        "                    vis_inds[j:] = 0\n",
        "                    break\n",
        "                # A point with orientation close enough to horizontal is considered as invisible\n",
        "                if j > 0:\n",
        "                    dx = lane[j, 0] - lane[j-1, 0]\n",
        "                    dy = self.anchor_y_steps[j] - self.anchor_y_steps[j-1]\n",
        "                    if abs(dx/dy) > 10:\n",
        "                        vis_inds[j:] = 0\n",
        "                        break\n",
        "            vis_inds_lanes.append(vis_inds)\n",
        "        return vis_inds_lanes, lane_anchors, ass_ids\n",
        "\n",
        "    def convert_label_to_anchor(self, laneline_gt, H_im2g):\n",
        "        \"\"\"\n",
        "            Convert a set of ground-truth lane points to the format of network anchor representation.\n",
        "            All the given laneline only include visible points. The interpolated points will be marked invisible\n",
        "        :param laneline_gt: a list of arrays where each array is a set of point coordinates in [x, y, z]\n",
        "        :param H_im2g: homographic transformation only used for tusimple dataset\n",
        "        :return: ass_id: the column id of current lane in anchor representation\n",
        "                 x_off_values: current lane's x offset from it associated anchor column\n",
        "                 z_values: current lane's z value in ground coordinates\n",
        "        \"\"\"\n",
        "        if self.no_3d:  # For ground-truth in 2D image coordinates (TuSimple)\n",
        "            gt_lane_2d = laneline_gt\n",
        "            # project to ground coordinates\n",
        "            gt_lane_grd_x, gt_lane_grd_y = homographic_transformation(H_im2g, gt_lane_2d[:, 0], gt_lane_2d[:, 1])\n",
        "            gt_lane_3d = np.zeros_like(gt_lane_2d, dtype=np.float32)\n",
        "            gt_lane_3d[:, 0] = gt_lane_grd_x\n",
        "            gt_lane_3d[:, 1] = gt_lane_grd_y\n",
        "        else:  # For ground-truth in ground coordinates (Apollo Sim)\n",
        "            gt_lane_3d = laneline_gt\n",
        "\n",
        "        # prune out points not in valid range, requires additional points to interpolate better\n",
        "        # prune out-of-range points after transforming to flat ground space, update visibility vector\n",
        "        valid_indices = np.logical_and(np.logical_and(gt_lane_3d[:, 1] > 0, gt_lane_3d[:, 1] < 200),\n",
        "                                       np.logical_and(gt_lane_3d[:, 0] > 3 * self.x_min,\n",
        "                                                      gt_lane_3d[:, 0] < 3 * self.x_max))\n",
        "        gt_lane_3d = gt_lane_3d[valid_indices, ...]\n",
        "        # use more restricted range to determine deletion or not\n",
        "        if gt_lane_3d.shape[0] < 2 or np.sum(np.logical_and(gt_lane_3d[:, 0] > self.x_min,\n",
        "                                                            gt_lane_3d[:, 0] < self.x_max)) < 2:\n",
        "            return -1, np.array([]), np.array([]), np.array([])\n",
        "\n",
        "        if self.dataset_name is 'tusimple':\n",
        "            # reverse the order of 3d pints to make the first point the closest\n",
        "            gt_lane_3d = gt_lane_3d[::-1, :]\n",
        "\n",
        "        # only keep the portion y is monotonically increasing above a threshold, to prune those super close points\n",
        "        gt_lane_3d = make_lane_y_mono_inc(gt_lane_3d)\n",
        "        if gt_lane_3d.shape[0] < 2:\n",
        "            return -1, np.array([]), np.array([]), np.array([])\n",
        "\n",
        "        # ignore GT ends before y_ref, for those start at y > y_ref, use its interpolated value at y_ref for association\n",
        "        # if gt_lane_3d[0, 1] > self.y_ref or gt_lane_3d[-1, 1] < self.y_ref:\n",
        "        if gt_lane_3d[-1, 1] < self.y_ref:\n",
        "            return -1, np.array([]), np.array([]), np.array([])\n",
        "\n",
        "        # resample ground-truth laneline at anchor y steps\n",
        "        x_values, z_values, visibility_vec = resample_laneline_in_y(gt_lane_3d, self.anchor_y_steps, out_vis=True)\n",
        "\n",
        "        if np.sum(visibility_vec) < 2:\n",
        "            return -1, np.array([]), np.array([]), np.array([])\n",
        "\n",
        "        # decide association at r_ref\n",
        "        ass_id = np.argmin((self.anchor_x_steps - x_values[self.ref_id]) ** 2)\n",
        "        # compute offset values\n",
        "        x_off_values = x_values - self.anchor_x_steps[ass_id]\n",
        "\n",
        "        return ass_id, x_off_values, z_values, visibility_vec\n",
        "\n",
        "    def transform_mats(self, idx):\n",
        "        \"\"\"\n",
        "            return the transform matrices associated with sample idx\n",
        "        :param idx:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if not self.fix_cam:\n",
        "            H_g2im = homograpthy_g2im(self._label_cam_pitch_all[idx],\n",
        "                                      self._label_cam_height_all[idx], self.K)\n",
        "            P_g2im = projection_g2im(self._label_cam_pitch_all[idx],\n",
        "                                     self._label_cam_height_all[idx], self.K)\n",
        "\n",
        "            H_im2ipm = np.linalg.inv(np.matmul(self.H_crop, np.matmul(H_g2im, self.H_ipm2g)))\n",
        "            return H_g2im, P_g2im, self.H_crop, H_im2ipm\n",
        "        else:\n",
        "            return self.H_g2im, self.P_g2im, self.H_crop, self.H_im2ipm\n",
        "\n",
        "\n",
        "def make_lane_y_mono_inc(lane):\n",
        "    \"\"\"\n",
        "        Due to lose of height dim, projected lanes to flat ground plane may not have monotonically increasing y.\n",
        "        This function trace the y with monotonically increasing y, and output a pruned lane\n",
        "    :param lane:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    idx2del = []\n",
        "    max_y = lane[0, 1]\n",
        "    for i in range(1, lane.shape[0]):\n",
        "        # hard-coded a smallest step, so the far-away near horizontal tail can be pruned\n",
        "        if lane[i, 1] <= max_y + 3:\n",
        "            idx2del.append(i)\n",
        "        else:\n",
        "            max_y = lane[i, 1]\n",
        "    lane = np.delete(lane, idx2del, 0)\n",
        "    return lane\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    Data Augmentation: \n",
        "        idea 1: (currently in use)\n",
        "            when initializing dataset, all labels will be prepared in 3D which do not need to be changed in image augmenting\n",
        "            Image data augmentation would change the spatial transform matrix integrated in the network, provide \n",
        "            the transformation matrix related to random cropping, scaling and rotation\n",
        "        idea 2:\n",
        "            Introduce random sampling of cam_h, cam_pitch and their associated transformed image\n",
        "            img2 = [R2[:, 0:2], T2] [R1[:, 0:2], T1]^-1 img1\n",
        "            output augmented hcam, pitch, and img2 and untouched 3D anchor label value, Before forward pass, update spatial\n",
        "            transform in network. However, However, image rotation is not considered, additional cropping is still needed\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def data_aug_rotate(img):\n",
        "    # assume img in PIL image format\n",
        "    rot = random.uniform(-np.pi/18, np.pi/18)\n",
        "    # rot = random.uniform(-10, 10)\n",
        "    center_x = img.width / 2\n",
        "    center_y = img.height / 2\n",
        "    rot_mat = cv2.getRotationMatrix2D((center_x, center_y), rot, 1.0)\n",
        "    img_rot = np.array(img)\n",
        "    img_rot = cv2.warpAffine(img_rot, rot_mat, (img.width, img.height), flags=cv2.INTER_LINEAR)\n",
        "    # img_rot = img.rotate(rot)\n",
        "    # rot = rot / 180 * np.pi\n",
        "    rot_mat = np.vstack([rot_mat, [0, 0, 1]])\n",
        "    return img_rot, rot_mat\n",
        "\n",
        "\n",
        "def get_loader(transformed_dataset, args):\n",
        "    \"\"\"\n",
        "        create dataset from ground-truth\n",
        "        return a batch sampler based ont the dataset\n",
        "    \"\"\"\n",
        "\n",
        "    # transformed_dataset = LaneDataset(dataset_base_dir, json_file_path, args)\n",
        "    sample_idx = range(transformed_dataset.n_samples)\n",
        "    sample_idx = sample_idx[0:len(sample_idx)//args.batch_size*args.batch_size]\n",
        "    data_sampler = torch.utils.data.sampler.SubsetRandomSampler(sample_idx)\n",
        "    data_loader = DataLoader(transformed_dataset,\n",
        "                             batch_size=args.batch_size, sampler=data_sampler,\n",
        "                             num_workers=args.nworkers, pin_memory=True)\n",
        "\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "def compute_2d_lanes(pred_anchor, h_samples, H_g2im, anchor_x_steps, anchor_y_steps, x_min, x_max, prob_th=0.5):\n",
        "    \"\"\"\n",
        "        convert anchor lanes to image lanes in tusimple format\n",
        "    :return: x values at h_samples in image coordinates\n",
        "    \"\"\"\n",
        "    lanes_out = []\n",
        "\n",
        "    # apply nms to output lanes\n",
        "    pred_anchor[:, -1] = nms_1d(pred_anchor[:, -1])\n",
        "\n",
        "    # need to resample network lane results at h_samples\n",
        "    for j in range(pred_anchor.shape[0]):\n",
        "        if pred_anchor[j, -1] > prob_th:\n",
        "            x_offsets = pred_anchor[j, :-1]\n",
        "            x_3d = x_offsets + anchor_x_steps[j]\n",
        "            # compute x, y in original image coordinates\n",
        "            x_2d, y_2d = homographic_transformation(H_g2im, x_3d, anchor_y_steps)\n",
        "            # reverse the order such that y_2d is ascending\n",
        "            x_2d = x_2d[::-1]\n",
        "            y_2d = y_2d[::-1]\n",
        "            # resample at h_samples\n",
        "            x_values, z_values = resample_laneline_in_y(np.vstack([x_2d, y_2d]).T, h_samples)\n",
        "            # assign out-of-range x values to be -2\n",
        "            x_values = x_values.astype(np.int)\n",
        "            x_values[np.where(np.logical_or(x_values < x_min, x_values >= x_max))] = -2\n",
        "            # assign far side y values to be -2\n",
        "            x_values[np.where(h_samples < y_2d[0])] = -2\n",
        "\n",
        "            lanes_out.append(x_values.data.tolist())\n",
        "    return lanes_out\n",
        "\n",
        "\n",
        "def compute_3d_lanes(pred_anchor, anchor_dim, anchor_x_steps, anchor_y_steps, h_cam, prob_th=0.5):\n",
        "    lanelines_out = []\n",
        "    centerlines_out = []\n",
        "    num_y_steps = anchor_y_steps.shape[0]\n",
        "\n",
        "    # apply nms to output lanes probabilities\n",
        "    # consider w/o centerline cases\n",
        "    pred_anchor[:, anchor_dim - 1] = nms_1d(pred_anchor[:, anchor_dim - 1])\n",
        "    pred_anchor[:, 2*anchor_dim - 1] = nms_1d(pred_anchor[:, 2*anchor_dim - 1])\n",
        "    pred_anchor[:, 3*anchor_dim - 1] = nms_1d(pred_anchor[:, 3*anchor_dim - 1])\n",
        "\n",
        "    # output only the visible portion of lane\n",
        "    \"\"\"\n",
        "        An important process is output lanes in the considered y-range. Interpolate the visibility attributes to \n",
        "        automatically determine whether to extend the lanes.\n",
        "    \"\"\"\n",
        "    for j in range(pred_anchor.shape[0]):\n",
        "        # draw laneline\n",
        "        if pred_anchor[j, anchor_dim - 1] > prob_th:\n",
        "            x_offsets = pred_anchor[j, :num_y_steps]\n",
        "            x_g = x_offsets + anchor_x_steps[j]\n",
        "            z_g = pred_anchor[j, num_y_steps:2*num_y_steps]\n",
        "            visibility = pred_anchor[j, 2*num_y_steps:3*num_y_steps]\n",
        "            line = np.vstack([x_g, anchor_y_steps, z_g]).T\n",
        "            # line = line[visibility > prob_th, :]\n",
        "            # convert to 3D ground space\n",
        "            x_g, y_g = transform_lane_gflat2g(h_cam, line[:, 0], line[:, 1], line[:, 2])\n",
        "            line[:, 0] = x_g\n",
        "            line[:, 1] = y_g\n",
        "            line = resample_laneline_in_y_with_vis(line, anchor_y_steps, visibility)\n",
        "            if line.shape[0] >= 2:\n",
        "                lanelines_out.append(line.data.tolist())\n",
        "\n",
        "        # draw centerline\n",
        "        if pred_anchor[j, 2*anchor_dim - 1] > prob_th:\n",
        "            x_offsets = pred_anchor[j, anchor_dim:anchor_dim + num_y_steps]\n",
        "            x_g = x_offsets + anchor_x_steps[j]\n",
        "            z_g = pred_anchor[j, anchor_dim + num_y_steps:anchor_dim + 2*num_y_steps]\n",
        "            visibility = pred_anchor[j, anchor_dim + 2*num_y_steps:anchor_dim + 3*num_y_steps]\n",
        "            line = np.vstack([x_g, anchor_y_steps, z_g]).T\n",
        "            # line = line[visibility > prob_th, :]\n",
        "            # convert to 3D ground space\n",
        "            x_g, y_g = transform_lane_gflat2g(h_cam, line[:, 0], line[:, 1], line[:, 2])\n",
        "            line[:, 0] = x_g\n",
        "            line[:, 1] = y_g\n",
        "            line = resample_laneline_in_y_with_vis(line, anchor_y_steps, visibility)\n",
        "            if line.shape[0] >= 2:\n",
        "                centerlines_out.append(line.data.tolist())\n",
        "\n",
        "        # draw the additional centerline for the merging case\n",
        "        if pred_anchor[j, 3*anchor_dim - 1] > prob_th:\n",
        "            x_offsets = pred_anchor[j, 2*anchor_dim:2*anchor_dim + num_y_steps]\n",
        "            x_g = x_offsets + anchor_x_steps[j]\n",
        "            z_g = pred_anchor[j, 2*anchor_dim + num_y_steps:2*anchor_dim + 2*num_y_steps]\n",
        "            visibility = pred_anchor[j, 2*anchor_dim + 2*num_y_steps:2*anchor_dim + 3*num_y_steps]\n",
        "            line = np.vstack([x_g, anchor_y_steps, z_g]).T\n",
        "            # line = line[visibility > prob_th, :]\n",
        "            # convert to 3D ground space\n",
        "            x_g, y_g = transform_lane_gflat2g(h_cam, line[:, 0], line[:, 1], line[:, 2])\n",
        "            line[:, 0] = x_g\n",
        "            line[:, 1] = y_g\n",
        "            line = resample_laneline_in_y_with_vis(line, anchor_y_steps, visibility)\n",
        "            if line.shape[0] >= 2:\n",
        "                centerlines_out.append(line.data.tolist())\n",
        "\n",
        "    return lanelines_out, centerlines_out\n",
        "\n",
        "\n",
        "def compute_3d_lanes_all_prob(pred_anchor, anchor_dim, anchor_x_steps, anchor_y_steps, h_cam):\n",
        "    lanelines_out = []\n",
        "    lanelines_prob = []\n",
        "    centerlines_out = []\n",
        "    centerlines_prob = []\n",
        "    num_y_steps = anchor_y_steps.shape[0]\n",
        "\n",
        "    # apply nms to output lanes probabilities\n",
        "    # consider w/o centerline cases\n",
        "    pred_anchor[:, anchor_dim - 1] = nms_1d(pred_anchor[:, anchor_dim - 1])\n",
        "    pred_anchor[:, 2*anchor_dim - 1] = nms_1d(pred_anchor[:, 2*anchor_dim - 1])\n",
        "    pred_anchor[:, 3*anchor_dim - 1] = nms_1d(pred_anchor[:, 3*anchor_dim - 1])\n",
        "\n",
        "    # output only the visible portion of lane\n",
        "    \"\"\"\n",
        "        An important process is output lanes in the considered y-range. Interpolate the visibility attributes to \n",
        "        automatically determine whether to extend the lanes.\n",
        "    \"\"\"\n",
        "    for j in range(pred_anchor.shape[0]):\n",
        "        # draw laneline\n",
        "        x_offsets = pred_anchor[j, :num_y_steps]\n",
        "        x_g = x_offsets + anchor_x_steps[j]\n",
        "        z_g = pred_anchor[j, num_y_steps:2*num_y_steps]\n",
        "        visibility = pred_anchor[j, 2*num_y_steps:3*num_y_steps]\n",
        "        line = np.vstack([x_g, anchor_y_steps, z_g]).T\n",
        "        # line = line[visibility > prob_th, :]\n",
        "        # convert to 3D ground space\n",
        "        x_g, y_g = transform_lane_gflat2g(h_cam, line[:, 0], line[:, 1], line[:, 2])\n",
        "        line[:, 0] = x_g\n",
        "        line[:, 1] = y_g\n",
        "        line = resample_laneline_in_y_with_vis(line, anchor_y_steps, visibility)\n",
        "        if line.shape[0] >= 2:\n",
        "            lanelines_out.append(line.data.tolist())\n",
        "            lanelines_prob.append(pred_anchor[j, anchor_dim - 1].tolist())\n",
        "\n",
        "        # draw centerline\n",
        "        x_offsets = pred_anchor[j, anchor_dim:anchor_dim + num_y_steps]\n",
        "        x_g = x_offsets + anchor_x_steps[j]\n",
        "        z_g = pred_anchor[j, anchor_dim + num_y_steps:anchor_dim + 2*num_y_steps]\n",
        "        visibility = pred_anchor[j, anchor_dim + 2*num_y_steps:anchor_dim + 3*num_y_steps]\n",
        "        line = np.vstack([x_g, anchor_y_steps, z_g]).T\n",
        "        # line = line[visibility > prob_th, :]\n",
        "        # convert to 3D ground space\n",
        "        x_g, y_g = transform_lane_gflat2g(h_cam, line[:, 0], line[:, 1], line[:, 2])\n",
        "        line[:, 0] = x_g\n",
        "        line[:, 1] = y_g\n",
        "        line = resample_laneline_in_y_with_vis(line, anchor_y_steps, visibility)\n",
        "        if line.shape[0] >= 2:\n",
        "            centerlines_out.append(line.data.tolist())\n",
        "            centerlines_prob.append(pred_anchor[j, 2 * anchor_dim - 1].tolist())\n",
        "\n",
        "        # draw the additional centerline for the merging case\n",
        "        x_offsets = pred_anchor[j, 2*anchor_dim:2*anchor_dim + num_y_steps]\n",
        "        x_g = x_offsets + anchor_x_steps[j]\n",
        "        z_g = pred_anchor[j, 2*anchor_dim + num_y_steps:2*anchor_dim + 2*num_y_steps]\n",
        "        visibility = pred_anchor[j, 2*anchor_dim + 2*num_y_steps:2*anchor_dim + 3*num_y_steps]\n",
        "        line = np.vstack([x_g, anchor_y_steps, z_g]).T\n",
        "        # line = line[visibility > prob_th, :]\n",
        "        # convert to 3D ground space\n",
        "        x_g, y_g = transform_lane_gflat2g(h_cam, line[:, 0], line[:, 1], line[:, 2])\n",
        "        line[:, 0] = x_g\n",
        "        line[:, 1] = y_g\n",
        "        line = resample_laneline_in_y_with_vis(line, anchor_y_steps, visibility)\n",
        "        if line.shape[0] >= 2:\n",
        "            centerlines_out.append(line.data.tolist())\n",
        "            centerlines_prob.append(pred_anchor[j, 3*anchor_dim - 1].tolist())\n",
        "\n",
        "    return lanelines_out, centerlines_out, lanelines_prob, centerlines_prob\n",
        "\n",
        "\n",
        "def unormalize_lane_anchor(anchor, dataset):\n",
        "    num_y_steps = dataset.num_y_steps\n",
        "    anchor_dim = dataset.anchor_dim\n",
        "    for i in range(dataset.num_types):\n",
        "        anchor[:, i*anchor_dim:i*anchor_dim + num_y_steps] = \\\n",
        "            np.multiply(anchor[:, i*anchor_dim: i*anchor_dim + num_y_steps], dataset._x_off_std)\n",
        "        if not dataset.no_3d:\n",
        "            anchor[:, i*anchor_dim + num_y_steps: i*anchor_dim + 2*num_y_steps] = \\\n",
        "                np.multiply(anchor[:, i*anchor_dim + num_y_steps: i*anchor_dim + 2*num_y_steps], dataset._z_std)\n",
        "\n",
        "\n",
        "# unit testR\n",
        "'''\n",
        "if __name__ == '__main__':\n",
        "    import sys\n",
        "    #from tools.utils import define_args\n",
        "\n",
        "    args = define_args()\n",
        "    #args = parser.parse_args()\n",
        "\n",
        "    # dataset_name: 'standard' / 'rare_subset' / 'illus_chg'\n",
        "    args.dataset_name = 'illus_chg'\n",
        "    args.dataset_dir = '/content/drive/Shareddrives/colab/Apollo_Sim_3D_Lane_Release'\n",
        "    args.test_dataset_dir = '/content/drive/Shareddrives/colab/Apollo_Sim_3D_Lane_Release'\n",
        "   # args.data_dir = ops.join('data_splits', args.dataset_name)\n",
        "    args.data_dir='/content/drive/Shareddrives/colab/data_splits/illus_chg'\n",
        "    # load configuration for certain dataset\n",
        "    if 'tusimple' in args.dataset_name:\n",
        "        tusimple_config(args)\n",
        "    else:\n",
        "        sim3d_config(args)\n",
        "    args.y_ref = 5.0\n",
        "\n",
        "    # set 3D ground area for visualization\n",
        "    vis_border_3d = np.array([[-1.75, 100.], [1.75, 100.], [-1.75, 5.], [1.75, 5.]])\n",
        "    print('visual area border:')\n",
        "    print(vis_border_3d)\n",
        "\n",
        "    # load data\n",
        "    dataset = LaneDataset(args.dataset_dir, ops.join(args.data_dir, 'train.json'), args, data_aug=True, save_std=True)\n",
        "    dataset.normalize_lane_label()\n",
        "    loader = get_loader(dataset, args)\n",
        "    anchor_x_steps = dataset.anchor_x_steps\n",
        "\n",
        "    # initialize visualizer\n",
        "    args.mod = 'ext'\n",
        "    visualizer = Visualizer(args)\n",
        "    Visualizer.anchor_dim = dataset.anchor_dim\n",
        "\n",
        "    # get a batch of data/label pairs from loader\n",
        "    for batch_ndx, (image_tensor, seg_labels, gt_tensor, idx, gt_cam_height, gt_cam_pitch, aug_mat) in enumerate(loader):\n",
        "        print('batch id: {:d}, image tensor shape:'.format(batch_ndx))\n",
        "        print(image_tensor.shape)\n",
        "        print('batch id: {:d}, gt tensor shape:'.format(batch_ndx))\n",
        "        print(gt_tensor.shape)\n",
        "\n",
        "        # convert to BGR and numpy for visualization in opencv\n",
        "        images = image_tensor.permute(0, 2, 3, 1).data.cpu().numpy()\n",
        "        seg_labels = seg_labels.data.cpu().numpy()\n",
        "        gt_anchors = gt_tensor.numpy()\n",
        "        idx = idx.numpy()\n",
        "        gt_cam_height = gt_cam_height.numpy()\n",
        "        gt_cam_pitch = gt_cam_pitch.numpy()\n",
        "        aug_mat = aug_mat.numpy()\n",
        "        for i in range(args.batch_size):\n",
        "            img = images[i]\n",
        "            seg_label = seg_labels[i][0]\n",
        "            img = img * np.array(args.vgg_std).astype(np.float32)\n",
        "            img = img + np.array(args.vgg_mean).astype(np.float32)\n",
        "            if img.min() < 0. or img.max() > 1.0:\n",
        "                print('found an invalid normalized sample')\n",
        "            img = np.clip(img, 0, 1)\n",
        "\n",
        "            # if args.no_3d:\n",
        "            H_g2im, P_g2im, H_crop, H_im2ipm = dataset.transform_mats(idx[i])\n",
        "            M = np.matmul(H_crop, H_g2im)\n",
        "            # update transformation with image augmentation\n",
        "            M = np.matmul(aug_mat[i], M)\n",
        "            x_2d, y_2d = homographic_transformation(M, vis_border_3d[:, 0], vis_border_3d[:, 1])\n",
        "\n",
        "            # update transformation with image augmentation\n",
        "            H_im2ipm = np.matmul(H_im2ipm, np.linalg.inv(aug_mat[i]))\n",
        "            im_ipm = cv2.warpPerspective(img, H_im2ipm, (args.ipm_w, args.ipm_h))\n",
        "            im_ipm = np.clip(im_ipm, 0, 1)\n",
        "\n",
        "            # draw visual border on image to confirm calibration\n",
        "            x_2d = x_2d.astype(np.int)\n",
        "            y_2d = y_2d.astype(np.int)\n",
        "            img = cv2.line(img, (x_2d[0], y_2d[0]), (x_2d[1], y_2d[1]), [1, 0, 0], 2)\n",
        "            img = cv2.line(img, (x_2d[2], y_2d[2]), (x_2d[3], y_2d[3]), [1, 0, 0], 2)\n",
        "            img = cv2.line(img, (x_2d[0], y_2d[0]), (x_2d[2], y_2d[2]), [1, 0, 0], 2)\n",
        "            img = cv2.line(img, (x_2d[1], y_2d[1]), (x_2d[3], y_2d[3]), [1, 0, 0], 2)\n",
        "            gt_anchor = gt_anchors[i, :, :]\n",
        "\n",
        "            # un-normalize\n",
        "            unormalize_lane_anchor(gt_anchor, dataset)\n",
        "\n",
        "            # visualize ground-truth anchor lanelines by projecting them on the image\n",
        "            img = visualizer.draw_on_img_new(img, gt_anchor, M, 'laneline', color=[0, 0, 1])\n",
        "            if not args.no_centerline:\n",
        "                img = visualizer.draw_on_img_new(img, gt_anchor, M, 'centerline', color=[0, 1, 0])\n",
        "\n",
        "            cv2.putText(img, 'camara pitch: {:.3f}'.format(gt_cam_pitch[i]/np.pi*180),\n",
        "                        (5, 30), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.7, color=(0, 0, 1), thickness=2)\n",
        "            cv2.putText(img, 'camara height: {:.3f}'.format(gt_cam_height[i]),\n",
        "                        (5, 60), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.7, color=(0, 0, 1), thickness=2)\n",
        "\n",
        "            # visualize on ipm\n",
        "            im_ipm = visualizer.draw_on_ipm_new(im_ipm, gt_anchor, 'laneline', color=[0, 0, 1])\n",
        "            if not args.no_centerline:\n",
        "                im_ipm = visualizer.draw_on_ipm_new(im_ipm, gt_anchor, 'centerline', color=[0, 1, 0])\n",
        "            \n",
        "            # convert image to BGR for opencv imshow\n",
        "            cv2.imshow('image gt check', np.flip(img, axis=2))\n",
        "            cv2.imshow('ipm gt check', np.flip(im_ipm, axis=2))\n",
        "            cv2.imshow('seg label check', seg_label)\n",
        "            cv2.waitKey()\n",
        "            \n",
        "            print('image: {:d} in batch: {:d}'.format(idx[i], batch_ndx))\n",
        "\n",
        "    print('done')\n",
        "    '''"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nif __name__ == '__main__':\\n    import sys\\n    #from tools.utils import define_args\\n\\n    args = define_args()\\n    #args = parser.parse_args()\\n\\n    # dataset_name: 'standard' / 'rare_subset' / 'illus_chg'\\n    args.dataset_name = 'illus_chg'\\n    args.dataset_dir = '/content/drive/Shareddrives/colab/Apollo_Sim_3D_Lane_Release'\\n    args.test_dataset_dir = '/content/drive/Shareddrives/colab/Apollo_Sim_3D_Lane_Release'\\n   # args.data_dir = ops.join('data_splits', args.dataset_name)\\n    args.data_dir='/content/drive/Shareddrives/colab/data_splits/illus_chg'\\n    # load configuration for certain dataset\\n    if 'tusimple' in args.dataset_name:\\n        tusimple_config(args)\\n    else:\\n        sim3d_config(args)\\n    args.y_ref = 5.0\\n\\n    # set 3D ground area for visualization\\n    vis_border_3d = np.array([[-1.75, 100.], [1.75, 100.], [-1.75, 5.], [1.75, 5.]])\\n    print('visual area border:')\\n    print(vis_border_3d)\\n\\n    # load data\\n    dataset = LaneDataset(args.dataset_dir, ops.join(args.data_dir, 'train.json'), args, data_aug=True, save_std=True)\\n    dataset.normalize_lane_label()\\n    loader = get_loader(dataset, args)\\n    anchor_x_steps = dataset.anchor_x_steps\\n\\n    # initialize visualizer\\n    args.mod = 'ext'\\n    visualizer = Visualizer(args)\\n    Visualizer.anchor_dim = dataset.anchor_dim\\n\\n    # get a batch of data/label pairs from loader\\n    for batch_ndx, (image_tensor, seg_labels, gt_tensor, idx, gt_cam_height, gt_cam_pitch, aug_mat) in enumerate(loader):\\n        print('batch id: {:d}, image tensor shape:'.format(batch_ndx))\\n        print(image_tensor.shape)\\n        print('batch id: {:d}, gt tensor shape:'.format(batch_ndx))\\n        print(gt_tensor.shape)\\n\\n        # convert to BGR and numpy for visualization in opencv\\n        images = image_tensor.permute(0, 2, 3, 1).data.cpu().numpy()\\n        seg_labels = seg_labels.data.cpu().numpy()\\n        gt_anchors = gt_tensor.numpy()\\n        idx = idx.numpy()\\n        gt_cam_height = gt_cam_height.numpy()\\n        gt_cam_pitch = gt_cam_pitch.numpy()\\n        aug_mat = aug_mat.numpy()\\n        for i in range(args.batch_size):\\n            img = images[i]\\n            seg_label = seg_labels[i][0]\\n            img = img * np.array(args.vgg_std).astype(np.float32)\\n            img = img + np.array(args.vgg_mean).astype(np.float32)\\n            if img.min() < 0. or img.max() > 1.0:\\n                print('found an invalid normalized sample')\\n            img = np.clip(img, 0, 1)\\n\\n            # if args.no_3d:\\n            H_g2im, P_g2im, H_crop, H_im2ipm = dataset.transform_mats(idx[i])\\n            M = np.matmul(H_crop, H_g2im)\\n            # update transformation with image augmentation\\n            M = np.matmul(aug_mat[i], M)\\n            x_2d, y_2d = homographic_transformation(M, vis_border_3d[:, 0], vis_border_3d[:, 1])\\n\\n            # update transformation with image augmentation\\n            H_im2ipm = np.matmul(H_im2ipm, np.linalg.inv(aug_mat[i]))\\n            im_ipm = cv2.warpPerspective(img, H_im2ipm, (args.ipm_w, args.ipm_h))\\n            im_ipm = np.clip(im_ipm, 0, 1)\\n\\n            # draw visual border on image to confirm calibration\\n            x_2d = x_2d.astype(np.int)\\n            y_2d = y_2d.astype(np.int)\\n            img = cv2.line(img, (x_2d[0], y_2d[0]), (x_2d[1], y_2d[1]), [1, 0, 0], 2)\\n            img = cv2.line(img, (x_2d[2], y_2d[2]), (x_2d[3], y_2d[3]), [1, 0, 0], 2)\\n            img = cv2.line(img, (x_2d[0], y_2d[0]), (x_2d[2], y_2d[2]), [1, 0, 0], 2)\\n            img = cv2.line(img, (x_2d[1], y_2d[1]), (x_2d[3], y_2d[3]), [1, 0, 0], 2)\\n            gt_anchor = gt_anchors[i, :, :]\\n\\n            # un-normalize\\n            unormalize_lane_anchor(gt_anchor, dataset)\\n\\n            # visualize ground-truth anchor lanelines by projecting them on the image\\n            img = visualizer.draw_on_img_new(img, gt_anchor, M, 'laneline', color=[0, 0, 1])\\n            if not args.no_centerline:\\n                img = visualizer.draw_on_img_new(img, gt_anchor, M, 'centerline', color=[0, 1, 0])\\n\\n            cv2.putText(img, 'camara pitch: {:.3f}'.format(gt_cam_pitch[i]/np.pi*180),\\n                        (5, 30), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.7, color=(0, 0, 1), thickness=2)\\n            cv2.putText(img, 'camara height: {:.3f}'.format(gt_cam_height[i]),\\n                        (5, 60), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.7, color=(0, 0, 1), thickness=2)\\n\\n            # visualize on ipm\\n            im_ipm = visualizer.draw_on_ipm_new(im_ipm, gt_anchor, 'laneline', color=[0, 0, 1])\\n            if not args.no_centerline:\\n                im_ipm = visualizer.draw_on_ipm_new(im_ipm, gt_anchor, 'centerline', color=[0, 1, 0])\\n            \\n            # convert image to BGR for opencv imshow\\n            cv2.imshow('image gt check', np.flip(img, axis=2))\\n            cv2.imshow('ipm gt check', np.flip(im_ipm, axis=2))\\n            cv2.imshow('seg label check', seg_label)\\n            cv2.waitKey()\\n            \\n            print('image: {:d} in batch: {:d}'.format(idx[i], batch_ndx))\\n\\n    print('done')\\n    \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "Ptl9T8fw-RH0",
        "outputId": "13ce66b1-ac31-48ae-cf4f-4de3c61a246e"
      },
      "source": [
        "#Pytorch_Generalized_3D_Lane_Detection/networks/Loss_crit.py /\n",
        "\"\"\"\n",
        "Loss functions\n",
        "Author: Yuliang Guo (33yuliangguo@gmail.com)\n",
        "Date: March, 2020\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Laneline_loss_3D(nn.Module):\n",
        "    \"\"\"\n",
        "    Compute the loss between predicted lanelines and ground-truth laneline in anchor representation.\n",
        "    The anchor representation is based on real 3D X, Y, Z.\n",
        "    loss = loss1 + loss2 + loss2\n",
        "    loss1: cross entropy loss for lane type classification\n",
        "    loss2: sum of geometric distance betwen 3D lane anchor points in X and Z offsets\n",
        "    loss3: error in estimating pitch and camera heights\n",
        "    \"\"\"\n",
        "    def __init__(self, num_types, anchor_dim, pred_cam):\n",
        "        super(Laneline_loss_3D, self).__init__()\n",
        "        self.num_types = num_types\n",
        "        self.anchor_dim = anchor_dim\n",
        "        self.pred_cam = pred_cam\n",
        "\n",
        "    def forward(self, pred_3D_lanes, gt_3D_lanes, pred_hcam, gt_hcam, pred_pitch, gt_pitch):\n",
        "        \"\"\"\n",
        "        :param pred_3D_lanes: predicted tensor with size N x (ipm_w/8) x 3*(2*K+1)\n",
        "        :param gt_3D_lanes: ground-truth tensor with size N x (ipm_w/8) x 3*(2*K+1)\n",
        "        :param pred_pitch: predicted pitch with size N\n",
        "        :param gt_pitch: ground-truth pitch with size N\n",
        "        :param pred_hcam: predicted camera height with size N\n",
        "        :param gt_hcam: ground-truth camera height with size N\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        sizes = pred_3D_lanes.shape\n",
        "        # reshape to N x ipm_w/8 x 3 x (2K+1)\n",
        "        pred_3D_lanes = pred_3D_lanes.reshape(sizes[0], sizes[1], self.num_types, self.anchor_dim)\n",
        "        gt_3D_lanes = gt_3D_lanes.reshape(sizes[0], sizes[1], self.num_types, self.anchor_dim)\n",
        "        # class prob N x ipm_w/8 x 3 x 1, anchor value N x ipm_w/8 x 3 x 2K\n",
        "        pred_class = pred_3D_lanes[:, :, :, -1].unsqueeze(-1)\n",
        "        pred_anchors = pred_3D_lanes[:, :, :, :-1]\n",
        "        gt_class = gt_3D_lanes[:, :, :, -1].unsqueeze(-1)\n",
        "        gt_anchors = gt_3D_lanes[:, :, :, :-1]\n",
        "\n",
        "        loss1 = -torch.sum(gt_class*torch.log(pred_class + torch.tensor(1e-9)) +\n",
        "                           (torch.ones_like(gt_class)-gt_class) *\n",
        "                           torch.log(torch.ones_like(pred_class)-pred_class + torch.tensor(1e-9)))\n",
        "        # applying L1 norm does not need to separate X and Z\n",
        "        loss2 = torch.sum(torch.norm(gt_class*(pred_anchors-gt_anchors), p=1, dim=3))\n",
        "        if not self.pred_cam:\n",
        "            return loss1+loss2\n",
        "        loss3 = torch.sum(torch.abs(gt_pitch-pred_pitch))+torch.sum(torch.abs(gt_hcam-pred_hcam))\n",
        "        return loss1+loss2+loss3\n",
        "\n",
        "\n",
        "class Laneline_loss_gflat(nn.Module):\n",
        "    \"\"\"\n",
        "    Compute the loss between predicted lanelines and ground-truth laneline in anchor representation.\n",
        "    The anchor representation is in flat ground space X', Y' and real 3D Z. Visibility estimation is also included.\n",
        "    loss = loss0 + loss1 + loss2 + loss2\n",
        "    loss0: cross entropy loss for lane point visibility\n",
        "    loss1: cross entropy loss for lane type classification\n",
        "    loss2: sum of geometric distance betwen 3D lane anchor points in X and Z offsets\n",
        "    loss3: error in estimating pitch and camera heights\n",
        "    \"\"\"\n",
        "    def __init__(self, num_types, num_y_steps, pred_cam):\n",
        "        super(Laneline_loss_gflat, self).__init__()\n",
        "        self.num_types = num_types\n",
        "        self.num_y_steps = num_y_steps\n",
        "        self.anchor_dim = 3*self.num_y_steps + 1\n",
        "        self.pred_cam = pred_cam\n",
        "\n",
        "    def forward(self, pred_3D_lanes, gt_3D_lanes, pred_hcam, gt_hcam, pred_pitch, gt_pitch):\n",
        "        \"\"\"\n",
        "        :param pred_3D_lanes: predicted tensor with size N x (ipm_w/8) x 3*(2*K+1)\n",
        "        :param gt_3D_lanes: ground-truth tensor with size N x (ipm_w/8) x 3*(2*K+1)\n",
        "        :param pred_pitch: predicted pitch with size N\n",
        "        :param gt_pitch: ground-truth pitch with size N\n",
        "        :param pred_hcam: predicted camera height with size N\n",
        "        :param gt_hcam: ground-truth camera height with size N\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        sizes = pred_3D_lanes.shape\n",
        "        # reshape to N x ipm_w/8 x 3 x (3K+1)\n",
        "        pred_3D_lanes = pred_3D_lanes.reshape(sizes[0], sizes[1], self.num_types, self.anchor_dim)\n",
        "        gt_3D_lanes = gt_3D_lanes.reshape(sizes[0], sizes[1], self.num_types, self.anchor_dim)\n",
        "        # class prob N x ipm_w/8 x 3 x 1, anchor value N x ipm_w/8 x 3 x 2K\n",
        "        pred_class = pred_3D_lanes[:, :, :, -1].unsqueeze(-1)\n",
        "        pred_anchors = pred_3D_lanes[:, :, :, :2*self.num_y_steps]\n",
        "        pred_visibility = pred_3D_lanes[:, :, :, 2*self.num_y_steps:3*self.num_y_steps]\n",
        "        gt_class = gt_3D_lanes[:, :, :, -1].unsqueeze(-1)\n",
        "        gt_anchors = gt_3D_lanes[:, :, :, :2*self.num_y_steps]\n",
        "        gt_visibility = gt_3D_lanes[:, :, :, 2*self.num_y_steps:3*self.num_y_steps]\n",
        "\n",
        "        # cross-entropy loss for visibility\n",
        "        loss0 = -torch.sum(\n",
        "            gt_visibility*torch.log(pred_visibility + torch.tensor(1e-9)) +\n",
        "            (torch.ones_like(gt_visibility) - gt_visibility + torch.tensor(1e-9)) *\n",
        "            torch.log(torch.ones_like(pred_visibility) - pred_visibility + torch.tensor(1e-9)))/self.num_y_steps\n",
        "        # cross-entropy loss for lane probability\n",
        "        loss1 = -torch.sum(\n",
        "            gt_class*torch.log(pred_class + torch.tensor(1e-9)) +\n",
        "            (torch.ones_like(gt_class)-gt_class) *\n",
        "            torch.log(torch.ones_like(pred_class) - pred_class + torch.tensor(1e-9)))\n",
        "        # applying L1 norm does not need to separate X and Z\n",
        "        loss2 = torch.sum(torch.norm(gt_class*torch.cat((gt_visibility, gt_visibility), 3) *\n",
        "                                     (pred_anchors-gt_anchors), p=1, dim=3))\n",
        "        if not self.pred_cam:\n",
        "            return loss0+loss1+loss2\n",
        "        loss3 = torch.sum(torch.abs(gt_pitch-pred_pitch))+torch.sum(torch.abs(gt_hcam-pred_hcam))\n",
        "        return loss0+loss1+loss2+loss3\n",
        "\n",
        "\n",
        "class Laneline_loss_gflat_3D(nn.Module):\n",
        "    \"\"\"\n",
        "    Compute the loss between predicted lanelines and ground-truth laneline in anchor representation.\n",
        "    The anchor representation is in flat ground space X', Y' and real 3D Z. Visibility estimation is also included.\n",
        "    The X' Y' and Z estimation will be transformed to real X, Y to compare with ground truth. An additional loss in\n",
        "    X, Y space is expected to guide the learning of features to satisfy the geometry constraints between two spaces\n",
        "    loss = loss0 + loss1 + loss2 + loss2\n",
        "    loss0: cross entropy loss for lane point visibility\n",
        "    loss1: cross entropy loss for lane type classification\n",
        "    loss2: sum of geometric distance betwen 3D lane anchor points in X and Z offsets\n",
        "    loss3: error in estimating pitch and camera heights\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_size, num_types, anchor_x_steps, anchor_y_steps, x_off_std, y_off_std, z_std, pred_cam=False, no_cuda=False):\n",
        "        super(Laneline_loss_gflat_3D, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.num_types = num_types\n",
        "        self.num_x_steps = anchor_x_steps.shape[0]\n",
        "        self.num_y_steps = anchor_y_steps.shape[0]\n",
        "        self.anchor_dim = 3*self.num_y_steps + 1\n",
        "        self.pred_cam = pred_cam\n",
        "\n",
        "        # prepare broadcast anchor_x_tensor, anchor_y_tensor, std_X, std_Y, std_Z\n",
        "        tmp_zeros = torch.zeros(self.batch_size, self.num_x_steps, self.num_types, self.num_y_steps)\n",
        "        self.x_off_std = torch.tensor(x_off_std.astype(np.float32)).reshape(1, 1, 1, self.num_y_steps) + tmp_zeros\n",
        "        self.y_off_std = torch.tensor(y_off_std.astype(np.float32)).reshape(1, 1, 1, self.num_y_steps) + tmp_zeros\n",
        "        self.z_std = torch.tensor(z_std.astype(np.float32)).reshape(1, 1, 1, self.num_y_steps) + tmp_zeros\n",
        "        self.anchor_x_tensor = torch.tensor(anchor_x_steps.astype(np.float32)).reshape(1, self.num_x_steps, 1, 1) + tmp_zeros\n",
        "        self.anchor_y_tensor = torch.tensor(anchor_y_steps.astype(np.float32)).reshape(1, 1, 1, self.num_y_steps) + tmp_zeros\n",
        "        self.anchor_x_tensor = self.anchor_x_tensor/self.x_off_std\n",
        "        self.anchor_y_tensor = self.anchor_y_tensor/self.y_off_std\n",
        "\n",
        "        if not no_cuda:\n",
        "            self.z_std = self.z_std.cuda()\n",
        "            self.anchor_x_tensor = self.anchor_x_tensor.cuda()\n",
        "            self.anchor_y_tensor = self.anchor_y_tensor.cuda()\n",
        "\n",
        "    def forward(self, pred_3D_lanes, gt_3D_lanes, pred_hcam, gt_hcam, pred_pitch, gt_pitch):\n",
        "        \"\"\"\n",
        "        :param pred_3D_lanes: predicted tensor with size N x (ipm_w/8) x 3*(2*K+1)\n",
        "        :param gt_3D_lanes: ground-truth tensor with size N x (ipm_w/8) x 3*(2*K+1)\n",
        "        :param pred_pitch: predicted pitch with size N\n",
        "        :param gt_pitch: ground-truth pitch with size N\n",
        "        :param pred_hcam: predicted camera height with size N\n",
        "        :param gt_hcam: ground-truth camera height with size N\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        sizes = pred_3D_lanes.shape\n",
        "        # reshape to N x ipm_w/8 x 3 x (3K+1)\n",
        "        pred_3D_lanes = pred_3D_lanes.reshape(sizes[0], sizes[1], self.num_types, self.anchor_dim)\n",
        "        gt_3D_lanes = gt_3D_lanes.reshape(sizes[0], sizes[1], self.num_types, self.anchor_dim)\n",
        "        # class prob N x ipm_w/8 x 3 x 1, anchor values N x ipm_w/8 x 3 x 2K, visibility N x ipm_w/8 x 3 x K\n",
        "        pred_class = pred_3D_lanes[:, :, :, -1].unsqueeze(-1)\n",
        "        pred_anchors = pred_3D_lanes[:, :, :, :2*self.num_y_steps]\n",
        "        pred_visibility = pred_3D_lanes[:, :, :, 2*self.num_y_steps:3*self.num_y_steps]\n",
        "        gt_class = gt_3D_lanes[:, :, :, -1].unsqueeze(-1)\n",
        "        gt_anchors = gt_3D_lanes[:, :, :, :2*self.num_y_steps]\n",
        "        gt_visibility = gt_3D_lanes[:, :, :, 2*self.num_y_steps:3*self.num_y_steps]\n",
        "\n",
        "        # cross-entropy loss for visibility\n",
        "        loss0 = -torch.sum(\n",
        "            gt_visibility*torch.log(pred_visibility + torch.tensor(1e-9)) +\n",
        "            (torch.ones_like(gt_visibility) - gt_visibility + torch.tensor(1e-9)) *\n",
        "            torch.log(torch.ones_like(pred_visibility) - pred_visibility + torch.tensor(1e-9)))/self.num_y_steps\n",
        "        # cross-entropy loss for lane probability\n",
        "        loss1 = -torch.sum(\n",
        "            gt_class*torch.log(pred_class + torch.tensor(1e-9)) +\n",
        "            (torch.ones_like(gt_class) - gt_class) *\n",
        "            torch.log(torch.ones_like(pred_class) - pred_class + torch.tensor(1e-9)))\n",
        "        # applying L1 norm does not need to separate X and Z\n",
        "        loss2 = torch.sum(\n",
        "            torch.norm(gt_class*torch.cat((gt_visibility, gt_visibility), 3)*(pred_anchors-gt_anchors), p=1, dim=3))\n",
        "\n",
        "        # compute loss in real 3D X, Y space, the transformation considers offset to anchor and normalization by std\n",
        "        pred_Xoff_g = pred_anchors[:, :, :, :self.num_y_steps]\n",
        "        pred_Z = pred_anchors[:, :, :, self.num_y_steps:2*self.num_y_steps]\n",
        "        gt_Xoff_g = gt_anchors[:, :, :, :self.num_y_steps]\n",
        "        gt_Z = gt_anchors[:, :, :, self.num_y_steps:2*self.num_y_steps]\n",
        "        pred_hcam = pred_hcam.reshape(self.batch_size, 1, 1, 1)\n",
        "        gt_hcam = gt_hcam.reshape(self.batch_size, 1, 1, 1)\n",
        "\n",
        "        pred_Xoff = (1 - pred_Z * self.z_std / pred_hcam) * pred_Xoff_g - pred_Z * self.z_std / pred_hcam * self.anchor_x_tensor\n",
        "        pred_Yoff = -pred_Z * self.z_std / pred_hcam * self.anchor_y_tensor\n",
        "        gt_Xoff = (1 - gt_Z * self.z_std / gt_hcam) * gt_Xoff_g - gt_Z * self.z_std / gt_hcam * self.anchor_x_tensor\n",
        "        gt_Yoff = -gt_Z * self.z_std / gt_hcam * self.anchor_y_tensor\n",
        "        loss3 = torch.sum(\n",
        "            torch.norm(\n",
        "                gt_class * torch.cat((gt_visibility, gt_visibility), 3) *\n",
        "                (torch.cat((pred_Xoff, pred_Yoff), 3) - torch.cat((gt_Xoff, gt_Yoff), 3)), p=1, dim=3))\n",
        "\n",
        "        if not self.pred_cam:\n",
        "            return loss0+loss1+loss2+loss3\n",
        "        loss4 = torch.sum(torch.abs(gt_pitch-pred_pitch)) + torch.sum(torch.abs(gt_hcam-pred_hcam))\n",
        "        return loss0+loss1+loss2+loss3+loss4\n",
        "\n",
        "\n",
        "# unit test\n",
        "'''\n",
        "if __name__ == '__main__':\n",
        "    num_types = 3\n",
        "\n",
        "    # for Laneline_loss_3D\n",
        "    print('Test Laneline_loss_3D')\n",
        "    anchor_dim = 2*6 + 1\n",
        "    pred_cam = True\n",
        "    criterion = Laneline_loss_3D(num_types, anchor_dim, pred_cam)\n",
        "    criterion = criterion.cuda()\n",
        "\n",
        "    pred_3D_lanes = torch.rand(8, 26, num_types*anchor_dim).cuda()\n",
        "    gt_3D_lanes = torch.rand(8, 26, num_types*anchor_dim).cuda()\n",
        "    pred_pitch = torch.ones(8).float().cuda()\n",
        "    gt_pitch = torch.ones(8).float().cuda()\n",
        "    pred_hcam = torch.ones(8).float().cuda()\n",
        "    gt_hcam = torch.ones(8).float().cuda()\n",
        "\n",
        "    loss = criterion(pred_3D_lanes, gt_3D_lanes, pred_pitch, gt_pitch, pred_hcam, gt_hcam)\n",
        "    print(loss)\n",
        "\n",
        "    # for Laneline_loss_gflat\n",
        "    print('Test Laneline_loss_gflat')\n",
        "    num_y_steps = 6\n",
        "    anchor_dim = 3*num_y_steps + 1\n",
        "    pred_cam = True\n",
        "    criterion = Laneline_loss_gflat(num_types, num_y_steps, pred_cam)\n",
        "    criterion = criterion.cuda()\n",
        "\n",
        "    pred_3D_lanes = torch.rand(8, 26, num_types*anchor_dim).cuda()\n",
        "    gt_3D_lanes = torch.rand(8, 26, num_types*anchor_dim).cuda()\n",
        "    pred_pitch = torch.ones(8).float().cuda()\n",
        "    gt_pitch = torch.ones(8).float().cuda()\n",
        "    pred_hcam = torch.ones(8).float().cuda()\n",
        "    gt_hcam = torch.ones(8).float().cuda()\n",
        "\n",
        "    loss = criterion(pred_3D_lanes, gt_3D_lanes, pred_pitch, gt_pitch, pred_hcam, gt_hcam)\n",
        "\n",
        "    print(loss)\n",
        "\n",
        "    # for Laneline_loss_gflat_3D\n",
        "    print('Test Laneline_loss_gflat_3D')\n",
        "    batch_size = 8\n",
        "    anchor_x_steps = np.linspace(-10, 10, 26, endpoint=True)\n",
        "    anchor_y_steps = np.array([3, 5, 10, 20, 30, 40, 50, 60, 80, 100])\n",
        "    num_y_steps = anchor_y_steps.shape[0]\n",
        "    x_off_std = np.ones(num_y_steps)\n",
        "    y_off_std = np.ones(num_y_steps)\n",
        "    z_std = np.ones(num_y_steps)\n",
        "    pred_cam = True\n",
        "    criterion = Laneline_loss_gflat_3D(batch_size, num_types, anchor_x_steps, anchor_y_steps, x_off_std, y_off_std, z_std, pred_cam, no_cuda=False)\n",
        "    # criterion = criterion.cuda()\n",
        "\n",
        "    anchor_dim = 3*num_y_steps + 1\n",
        "    pred_3D_lanes = torch.rand(batch_size, 26, num_types*anchor_dim).cuda()\n",
        "    gt_3D_lanes = torch.rand(batch_size, 26, num_types*anchor_dim).cuda()\n",
        "    pred_pitch = torch.ones(batch_size).float().cuda()\n",
        "    gt_pitch = torch.ones(batch_size).float().cuda()\n",
        "    pred_hcam = torch.ones(batch_size).float().cuda()*1.5\n",
        "    gt_hcam = torch.ones(batch_size).float().cuda()*1.5\n",
        "\n",
        "    loss = criterion(pred_3D_lanes, gt_3D_lanes, pred_pitch, gt_pitch, pred_hcam, gt_hcam)\n",
        "\n",
        "    print(loss)\n",
        "    '''"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nif __name__ == '__main__':\\n    num_types = 3\\n\\n    # for Laneline_loss_3D\\n    print('Test Laneline_loss_3D')\\n    anchor_dim = 2*6 + 1\\n    pred_cam = True\\n    criterion = Laneline_loss_3D(num_types, anchor_dim, pred_cam)\\n    criterion = criterion.cuda()\\n\\n    pred_3D_lanes = torch.rand(8, 26, num_types*anchor_dim).cuda()\\n    gt_3D_lanes = torch.rand(8, 26, num_types*anchor_dim).cuda()\\n    pred_pitch = torch.ones(8).float().cuda()\\n    gt_pitch = torch.ones(8).float().cuda()\\n    pred_hcam = torch.ones(8).float().cuda()\\n    gt_hcam = torch.ones(8).float().cuda()\\n\\n    loss = criterion(pred_3D_lanes, gt_3D_lanes, pred_pitch, gt_pitch, pred_hcam, gt_hcam)\\n    print(loss)\\n\\n    # for Laneline_loss_gflat\\n    print('Test Laneline_loss_gflat')\\n    num_y_steps = 6\\n    anchor_dim = 3*num_y_steps + 1\\n    pred_cam = True\\n    criterion = Laneline_loss_gflat(num_types, num_y_steps, pred_cam)\\n    criterion = criterion.cuda()\\n\\n    pred_3D_lanes = torch.rand(8, 26, num_types*anchor_dim).cuda()\\n    gt_3D_lanes = torch.rand(8, 26, num_types*anchor_dim).cuda()\\n    pred_pitch = torch.ones(8).float().cuda()\\n    gt_pitch = torch.ones(8).float().cuda()\\n    pred_hcam = torch.ones(8).float().cuda()\\n    gt_hcam = torch.ones(8).float().cuda()\\n\\n    loss = criterion(pred_3D_lanes, gt_3D_lanes, pred_pitch, gt_pitch, pred_hcam, gt_hcam)\\n\\n    print(loss)\\n\\n    # for Laneline_loss_gflat_3D\\n    print('Test Laneline_loss_gflat_3D')\\n    batch_size = 8\\n    anchor_x_steps = np.linspace(-10, 10, 26, endpoint=True)\\n    anchor_y_steps = np.array([3, 5, 10, 20, 30, 40, 50, 60, 80, 100])\\n    num_y_steps = anchor_y_steps.shape[0]\\n    x_off_std = np.ones(num_y_steps)\\n    y_off_std = np.ones(num_y_steps)\\n    z_std = np.ones(num_y_steps)\\n    pred_cam = True\\n    criterion = Laneline_loss_gflat_3D(batch_size, num_types, anchor_x_steps, anchor_y_steps, x_off_std, y_off_std, z_std, pred_cam, no_cuda=False)\\n    # criterion = criterion.cuda()\\n\\n    anchor_dim = 3*num_y_steps + 1\\n    pred_3D_lanes = torch.rand(batch_size, 26, num_types*anchor_dim).cuda()\\n    gt_3D_lanes = torch.rand(batch_size, 26, num_types*anchor_dim).cuda()\\n    pred_pitch = torch.ones(batch_size).float().cuda()\\n    gt_pitch = torch.ones(batch_size).float().cuda()\\n    pred_hcam = torch.ones(batch_size).float().cuda()*1.5\\n    gt_hcam = torch.ones(batch_size).float().cuda()*1.5\\n\\n    loss = criterion(pred_3D_lanes, gt_3D_lanes, pred_pitch, gt_pitch, pred_hcam, gt_hcam)\\n\\n    print(loss)\\n    \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "fBQsiqJo-vAd",
        "outputId": "0dcc8a58-a925-469a-a845-b143798fd509"
      },
      "source": [
        "\"\"\"\n",
        "Pytorch_Generalized_3D_Lane_Detection/networks/GeoNet3D_ext.py /\n",
        "\n",
        "3D-GeoNet with new anchor: predict 3D lanes from segmentation input. The geometry-guided anchor design is based on:\n",
        "    \"Gen-laneNet: a generalized and scalable approach for 3D lane detection\"\n",
        "New Anchor:\n",
        "    1. Prediction head's lane representation is in X_g, Y_g in flat ground space and Z in real 3D ground space.\n",
        "    Y_g is sampled equally, X_g, Z is regressed from network output.\n",
        "    2. In addition, visibility of each point is added into the anchor representation and regressed from network.\n",
        "Overall dimension of the output tensor would be: N * W * 3 *(3 * K + 1), where\n",
        "    K          : number of y samples.\n",
        "    (3 * K + 1): Each lane includes K attributes for X_g offset + K attributes for Z + K attributes for visibility + 1 lane probability\n",
        "    3          : Each anchor column include one laneline and two centerlines --> 3\n",
        "    W          : Number of columns for the output tensor each corresponds to a IPM X_g location\n",
        "    N          : batch size\n",
        "Use of this network requires to use its corresponding data loader and loss criterion.\n",
        "Author: Yuliang Guo (33yuliangguo@gmail.com)\n",
        "Date: March, 2020\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "#from tools.utils import define_args, define_init_weights, homography_im2ipm_norm, homography_crop_resize, homography_ipmnorm2g, tusimple_config, sim3d_config\n",
        "\n",
        "\n",
        "def make_layers(cfg, in_channels=3, batch_norm=False):\n",
        "    layers = []\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def make_one_layer(in_channels, out_channels, kernel_size=3, padding=1, stride=1, batch_norm=False):\n",
        "    conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n",
        "    if batch_norm:\n",
        "        layers = [conv2d, nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True)]\n",
        "    else:\n",
        "        layers = [conv2d, nn.ReLU(inplace=True)]\n",
        "    return layers\n",
        "\n",
        "\n",
        "# initialize base_grid with different sizes can adapt to different sizes\n",
        "class ProjectiveGridGenerator(nn.Module):\n",
        "    def __init__(self, size_ipm, M, no_cuda):\n",
        "        \"\"\"\n",
        "        :param size_ipm: size of ipm tensor NCHW\n",
        "        :param im_h: height of image tensor\n",
        "        :param im_w: width of image tensor\n",
        "        :param M: normalized transformation matrix between image view and IPM\n",
        "        :param no_cuda:\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.N, self.H, self.W = size_ipm\n",
        "        # self.im_h = im_h\n",
        "        # self.im_w = im_w\n",
        "        linear_points_W = torch.linspace(0, 1 - 1/self.W, self.W)\n",
        "        linear_points_H = torch.linspace(0, 1 - 1/self.H, self.H)\n",
        "\n",
        "        # use M only to decide the type not value\n",
        "        self.base_grid = M.new(self.N, self.H, self.W, 3)\n",
        "        self.base_grid[:, :, :, 0] = torch.ger(\n",
        "                torch.ones(self.H), linear_points_W).expand_as(self.base_grid[:, :, :, 0])\n",
        "        self.base_grid[:, :, :, 1] = torch.ger(\n",
        "                linear_points_H, torch.ones(self.W)).expand_as(self.base_grid[:, :, :, 1])\n",
        "        self.base_grid[:, :, :, 2] = 1\n",
        "\n",
        "        self.base_grid = Variable(self.base_grid)\n",
        "        if not no_cuda:\n",
        "            self.base_grid = self.base_grid.cuda()\n",
        "            # self.im_h = self.im_h.cuda()\n",
        "            # self.im_w = self.im_w.cuda()\n",
        "\n",
        "    def forward(self, M):\n",
        "        # compute the grid mapping based on the input transformation matrix M\n",
        "        # if base_grid is top-view, M should be ipm-to-img homography transformation, and vice versa\n",
        "        grid = torch.bmm(self.base_grid.view(self.N, self.H * self.W, 3), M.transpose(1, 2))\n",
        "        grid = torch.div(grid[:, :, 0:2], grid[:, :, 2:]).reshape((self.N, self.H, self.W, 2))\n",
        "        #\n",
        "        \"\"\"\n",
        "        output grid to be used for grid_sample. \n",
        "            1. grid specifies the sampling pixel locations normalized by the input spatial dimensions.\n",
        "            2. pixel locations need to be converted to the range (-1, 1)\n",
        "        \"\"\"\n",
        "        grid = (grid - 0.5) * 2\n",
        "        return grid\n",
        "\n",
        "\n",
        "# Sub-network corresponding to the top view pathway\n",
        "class TopViewPathway(nn.Module):\n",
        "    def __init__(self, batch_norm=False, init_weights=True):\n",
        "        super(TopViewPathway, self).__init__()\n",
        "        self.features1 = make_layers(['M', 128, 128, 128], 128, batch_norm)\n",
        "        self.features2 = make_layers(['M', 256, 256, 256], 256, batch_norm)\n",
        "        self.features3 = make_layers(['M', 256, 256, 256], 512, batch_norm)\n",
        "\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, a, b, c, d):\n",
        "        x = self.features1(a)\n",
        "        feat_1 = x\n",
        "        x = torch.cat((x, b), 1)\n",
        "        x = self.features2(x)\n",
        "        feat_2 = x\n",
        "        x = torch.cat((x, c), 1)\n",
        "        x = self.features3(x)\n",
        "        feat_3 = x\n",
        "        x = torch.cat((x, d), 1)\n",
        "        return x, feat_1, feat_2, feat_3\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                # if m.bias is not None:\n",
        "                #     nn.init.constant_(m.bias, 0)\n",
        "                nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "#  Lane Prediction Head: through a series of convolutions with no padding in the y dimension, the feature maps are\n",
        "#  reduced in height, and finally the prediction layer size is N × 1 × 3 ·(3 · K + 1)\n",
        "class LanePredictionHead(nn.Module):\n",
        "    def __init__(self, num_lane_type, num_y_steps, batch_norm=False):\n",
        "        super(LanePredictionHead, self).__init__()\n",
        "        self.num_lane_type = num_lane_type\n",
        "        self.num_y_steps = num_y_steps\n",
        "        self.anchor_dim = 3*self.num_y_steps + 1\n",
        "        layers = []\n",
        "        layers += make_one_layer(64, 64, kernel_size=3, padding=(0, 1), batch_norm=batch_norm)\n",
        "        layers += make_one_layer(64, 64, kernel_size=3, padding=(0, 1), batch_norm=batch_norm)\n",
        "        layers += make_one_layer(64, 64, kernel_size=3, padding=(0, 1), batch_norm=batch_norm)\n",
        "\n",
        "        layers += make_one_layer(64, 64, kernel_size=5, padding=(0, 2), batch_norm=batch_norm)\n",
        "        layers += make_one_layer(64, 64, kernel_size=5, padding=(0, 2), batch_norm=batch_norm)\n",
        "        layers += make_one_layer(64, 64, kernel_size=5, padding=(0, 2), batch_norm=batch_norm)\n",
        "        layers += make_one_layer(64, 64, kernel_size=5, padding=(0, 2), batch_norm=batch_norm)\n",
        "        self.features = nn.Sequential(*layers)\n",
        "\n",
        "        # x suppose to be N X 64 X 4 X ipm_w/8, need to be reshaped to N X 256 X ipm_w/8 X 1\n",
        "        # TODO: use large kernel_size in x or fc layer to estimate z with global parallelism\n",
        "        dim_rt_layers = []\n",
        "        dim_rt_layers += make_one_layer(256, 128, kernel_size=(5, 1), padding=(2, 0), batch_norm=batch_norm)\n",
        "        dim_rt_layers += [nn.Conv2d(128, self.num_lane_type*self.anchor_dim, kernel_size=(5, 1), padding=(2, 0))]\n",
        "        self.dim_rt = nn.Sequential(*dim_rt_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        # x suppose to be N X 64 X 4 X ipm_w/8, reshape to N X 256 X ipm_w/8 X 1\n",
        "        sizes = x.shape\n",
        "        x = x.reshape(sizes[0], sizes[1]*sizes[2], sizes[3], 1)\n",
        "        x = self.dim_rt(x)\n",
        "        x = x.squeeze(-1).transpose(1, 2)\n",
        "        # apply sigmoid to the probability terms to make it in (0, 1)\n",
        "        for i in range(self.num_lane_type):\n",
        "            x[:, :, i*self.anchor_dim + 2*self.num_y_steps:(i+1)*self.anchor_dim] = \\\n",
        "                torch.sigmoid(x[:, :, i*self.anchor_dim + 2*self.num_y_steps:(i+1)*self.anchor_dim])\n",
        "        return x\n",
        "\n",
        "\n",
        "# The 3D-lanenet composed of image encode, top view pathway, and lane predication head\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, args, input_dim=1, debug=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.no_cuda = args.no_cuda\n",
        "        self.debug = debug\n",
        "        self.pred_cam = args.pred_cam\n",
        "        self.batch_size = args.batch_size\n",
        "        if args.no_centerline:\n",
        "            self.num_lane_type = 1\n",
        "        else:\n",
        "            self.num_lane_type = 3\n",
        "\n",
        "        self.num_y_steps = args.num_y_steps\n",
        "        if args.no_3d:\n",
        "            self.anchor_dim = args.num_y_steps + 1\n",
        "        else:\n",
        "            self.anchor_dim = 3*args.num_y_steps + 1\n",
        "\n",
        "        # define required transformation matrices\n",
        "        # define homographic transformation between image and ipm\n",
        "        org_img_size = np.array([args.org_h, args.org_w])\n",
        "        resize_img_size = np.array([args.resize_h, args.resize_w])\n",
        "        cam_pitch = np.pi / 180 * args.pitch\n",
        "\n",
        "        self.cam_height = torch.tensor(args.cam_height).unsqueeze_(0).expand([self.batch_size, 1]).type(torch.FloatTensor)\n",
        "        self.cam_pitch = torch.tensor(cam_pitch).unsqueeze_(0).expand([self.batch_size, 1]).type(torch.FloatTensor)\n",
        "        self.cam_height_default = torch.tensor(args.cam_height).unsqueeze_(0).expand(self.batch_size).type(torch.FloatTensor)\n",
        "        self.cam_pitch_default = torch.tensor(cam_pitch).unsqueeze_(0).expand(self.batch_size).type(torch.FloatTensor)\n",
        "\n",
        "        # image scale matrix\n",
        "        self.S_im = torch.from_numpy(np.array([[args.resize_w,              0, 0],\n",
        "                                               [            0,  args.resize_h, 0],\n",
        "                                               [            0,              0, 1]], dtype=np.float32))\n",
        "        self.S_im_inv = torch.from_numpy(np.array([[1/np.float(args.resize_w),                         0, 0],\n",
        "                                                   [                        0, 1/np.float(args.resize_h), 0],\n",
        "                                                   [                        0,                         0, 1]], dtype=np.float32))\n",
        "        self.S_im_inv_batch = self.S_im_inv.unsqueeze_(0).expand([self.batch_size, 3, 3]).type(torch.FloatTensor)\n",
        "\n",
        "        # image transform matrix\n",
        "        H_c = homography_crop_resize(org_img_size, args.crop_y, resize_img_size)\n",
        "        self.H_c = torch.from_numpy(H_c).unsqueeze_(0).expand([self.batch_size, 3, 3]).type(torch.FloatTensor)\n",
        "\n",
        "        # camera intrinsic matrix\n",
        "        self.K = torch.from_numpy(args.K).unsqueeze_(0).expand([self.batch_size, 3, 3]).type(torch.FloatTensor)\n",
        "\n",
        "        # homograph ground to camera\n",
        "        # H_g2cam = np.array([[1,                             0,               0],\n",
        "        #                     [0, np.cos(np.pi / 2 + cam_pitch), args.cam_height],\n",
        "        #                     [0, np.sin(np.pi / 2 + cam_pitch),               0]])\n",
        "        H_g2cam = np.array([[1,                             0,               0],\n",
        "                            [0, np.sin(-cam_pitch), args.cam_height],\n",
        "                            [0, np.cos(-cam_pitch),               0]])\n",
        "        self.H_g2cam = torch.from_numpy(H_g2cam).unsqueeze_(0).expand([self.batch_size, 3, 3]).type(torch.FloatTensor)\n",
        "\n",
        "        # transform from ipm normalized coordinates to ground coordinates\n",
        "        H_ipmnorm2g = homography_ipmnorm2g(args.top_view_region)\n",
        "        self.H_ipmnorm2g = torch.from_numpy(H_ipmnorm2g).unsqueeze_(0).expand([self.batch_size, 3, 3]).type(torch.FloatTensor)\n",
        "\n",
        "        # compute the tranformation from ipm norm coords to image norm coords\n",
        "        M_ipm2im = torch.bmm(self.H_g2cam, self.H_ipmnorm2g)\n",
        "        M_ipm2im = torch.bmm(self.K, M_ipm2im)\n",
        "        M_ipm2im = torch.bmm(self.H_c, M_ipm2im)\n",
        "        M_ipm2im = torch.bmm(self.S_im_inv_batch, M_ipm2im)\n",
        "        M_ipm2im = torch.div(M_ipm2im,  M_ipm2im[:, 2, 2].reshape([self.batch_size, 1, 1]).expand([self.batch_size, 3, 3]))\n",
        "        self.M_inv = M_ipm2im\n",
        "\n",
        "        if not self.no_cuda:\n",
        "            self.M_inv = self.M_inv.cuda()\n",
        "            self.S_im = self.S_im.cuda()\n",
        "            self.S_im_inv = self.S_im_inv.cuda()\n",
        "            self.S_im_inv_batch = self.S_im_inv_batch.cuda()\n",
        "            self.H_c = self.H_c.cuda()\n",
        "            self.K = self.K.cuda()\n",
        "            self.H_g2cam = self.H_g2cam.cuda()\n",
        "            self.H_ipmnorm2g = self.H_ipmnorm2g.cuda()\n",
        "            self.cam_height_default = self.cam_height_default.cuda()\n",
        "            self.cam_pitch_default = self.cam_pitch_default.cuda()\n",
        "\n",
        "            # Define network\n",
        "            # the grid considers both src and dst grid normalized\n",
        "            size_top = torch.Size([self.batch_size, np.int(args.ipm_h), np.int(args.ipm_w)])\n",
        "            self.project_layer = ProjectiveGridGenerator(size_top, self.M_inv, args.no_cuda)\n",
        "\n",
        "            # Conv layers to convert original resolution binary map to target resolution with high-dimension\n",
        "            self.encoder = make_layers([8, 'M', 16, 'M', 32, 'M', 64], input_dim, batch_norm=args.batch_norm)\n",
        "\n",
        "            self.lane_out = LanePredictionHead(self.num_lane_type, self.num_y_steps, args.batch_norm)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # compute image features from multiple layers\n",
        "\n",
        "        cam_height = self.cam_height\n",
        "        cam_pitch = self.cam_pitch\n",
        "\n",
        "        # spatial transfer image features to IPM features\n",
        "        grid = self.project_layer(self.M_inv)\n",
        "        x_proj = F.grid_sample(input, grid)\n",
        "\n",
        "        # conv layers to convert original resolution binary map to target resolution with high-dimension\n",
        "        x_feat = self.encoder(x_proj)\n",
        "\n",
        "        # convert top-view features to anchor output\n",
        "        out = self.lane_out(x_feat)\n",
        "\n",
        "        if self.debug:\n",
        "            return out, cam_height, cam_pitch, x_proj, x_feat\n",
        "\n",
        "        return out, cam_height, cam_pitch\n",
        "\n",
        "    def update_projection(self, args, cam_height, cam_pitch):\n",
        "        \"\"\"\n",
        "            Update transformation matrix based on ground-truth cam_height and cam_pitch\n",
        "            This function is \"Mutually Exclusive\" to the updates of M_inv from network prediction\n",
        "        :param args:\n",
        "        :param cam_height:\n",
        "        :param cam_pitch:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        for i in range(self.batch_size):\n",
        "            M, M_inv = homography_im2ipm_norm(args.top_view_region, np.array([args.org_h, args.org_w]),\n",
        "                                              args.crop_y, np.array([args.resize_h, args.resize_w]),\n",
        "                                              cam_pitch[i].data.cpu().numpy(), cam_height[i].data.cpu().numpy(), args.K)\n",
        "            self.M_inv[i] = torch.from_numpy(M_inv).type(torch.FloatTensor)\n",
        "        self.cam_height = cam_height\n",
        "        self.cam_pitch = cam_pitch\n",
        "\n",
        "    def update_projection_for_data_aug(self, aug_mats):\n",
        "        \"\"\"\n",
        "            update transformation matrix when data augmentation have been applied, and the image augmentation matrix are provided\n",
        "            Need to consider both the cases of 1. when using ground-truth cam_height, cam_pitch, update M_inv\n",
        "                                               2. when cam_height, cam_pitch are online estimated, update H_c for later use\n",
        "        \"\"\"\n",
        "        if not self.no_cuda:\n",
        "            aug_mats = aug_mats.cuda()\n",
        "\n",
        "        for i in range(aug_mats.shape[0]):\n",
        "            # update H_c directly\n",
        "            self.H_c[i] = torch.matmul(aug_mats[i], self.H_c[i])\n",
        "            # augmentation need to be applied in unnormalized image coords for M_inv\n",
        "            aug_mats[i] = torch.matmul(torch.matmul(self.S_im_inv, aug_mats[i]), self.S_im)\n",
        "            self.M_inv[i] = torch.matmul(aug_mats[i], self.M_inv[i])\n",
        "\n",
        "\n",
        "# unit test\n",
        "'''\n",
        "if __name__ == '__main__':\n",
        "    import os\n",
        "    from PIL import Image\n",
        "    from torchvision import transforms\n",
        "    import torchvision.transforms.functional as F2\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "    global args\n",
        "    parser = define_args()\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # args.dataset_name = 'tusimple'\n",
        "    # tusimple_config(args)\n",
        "    args.dataset_name = 'sim3d'\n",
        "    sim3d_config(args)\n",
        "    args.pred_cam = True\n",
        "    args.batch_size = 1\n",
        "\n",
        "    # construct model\n",
        "    model = Net(args)\n",
        "    print(model)\n",
        "\n",
        "    # initialize model weights\n",
        "    define_init_weights(model, args.weight_init)\n",
        "\n",
        "    # load in vgg pretrained weights on ImageNet\n",
        "    if args.pretrained:\n",
        "        model.load_pretrained_vgg(args.batch_norm)\n",
        "        print('vgg weights pretrained on ImageNet loaded!')\n",
        "    model = model.cuda()\n",
        "\n",
        "    # prepare input\n",
        "    image = torch.randn(1, 1, args.resize_h, args.resize_w)\n",
        "    image = image.cuda()\n",
        "\n",
        "    # test update of camera height and pitch\n",
        "    cam_height = torch.tensor(1.65).unsqueeze_(0).expand([args.batch_size, 1]).type(torch.FloatTensor)\n",
        "    cam_pitch = torch.tensor(0.1).unsqueeze_(0).expand([args.batch_size, 1]).type(torch.FloatTensor)\n",
        "    # model.update_projection(args, cam_height, cam_pitch)\n",
        "\n",
        "    # inference the model\n",
        "    output_net, pred_height, pred_pitch = model(image)\n",
        "\n",
        "    print(output_net.shape)\n",
        "    print(pred_height)\n",
        "    print(pred_pitch)\n",
        "    '''"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nif __name__ == \\'__main__\\':\\n    import os\\n    from PIL import Image\\n    from torchvision import transforms\\n    import torchvision.transforms.functional as F2\\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\\n\\n    global args\\n    parser = define_args()\\n    args = parser.parse_args()\\n\\n    # args.dataset_name = \\'tusimple\\'\\n    # tusimple_config(args)\\n    args.dataset_name = \\'sim3d\\'\\n    sim3d_config(args)\\n    args.pred_cam = True\\n    args.batch_size = 1\\n\\n    # construct model\\n    model = Net(args)\\n    print(model)\\n\\n    # initialize model weights\\n    define_init_weights(model, args.weight_init)\\n\\n    # load in vgg pretrained weights on ImageNet\\n    if args.pretrained:\\n        model.load_pretrained_vgg(args.batch_norm)\\n        print(\\'vgg weights pretrained on ImageNet loaded!\\')\\n    model = model.cuda()\\n\\n    # prepare input\\n    image = torch.randn(1, 1, args.resize_h, args.resize_w)\\n    image = image.cuda()\\n\\n    # test update of camera height and pitch\\n    cam_height = torch.tensor(1.65).unsqueeze_(0).expand([args.batch_size, 1]).type(torch.FloatTensor)\\n    cam_pitch = torch.tensor(0.1).unsqueeze_(0).expand([args.batch_size, 1]).type(torch.FloatTensor)\\n    # model.update_projection(args, cam_height, cam_pitch)\\n\\n    # inference the model\\n    output_net, pred_height, pred_pitch = model(image)\\n\\n    print(output_net.shape)\\n    print(pred_height)\\n    print(pred_pitch)\\n    '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yQPsWRl_Cut"
      },
      "source": [
        "# ERFNET full network definition for Pytorch\n",
        "# Sept 2017\n",
        "# Eduardo Romera\n",
        "#######################\n",
        "#Pytorch_Generalized_3D_Lane_Detection/networks/erfnet.py /\n",
        "\n",
        "\"\"\"\n",
        "This code is modified from pytorch ERFNET implementation:\n",
        "https://github.com/cardwing/Codes-for-Lane-Detection/tree/master/ERFNet-CULane-PyTorch\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DownsamplerBlock (nn.Module):\n",
        "    def __init__(self, ninput, noutput):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(ninput, noutput-ninput, (3, 3), stride=2, padding=1, bias=True)\n",
        "        self.pool = nn.MaxPool2d(2, stride=2)\n",
        "        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = torch.cat([self.conv(input), self.pool(input)], 1)\n",
        "        output = self.bn(output)\n",
        "        return F.relu(output)\n",
        "    \n",
        "\n",
        "class non_bottleneck_1d (nn.Module):\n",
        "    def __init__(self, chann, dropprob, dilated):        \n",
        "        super().__init__()\n",
        "\n",
        "        self.conv3x1_1 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1,0), bias=True)\n",
        "\n",
        "        self.conv1x3_1 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1), bias=True)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(chann, eps=1e-03)\n",
        "\n",
        "        self.conv3x1_2 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1*dilated,0), bias=True, dilation = (dilated,1))\n",
        "\n",
        "        self.conv1x3_2 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1*dilated), bias=True, dilation = (1, dilated))\n",
        "\n",
        "        self.bn2 = nn.BatchNorm2d(chann, eps=1e-03)\n",
        "\n",
        "        self.dropout = nn.Dropout2d(dropprob)\n",
        "        \n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        output = self.conv3x1_1(input)\n",
        "        output = F.relu(output)\n",
        "        output = self.conv1x3_1(output)\n",
        "        output = self.bn1(output)\n",
        "        output = F.relu(output)\n",
        "\n",
        "        output = self.conv3x1_2(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.conv1x3_2(output)\n",
        "        output = self.bn2(output)\n",
        "\n",
        "        if (self.dropout.p != 0):\n",
        "            output = self.dropout(output)\n",
        "        \n",
        "        return F.relu(output+input)    #+input = identity (residual connection)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.initial_block = DownsamplerBlock(3,16)\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        self.layers.append(DownsamplerBlock(16,64))\n",
        "\n",
        "        for x in range(0, 5):    #5 times\n",
        "           self.layers.append(non_bottleneck_1d(64, 0.1, 1))  \n",
        "\n",
        "        self.layers.append(DownsamplerBlock(64,128))\n",
        "\n",
        "        for x in range(0, 2):    #2 times\n",
        "            self.layers.append(non_bottleneck_1d(128, 0.1, 2))\n",
        "            self.layers.append(non_bottleneck_1d(128, 0.1, 4))\n",
        "            self.layers.append(non_bottleneck_1d(128, 0.1, 8))\n",
        "            self.layers.append(non_bottleneck_1d(128, 0.1, 16))\n",
        "\n",
        "        #only for encoder mode:\n",
        "        self.output_conv = nn.Conv2d(128, num_classes, 1, stride=1, padding=0, bias=True)\n",
        "\n",
        "    def forward(self, input, predict=False):\n",
        "        output = self.initial_block(input)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output)\n",
        "\n",
        "        if predict:\n",
        "            output = self.output_conv(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class UpsamplerBlock (nn.Module):\n",
        "    def __init__(self, ninput, noutput):\n",
        "        super().__init__()\n",
        "        self.conv = nn.ConvTranspose2d(ninput, noutput, 3, stride=2, padding=1, output_padding=1, bias=True)\n",
        "        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.conv(input)\n",
        "        output = self.bn(output)\n",
        "        return F.relu(output)\n",
        "\n",
        "class Decoder (nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        self.layers.append(UpsamplerBlock(128,64))\n",
        "        self.layers.append(non_bottleneck_1d(64, 0, 1))\n",
        "        self.layers.append(non_bottleneck_1d(64, 0, 1))\n",
        "\n",
        "        self.layers.append(UpsamplerBlock(64,16))\n",
        "        self.layers.append(non_bottleneck_1d(16, 0, 1))\n",
        "        self.layers.append(non_bottleneck_1d(16, 0, 1))\n",
        "\n",
        "        self.output_conv = nn.ConvTranspose2d( 16, num_classes, 2, stride=2, padding=0, output_padding=0, bias=True)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = input\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output)\n",
        "\n",
        "        output = self.output_conv(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class Lane_exist (nn.Module):\n",
        "    def __init__(self, num_output):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        self.layers.append(nn.Conv2d(128, 32, (3, 3), stride=1, padding=(4,4), bias=False, dilation = (4,4)))\n",
        "        self.layers.append(nn.BatchNorm2d(32, eps=1e-03))\n",
        "\n",
        "        self.layers_final = nn.ModuleList()\n",
        "\n",
        "        self.layers_final.append(nn.Dropout2d(0.1))\n",
        "        self.layers_final.append(nn.Conv2d(32, 5, (1, 1), stride=1, padding=(0,0), bias=True))\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(2, stride=2)\n",
        "        self.linear1 = nn.Linear(3965, 128)\n",
        "        self.linear2 = nn.Linear(128, 4)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = input\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output)\n",
        "       \n",
        "        output = F.relu(output)\n",
        "\n",
        "        for layer in self.layers_final:\n",
        "            output = layer(output)\n",
        "\n",
        "        output = F.softmax(output, dim=1)\n",
        "        output = self.maxpool(output)\n",
        "        # print(output.shape)\n",
        "        output = output.view(-1, 3965)\n",
        "        output = self.linear1(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.linear2(output)\n",
        "        output = F.sigmoid(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class ERFNet(nn.Module):\n",
        "    def __init__(self, num_classes, partial_bn=False, encoder=None):  #use encoder to pass pretrained encoder\n",
        "        super().__init__()\n",
        "\n",
        "        if (encoder == None):\n",
        "            self.encoder = Encoder(num_classes)\n",
        "        else:\n",
        "            self.encoder = encoder\n",
        "        self.decoder = Decoder(num_classes)\n",
        "        self.lane_exist = Lane_exist(4) # num_output\n",
        "        self.input_mean = [103.939, 116.779, 123.68] # [0, 0, 0]\n",
        "        self.input_std = [1, 1, 1]\n",
        "        self._enable_pbn = partial_bn\n",
        "\n",
        "        if partial_bn:\n",
        "            self.partialBN(True)\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        \"\"\"\n",
        "        Override the default train() to freeze the BN parameters\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        super(ERFNet, self).train(mode)\n",
        "        if self._enable_pbn:\n",
        "            print(\"Freezing BatchNorm2D.\")\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, nn.BatchNorm2d):\n",
        "                    m.eval()\n",
        "                    # shutdown update in frozen mode\n",
        "                    m.weight.requires_grad = False\n",
        "                    m.bias.requires_grad = False\n",
        "\n",
        "    def partialBN(self, enable):\n",
        "        self._enable_pbn = enable\n",
        "\n",
        "    def get_optim_policies(self):\n",
        "        base_weight = []\n",
        "        base_bias = []\n",
        "        base_bn = []\n",
        "\n",
        "        addtional_weight = []\n",
        "        addtional_bias = []\n",
        "        addtional_bn = []\n",
        "\n",
        "        # print(self.modules())\n",
        "\n",
        "        for m in self.encoder.modules(): # self.base_model.modules()\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # print(1)\n",
        "                ps = list(m.parameters())\n",
        "                base_weight.append(ps[0])\n",
        "                if len(ps) == 2:\n",
        "                    base_bias.append(ps[1])\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                # print(2)\n",
        "                base_bn.extend(list(m.parameters()))\n",
        "\n",
        "        for m in self.decoder.modules(): # self.base_model.modules()\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # print(1)\n",
        "                ps = list(m.parameters())\n",
        "                base_weight.append(ps[0])\n",
        "                if len(ps) == 2:\n",
        "                    base_bias.append(ps[1])\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                # print(2)\n",
        "                base_bn.extend(list(m.parameters()))\n",
        "\n",
        "\n",
        "        return [\n",
        "            {\n",
        "                'params': addtional_weight,\n",
        "                'lr_mult': 10,\n",
        "                'decay_mult': 1,\n",
        "                'name': \"addtional weight\"\n",
        "            },\n",
        "            {\n",
        "                'params': addtional_bias,\n",
        "                'lr_mult': 20,\n",
        "                'decay_mult': 1,\n",
        "                'name': \"addtional bias\"\n",
        "            },\n",
        "            {\n",
        "                'params': addtional_bn,\n",
        "                'lr_mult': 10,\n",
        "                'decay_mult': 0,\n",
        "                'name': \"addtional BN scale/shift\"\n",
        "            },\n",
        "            {\n",
        "                'params': base_weight,\n",
        "                'lr_mult': 1,\n",
        "                'decay_mult': 1,\n",
        "                'name': \"base weight\"\n",
        "            },\n",
        "            {\n",
        "                'params': base_bias,\n",
        "                'lr_mult': 2,\n",
        "                'decay_mult': 0,\n",
        "                'name': \"base bias\"\n",
        "            },\n",
        "            {\n",
        "                'params': base_bn,\n",
        "                'lr_mult': 1,\n",
        "                'decay_mult': 0,\n",
        "                'name': \"base BN scale/shift\"\n",
        "            },\n",
        "        ]\n",
        "\n",
        "    def forward(self, input, only_encode=False, no_lane_exist=False):\n",
        "        '''if only_encode:\n",
        "            return self.encoder.forward(input, predict=True)\n",
        "        else:'''\n",
        "        output = self.encoder(input)    #predict=False by default\n",
        "        if no_lane_exist:\n",
        "            return self.decoder.forward(output)\n",
        "        return self.decoder.forward(output), self.lane_exist(output)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0Y897T1KG_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "951e8cdf-c539-4cef-d110-a0d05a3217bc"
      },
      "source": [
        "! pip install tensorboardX"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n",
            "\r\u001b[K     |██▊                             | 10kB 18.9MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 20kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 30kB 14.9MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 40kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 51kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 61kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 71kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 81kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 92kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 102kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 112kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30pAEKlazv-O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "aba6b6d6-d010-4e24-8e4a-c27cc48a3c36"
      },
      "source": [
        "##Training 코드->  'pretrained/erfnet_model_sim3d.tar'가 있어야함\n",
        "\"\"\"\n",
        "Pytorch_Generalized_3D_Lane_Detection/main_train_GenLaneNet_ext.py /\n",
        "\n",
        "The training code for 'Gen-LaneNet' which is a two-stage framework composed of segmentation subnetwork (erfnet)\n",
        "and 3D lane prediction subnetwork (3D-GeoNet). A new lane anchor is integrated in the 3D-GeoNet. The architecture and\n",
        "new anchor design are based on:\n",
        "    \"Gen-laneNet: a generalized and scalable approach for 3D lane detection\", Y.Guo, etal., arxiv 2020\n",
        "The training of Gen-LaneNet is based on a pretrained ERFNet saved in ./pretrained folder. The training is on a\n",
        "synthetic dataset for 3D lane detection proposed in the above paper.\n",
        "Author: Yuliang Guo (33yuliangguo@gmail.com)\n",
        "Date: March, 2020\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim\n",
        "import torch.nn as nn\n",
        "import glob\n",
        "import time\n",
        "import shutil\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from tensorboardX import SummaryWriter\n",
        "#from dataloader.Load_Data_3DLane_ext import *\n",
        "#from networks import Loss_crit, GeoNet3D_ext, erfnet\n",
        "#from tools.utils import *\n",
        "#from tools import eval_3D_lane\n",
        "\n",
        "\n",
        "def load_my_state_dict(model, state_dict):  # custom function to load model when not all dict elements\n",
        "    own_state = model.state_dict()\n",
        "    ckpt_name = []\n",
        "    cnt = 0\n",
        "    for name, param in state_dict.items():\n",
        "        # TODO: why the trained model do not have modules in name?\n",
        "        if name[7:] not in list(own_state.keys()) or 'output_conv' in name:\n",
        "            ckpt_name.append(name)\n",
        "            # continue\n",
        "        own_state[name[7:]].copy_(param)\n",
        "        cnt += 1\n",
        "    print('#reused param: {}'.format(cnt))\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_net():\n",
        "\n",
        "    # Check GPU availability\n",
        "    if not args.no_cuda and not torch.cuda.is_available():\n",
        "        raise Exception(\"No gpu available for usage\")\n",
        "    torch.backends.cudnn.benchmark = args.cudnn\n",
        "\n",
        "    # Define save path\n",
        "    # save_id = 'Model_{}_crit_{}_opt_{}_lr_{}_batch_{}_{}X{}_pretrain_{}_batchnorm_{}_predcam_{}' \\\n",
        "    #           .format(args.mod,\n",
        "    #                   crit_string,\n",
        "    #                   args.optimizer,\n",
        "    #                   args.learning_rate,\n",
        "    #                   args.batch_size,\n",
        "    #                   args.resize_h,\n",
        "    #                   args.resize_w,\n",
        "    #                   args.pretrained,\n",
        "    #                   args.batch_norm,\n",
        "    #                   args.pred_cam)\n",
        "    save_id = args.mod\n",
        "    args.save_path = os.path.join(args.save_path, save_id)\n",
        "    mkdir_if_missing(args.save_path)\n",
        "    mkdir_if_missing(os.path.join(args.save_path, 'example/'))\n",
        "    mkdir_if_missing(os.path.join(args.save_path, 'example/train'))\n",
        "    mkdir_if_missing(os.path.join(args.save_path, 'example/valid'))\n",
        "\n",
        "    # dataloader for training and validation set\n",
        "    val_gt_file = ops.join(args.data_dir, 'test.json')\n",
        "    train_dataset = LaneDataset(args.dataset_dir, ops.join(args.data_dir, 'train.json'), args, data_aug=True, save_std=True)\n",
        "    train_dataset.normalize_lane_label()\n",
        "    train_loader = get_loader(train_dataset, args)\n",
        "    valid_dataset = LaneDataset(args.dataset_dir, val_gt_file, args)\n",
        "    # assign std of valid dataset to be consistent with train dataset\n",
        "    valid_dataset.set_x_off_std(train_dataset._x_off_std)\n",
        "    if not args.no_3d:\n",
        "        valid_dataset.set_z_std(train_dataset._z_std)\n",
        "    valid_dataset.normalize_lane_label()\n",
        "    valid_loader = get_loader(valid_dataset, args)\n",
        "\n",
        "    # extract valid set labels for evaluation later\n",
        "    global valid_set_labels\n",
        "    valid_set_labels = [json.loads(line) for line in open(val_gt_file).readlines()]\n",
        "\n",
        "    # Define network\n",
        "    model1 = ERFNet(args.num_class)\n",
        "    model2 = Net(args, input_dim=args.num_class - 1)\n",
        "    define_init_weights(model2, args.weight_init)\n",
        "\n",
        "    if not args.no_cuda:\n",
        "        # Load model on gpu before passing params to optimizer\n",
        "        model1 = model1.cuda()\n",
        "        model2 = model2.cuda()\n",
        "\n",
        "    # load in vgg pretrained weights\n",
        "    checkpoint = torch.load(args.pretrained_feat_model)\n",
        "    # args.start_epoch = checkpoint['epoch']\n",
        "    model1 = load_my_state_dict(model1, checkpoint['state_dict'])\n",
        "    model1.eval()  # do not back propagate to model1\n",
        "\n",
        "    # Define optimizer and scheduler\n",
        "    optimizer = define_optim(args.optimizer, model2.parameters(),\n",
        "                             args.learning_rate, args.weight_decay)\n",
        "    scheduler = define_scheduler(optimizer, args)\n",
        "\n",
        "    # Define loss criteria\n",
        "    if crit_string == 'loss_gflat_3D':\n",
        "        criterion = Laneline_loss_gflat_3D(args.batch_size, train_dataset.num_types,\n",
        "                                                     train_dataset.anchor_x_steps, train_dataset.anchor_y_steps,\n",
        "                                                     train_dataset._x_off_std, train_dataset._y_off_std,\n",
        "                                                     train_dataset._z_std, args.pred_cam, args.no_cuda)\n",
        "    else:\n",
        "        criterion = Laneline_loss_gflat(train_dataset.num_types, args.num_y_steps, args.pred_cam)\n",
        "\n",
        "    if not args.no_cuda:\n",
        "        criterion = criterion.cuda()\n",
        "\n",
        "    # Logging setup\n",
        "    best_epoch = 0\n",
        "    lowest_loss = np.inf\n",
        "    log_file_name = 'log_train_start_0.txt'\n",
        "\n",
        "    # Tensorboard writer\n",
        "    if not args.no_tb:\n",
        "        global writer\n",
        "        writer = SummaryWriter(os.path.join(args.save_path, 'Tensorboard/'))\n",
        "\n",
        "    # initialize visual saver\n",
        "    vs_saver = Visualizer(args)\n",
        "\n",
        "    # Train, evaluate or resume\n",
        "    args.resume = first_run(args.save_path)\n",
        "    if args.resume and not args.test_mode and not args.evaluate:\n",
        "        path = os.path.join(args.save_path, 'checkpoint_model_epoch_{}.pth.tar'.format(\n",
        "            int(args.resume)))\n",
        "        if os.path.isfile(path):\n",
        "            log_file_name = 'log_train_start_{}.txt'.format(args.resume)\n",
        "            # Redirect stdout\n",
        "            sys.stdout = Logger(os.path.join(args.save_path, log_file_name))\n",
        "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
        "            checkpoint = torch.load(path)\n",
        "            args.start_epoch = checkpoint['epoch']\n",
        "            lowest_loss = checkpoint['loss']\n",
        "            best_epoch = checkpoint['best epoch']\n",
        "            model2.load_state_dict(checkpoint['state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
        "                  .format(args.resume, checkpoint['epoch']))\n",
        "        else:\n",
        "            log_file_name = 'log_train_start_0.txt'\n",
        "            # Redirect stdout\n",
        "            sys.stdout = Logger(os.path.join(args.save_path, log_file_name))\n",
        "            print(\"=> no checkpoint found at '{}'\".format(path))\n",
        "\n",
        "    # Only evaluate\n",
        "    elif args.evaluate:\n",
        "        best_file_name = glob.glob(os.path.join(args.save_path, 'model_best*'))[0]\n",
        "        if os.path.isfile(best_file_name):\n",
        "            sys.stdout = Logger(os.path.join(args.save_path, 'Evaluate.txt'))\n",
        "            print(\"=> loading checkpoint '{}'\".format(best_file_name))\n",
        "            checkpoint = torch.load(best_file_name)\n",
        "            model2.load_state_dict(checkpoint['state_dict'])\n",
        "        else:\n",
        "            print(\"=> no checkpoint found at '{}'\".format(best_file_name))\n",
        "        mkdir_if_missing(os.path.join(args.save_path, 'example/val_vis'))\n",
        "        losses_valid, eval_stats = validate(valid_loader, valid_dataset, model1, model2, criterion, vs_saver, val_gt_file)\n",
        "        return\n",
        "\n",
        "    # Start training from clean slate\n",
        "    else:\n",
        "        # Redirect stdout\n",
        "        sys.stdout = Logger(os.path.join(args.save_path, log_file_name))\n",
        "\n",
        "    # INIT MODEL\n",
        "    print(40*\"=\"+\"\\nArgs:{}\\n\".format(args)+40*\"=\")\n",
        "    print(\"Init model: '{}'\".format(args.mod))\n",
        "    print(\"Number of parameters in model {} is {:.3f}M\".format(\n",
        "        args.mod, sum(tensor.numel() for tensor in model2.parameters())/1e6))\n",
        "\n",
        "    # Start training and validation for nepochs\n",
        "    for epoch in range(args.start_epoch, args.nepochs):\n",
        "        print(\"\\n => Start train set for EPOCH {}\".format(epoch + 1))\n",
        "        # Adjust learning rate\n",
        "        if args.lr_policy is not None and args.lr_policy != 'plateau':\n",
        "            scheduler.step()\n",
        "            lr = optimizer.param_groups[0]['lr']\n",
        "            print('lr is set to {}'.format(lr))\n",
        "\n",
        "        # Define container objects to keep track of multiple losses/metrics\n",
        "        batch_time = AverageMeter()\n",
        "        data_time = AverageMeter()\n",
        "        losses = AverageMeter()\n",
        "\n",
        "        # Specify operation modules\n",
        "        model2.train()\n",
        "\n",
        "        # compute timing\n",
        "        end = time.time()\n",
        "\n",
        "        # Start training loop\n",
        "        for i, (input, seg_maps, gt, idx, gt_hcam, gt_pitch, aug_mat) in tqdm(enumerate(train_loader)):\n",
        "\n",
        "            # Time dataloader\n",
        "            data_time.update(time.time() - end)\n",
        "\n",
        "            # Put inputs on gpu if possible\n",
        "            if not args.no_cuda:\n",
        "                input, gt = input.cuda(non_blocking=True), gt.cuda(non_blocking=True)\n",
        "                seg_maps = seg_maps.cuda(non_blocking=True)\n",
        "                gt_hcam = gt_hcam.cuda()\n",
        "                gt_pitch = gt_pitch.cuda()\n",
        "            input = input.contiguous().float()\n",
        "\n",
        "            if not args.fix_cam and not args.pred_cam:\n",
        "                model2.update_projection(args, gt_hcam, gt_pitch)\n",
        "\n",
        "            # update transformation for data augmentation (only for training)\n",
        "            model2.update_projection_for_data_aug(aug_mat)\n",
        "\n",
        "            # Run model\n",
        "            optimizer.zero_grad()\n",
        "            # Inference model\n",
        "            try:\n",
        "                output1 = model1(input, no_lane_exist=True)\n",
        "                with torch.no_grad():\n",
        "                    # output1 = F.softmax(output1, dim=1)\n",
        "                    output1 = output1.softmax(dim=1)\n",
        "                    output1 = output1 / torch.max(torch.max(output1, dim=2, keepdim=True)[0], dim=3, keepdim=True)[0]\n",
        "                # pred = output1.data.cpu().numpy()[0, 1:, :, :]\n",
        "                # pred = np.max(pred, axis=0)\n",
        "                # cv2.imshow('check probmap', pred)\n",
        "                # cv2.waitKey()\n",
        "                output1 = output1[:, 1:, :, :]\n",
        "                output_net, pred_hcam, pred_pitch = model2(output1)\n",
        "            except RuntimeError as e:\n",
        "                print(\"Batch with idx {} skipped due to inference error\".format(idx.numpy()))\n",
        "                print(e)\n",
        "                continue\n",
        "\n",
        "            # Compute losses on\n",
        "            loss = criterion(output_net, gt, pred_hcam, gt_hcam, pred_pitch, gt_pitch)\n",
        "            losses.update(loss.item(), input.size(0))\n",
        "\n",
        "            # Clip gradients (usefull for instabilities or mistakes in ground truth)\n",
        "            if args.clip_grad_norm != 0:\n",
        "                nn.utils.clip_grad_norm(model2.parameters(), args.clip_grad_norm)\n",
        "\n",
        "            # Setup backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Time trainig iteration\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            pred_pitch = pred_pitch.data.cpu().numpy().flatten()\n",
        "            pred_hcam = pred_hcam.data.cpu().numpy().flatten()\n",
        "            aug_mat = aug_mat.data.cpu().numpy()\n",
        "            output_net = output_net.data.cpu().numpy()\n",
        "            gt = gt.data.cpu().numpy()\n",
        "\n",
        "            # unormalize lane outputs\n",
        "            num_el = input.size(0)\n",
        "            for j in range(num_el):\n",
        "                unormalize_lane_anchor(output_net[j], train_dataset)\n",
        "                unormalize_lane_anchor(gt[j], train_dataset)\n",
        "\n",
        "            # Print info\n",
        "            if (i + 1) % args.print_freq == 0:\n",
        "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                      'Loss {loss.val:.8f} ({loss.avg:.8f})'.format(\n",
        "                       epoch+1, i+1, len(train_loader), batch_time=batch_time,\n",
        "                       data_time=data_time, loss=losses))\n",
        "\n",
        "            # Plot curves in two views\n",
        "            if (i + 1) % args.save_freq == 0:\n",
        "                vs_saver.save_result_new(train_dataset, 'train', epoch, i, idx,\n",
        "                                         input, gt, output_net, pred_pitch, pred_hcam, aug_mat)\n",
        "\n",
        "        losses_valid, eval_stats = validate(valid_loader, valid_dataset, model1, model2, criterion, vs_saver, val_gt_file, epoch)\n",
        "\n",
        "        print(\"===> Average {}-loss on training set is {:.8f}\".format(crit_string, losses.avg))\n",
        "        print(\"===> Average {}-loss on validation set is {:.8f}\".format(crit_string, losses_valid))\n",
        "        print(\"===> Evaluation laneline F-measure: {:3f}\".format(eval_stats[0]))\n",
        "        print(\"===> Evaluation laneline Recall: {:3f}\".format(eval_stats[1]))\n",
        "        print(\"===> Evaluation laneline Precision: {:3f}\".format(eval_stats[2]))\n",
        "        print(\"===> Evaluation centerline F-measure: {:3f}\".format(eval_stats[7]))\n",
        "        print(\"===> Evaluation centerline Recall: {:3f}\".format(eval_stats[8]))\n",
        "        print(\"===> Evaluation centerline Precision: {:3f}\".format(eval_stats[9]))\n",
        "\n",
        "        print(\"===> Last best {}-loss was {:.8f} in epoch {}\".format(crit_string, lowest_loss, best_epoch))\n",
        "\n",
        "        if not args.no_tb:\n",
        "            writer.add_scalars('3D-Lane-Loss', {'Training': losses.avg}, epoch)\n",
        "            writer.add_scalars('3D-Lane-Loss', {'Validation': losses_valid}, epoch)\n",
        "            writer.add_scalars('Evaluation', {'laneline F-measure': eval_stats[0]}, epoch)\n",
        "            writer.add_scalars('Evaluation', {'centerline F-measure': eval_stats[7]}, epoch)\n",
        "        total_score = losses.avg\n",
        "\n",
        "        # Adjust learning_rate if loss plateaued\n",
        "        if args.lr_policy == 'plateau':\n",
        "            scheduler.step(total_score)\n",
        "            lr = optimizer.param_groups[0]['lr']\n",
        "            print('LR plateaued, hence is set to {}'.format(lr))\n",
        "\n",
        "        # File to keep latest epoch\n",
        "        with open(os.path.join(args.save_path, 'first_run.txt'), 'w') as f:\n",
        "            f.write(str(epoch))\n",
        "        # Save model\n",
        "        to_save = False\n",
        "        if total_score < lowest_loss:\n",
        "            to_save = True\n",
        "            best_epoch = epoch+1\n",
        "            lowest_loss = total_score\n",
        "        save_checkpoint({\n",
        "            'epoch': epoch + 1,\n",
        "            'best epoch': best_epoch,\n",
        "            'arch': args.mod,\n",
        "            'state_dict': model2.state_dict(),\n",
        "            'loss': lowest_loss,\n",
        "            'optimizer': optimizer.state_dict()}, to_save, epoch)\n",
        "    if not args.no_tb:\n",
        "        writer.close()\n",
        "\n",
        "\n",
        "def validate(loader, dataset, model1, model2, criterion, vs_saver, val_gt_file, epoch=0):\n",
        "\n",
        "    # Define container to keep track of metric and loss\n",
        "    losses = AverageMeter()\n",
        "    lane_pred_file = ops.join(args.save_path, 'test_pred_file.json')\n",
        "\n",
        "    # Evaluate model\n",
        "    model2.eval()\n",
        "\n",
        "    # Only forward pass, hence no gradients needed\n",
        "    with torch.no_grad():\n",
        "        with open(lane_pred_file, 'w') as jsonFile:\n",
        "            # Start validation loop\n",
        "            for i, (input, seg_maps, gt, idx, gt_hcam, gt_pitch) in tqdm(enumerate(loader)):\n",
        "                if not args.no_cuda:\n",
        "                    input, gt = input.cuda(non_blocking=True), gt.cuda(non_blocking=True)\n",
        "                    seg_maps = seg_maps.cuda(non_blocking=True)\n",
        "                    gt_hcam = gt_hcam.cuda()\n",
        "                    gt_pitch = gt_pitch.cuda()\n",
        "                input = input.contiguous().float()\n",
        "\n",
        "                if not args.fix_cam and not args.pred_cam:\n",
        "                    model2.update_projection(args, gt_hcam, gt_pitch)\n",
        "                # Inference model\n",
        "                try:\n",
        "                    output1 = model1(input, no_lane_exist=True)\n",
        "                    # output1 = F.softmax(output1, dim=1)\n",
        "                    output1 = output1.softmax(dim=1)\n",
        "                    output1 = output1 / torch.max(torch.max(output1, dim=2, keepdim=True)[0], dim=3, keepdim=True)[0]\n",
        "                    output1 = output1[:, 1:, :, :]\n",
        "                    output_net, pred_hcam, pred_pitch = model2(output1)\n",
        "                except RuntimeError as e:\n",
        "                    print(\"Batch with idx {} skipped due to inference error\".format(idx.numpy()))\n",
        "                    print(e)\n",
        "                    continue\n",
        "\n",
        "                # Compute losses on parameters or segmentation\n",
        "                loss = criterion(output_net, gt, pred_hcam, gt_hcam, pred_pitch, gt_pitch)\n",
        "                losses.update(loss.item(), input.size(0))\n",
        "\n",
        "                pred_pitch = pred_pitch.data.cpu().numpy().flatten()\n",
        "                pred_hcam = pred_hcam.data.cpu().numpy().flatten()\n",
        "                output_net = output_net.data.cpu().numpy()\n",
        "                gt = gt.data.cpu().numpy()\n",
        "\n",
        "                # unormalize lane outputs\n",
        "                num_el = input.size(0)\n",
        "                for j in range(num_el):\n",
        "                    unormalize_lane_anchor(output_net[j], dataset)\n",
        "                    unormalize_lane_anchor(gt[j], dataset)\n",
        "\n",
        "                # Print info\n",
        "                if (i + 1) % args.print_freq == 0:\n",
        "                        print('Test: [{0}/{1}]\\t'\n",
        "                              'Loss {loss.val:.8f} ({loss.avg:.8f})'.format(\n",
        "                               i+1, len(loader), loss=losses))\n",
        "\n",
        "                # Plot curves in two views\n",
        "                if (i + 1) % args.save_freq == 0 or args.evaluate:\n",
        "                    vs_saver.save_result_new(dataset, 'valid', epoch, i, idx,\n",
        "                                             input, gt, output_net, pred_pitch, pred_hcam, evaluate=args.evaluate)\n",
        "\n",
        "                # write results and evaluate\n",
        "                for j in range(num_el):\n",
        "                    im_id = idx[j]\n",
        "                    H_g2im, P_g2im, H_crop, H_im2ipm = dataset.transform_mats(idx[j])\n",
        "                    json_line = valid_set_labels[im_id]\n",
        "                    lane_anchors = output_net[j]\n",
        "                    # convert to json output format\n",
        "                    # P_g2gflat = np.matmul(np.linalg.inv(H_g2im), P_g2im)\n",
        "                    lanelines_pred, centerlines_pred, lanelines_prob, centerlines_prob = \\\n",
        "                        compute_3d_lanes_all_prob(lane_anchors, dataset.anchor_dim,\n",
        "                                                  dataset.anchor_x_steps, args.anchor_y_steps, pred_hcam[j])\n",
        "                    json_line[\"laneLines\"] = lanelines_pred\n",
        "                    json_line[\"centerLines\"] = centerlines_pred\n",
        "                    json_line[\"laneLines_prob\"] = lanelines_prob\n",
        "                    json_line[\"centerLines_prob\"] = centerlines_prob\n",
        "                    json.dump(json_line, jsonFile)\n",
        "                    jsonFile.write('\\n')\n",
        "        eval_stats = evaluator.bench_one_submit(lane_pred_file, val_gt_file)\n",
        "\n",
        "        if args.evaluate:\n",
        "            print(\"===> Average {}-loss on validation set is {:.8}\".format(crit_string, losses.avg))\n",
        "            print(\"===> Evaluation on validation set: \\n\"\n",
        "                  \"laneline F-measure {:.8} \\n\"\n",
        "                  \"laneline Recall  {:.8} \\n\"\n",
        "                  \"laneline Precision  {:.8} \\n\"\n",
        "                  \"laneline x error (close)  {:.8} m\\n\"\n",
        "                  \"laneline x error (far)  {:.8} m\\n\"\n",
        "                  \"laneline z error (close)  {:.8} m\\n\"\n",
        "                  \"laneline z error (far)  {:.8} m\\n\\n\"\n",
        "                  \"centerline F-measure {:.8} \\n\"\n",
        "                  \"centerline Recall  {:.8} \\n\"\n",
        "                  \"centerline Precision  {:.8} \\n\"\n",
        "                  \"centerline x error (close)  {:.8} m\\n\"\n",
        "                  \"centerline x error (far)  {:.8} m\\n\"\n",
        "                  \"centerline z error (close)  {:.8} m\\n\"\n",
        "                  \"centerline z error (far)  {:.8} m\\n\".format(eval_stats[0], eval_stats[1],\n",
        "                                                               eval_stats[2], eval_stats[3],\n",
        "                                                               eval_stats[4], eval_stats[5],\n",
        "                                                               eval_stats[6], eval_stats[7],\n",
        "                                                               eval_stats[8], eval_stats[9],\n",
        "                                                               eval_stats[10], eval_stats[11],\n",
        "                                                               eval_stats[12], eval_stats[13]))\n",
        "\n",
        "        return losses.avg, eval_stats\n",
        "\n",
        "\n",
        "def save_checkpoint(state, to_copy, epoch):\n",
        "    filepath = os.path.join(args.save_path, 'checkpoint_model_epoch_{}.pth.tar'.format(epoch))\n",
        "    torch.save(state, filepath)\n",
        "    if to_copy:\n",
        "        if epoch > 0:\n",
        "            lst = glob.glob(os.path.join(args.save_path, 'model_best*'))\n",
        "            if len(lst) != 0:\n",
        "                os.remove(lst[0])\n",
        "        shutil.copyfile(filepath, os.path.join(args.save_path, \n",
        "            'model_best_epoch_{}.pth.tar'.format(epoch)))\n",
        "        print(\"Best model copied\")\n",
        "    if epoch > 0:\n",
        "        prev_checkpoint_filename = os.path.join(args.save_path, \n",
        "                'checkpoint_model_epoch_{}.pth.tar'.format(epoch-1))\n",
        "        if os.path.exists(prev_checkpoint_filename):\n",
        "            os.remove(prev_checkpoint_filename)\n",
        "\n",
        "'''\n",
        "if __name__ == '__main__':\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "    global args\n",
        "    args = define_args()\n",
        "    #args = parser.parse_args()\n",
        "\n",
        "    # dataset_name: 'standard' / 'rare_subset' / 'illus_chg'\n",
        "    args.dataset_name = 'illus_chg'\n",
        "    args.dataset_dir = '/content/drive/Shareddrives/colab/Apollo_Sim_3D_Lane_Release'\n",
        "    #args.data_dir = ops.join('data_splits', args.dataset_name)\n",
        "    #args.save_path = ops.join('data_splits', args.dataset_name)\n",
        "    args.data_dir='/content/drive/Shareddrives/colab/data_splits/illus_chg'\n",
        "    args.save_path='/content/drive/Shareddrives/colab/data_splits/illus_chg'\n",
        "    # load configuration for certain dataset\n",
        "    global evaluator\n",
        "    sim3d_config(args)\n",
        "    # define evaluator\n",
        "    #evaluator = eval_3D_lane.LaneEval(args)\n",
        "    evaluator=LaneEval(args)\n",
        "    args.prob_th = 0.5\n",
        "\n",
        "    # define the network model\n",
        "    args.num_class = 2  # 1 background + n lane labels\n",
        "    args.pretrained_feat_model = '/content/drive/Shareddrives/colab/erfnet_model_sim3d.tar'\n",
        "    args.mod = 'Gen_LaneNet_ext'\n",
        "    args.y_ref = 5  # new anchor prefer closer range gt assign\n",
        "    global crit_string\n",
        "    crit_string = 'loss_gflat'\n",
        "\n",
        "    # for the case only running evaluation\n",
        "    args.evaluate = False\n",
        "\n",
        "    # settings for save and visualize\n",
        "    args.print_freq = 50\n",
        "    args.save_freq = 50\n",
        "\n",
        "    # run the training\n",
        "    train_net()\n",
        "    '''"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nif __name__ == \\'__main__\\':\\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\\n\\n    global args\\n    args = define_args()\\n    #args = parser.parse_args()\\n\\n    # dataset_name: \\'standard\\' / \\'rare_subset\\' / \\'illus_chg\\'\\n    args.dataset_name = \\'illus_chg\\'\\n    args.dataset_dir = \\'/content/drive/Shareddrives/colab/Apollo_Sim_3D_Lane_Release\\'\\n    #args.data_dir = ops.join(\\'data_splits\\', args.dataset_name)\\n    #args.save_path = ops.join(\\'data_splits\\', args.dataset_name)\\n    args.data_dir=\\'/content/drive/Shareddrives/colab/data_splits/illus_chg\\'\\n    args.save_path=\\'/content/drive/Shareddrives/colab/data_splits/illus_chg\\'\\n    # load configuration for certain dataset\\n    global evaluator\\n    sim3d_config(args)\\n    # define evaluator\\n    #evaluator = eval_3D_lane.LaneEval(args)\\n    evaluator=LaneEval(args)\\n    args.prob_th = 0.5\\n\\n    # define the network model\\n    args.num_class = 2  # 1 background + n lane labels\\n    args.pretrained_feat_model = \\'/content/drive/Shareddrives/colab/erfnet_model_sim3d.tar\\'\\n    args.mod = \\'Gen_LaneNet_ext\\'\\n    args.y_ref = 5  # new anchor prefer closer range gt assign\\n    global crit_string\\n    crit_string = \\'loss_gflat\\'\\n\\n    # for the case only running evaluation\\n    args.evaluate = False\\n\\n    # settings for save and visualize\\n    args.print_freq = 50\\n    args.save_freq = 50\\n\\n    # run the training\\n    train_net()\\n    '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBBMKKCa6vnr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "e9cf2829-a5ac-4ee4-817b-fcfb75be6959"
      },
      "source": [
        "# __main__ 은 batch testing용\n",
        "\"\"\"\n",
        "Pytorch_Generalized_3D_Lane_Detection/main_test_GenLaneNet_ext.py \n",
        "Batch test code for Gen-LaneNet with new anchor extension. It predicts 3D lanes per image.\n",
        "Author: Yuliang Guo (33yuliangguo@gmail.com)\n",
        "Date: March, 2020\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "#from dataloader.Load_Data_3DLane_ext import *\n",
        "#from networks import GeoNet3D_ext, erfnet\n",
        "#from tools.utils import *\n",
        "#from tools import eval_3D_lane\n",
        "\n",
        "\n",
        "def load_my_state_dict(model, state_dict):  # custom function to load model when not all dict elements\n",
        "    own_state = model.state_dict()\n",
        "    ckpt_name = []\n",
        "    cnt = 0\n",
        "    for name, param in state_dict.items():\n",
        "        if name[7:] not in list(own_state.keys()) or 'output_conv' in name:\n",
        "            ckpt_name.append(name)\n",
        "            # continue\n",
        "        own_state[name[7:]].copy_(param)\n",
        "        cnt += 1\n",
        "    print('#reused param: {}'.format(cnt))\n",
        "    return model\n",
        "\n",
        "\n",
        "def deploy(args, loader, dataset, model_seg, model_geo, vs_saver, test_gt_file, vis=False, epoch=0):\n",
        "\n",
        "    # model deploy mode\n",
        "    model_geo.eval()\n",
        "\n",
        "    # read ground-truth lanes for later evaluation\n",
        "    test_set_labels = [json.loads(line) for line in open(test_gt_file).readlines()]\n",
        "\n",
        "    # Only forward pass, hence no gradients needed\n",
        "    with torch.no_grad():\n",
        "        with open(lane_pred_file, 'w') as jsonFile:\n",
        "            # Start validation loop\n",
        "            for i, (input, _, gt, idx, gt_hcam, gt_pitch) in tqdm(enumerate(loader)):\n",
        "                if not args.no_cuda:\n",
        "                    input, gt = input.cuda(non_blocking=True), gt.cuda(non_blocking=True)\n",
        "                    input = input.float()\n",
        "                input = input.contiguous()\n",
        "                input = torch.autograd.Variable(input)\n",
        "\n",
        "                # if not args.fix_cam and not args.pred_cam:\n",
        "                # ATTENTION: here requires to update with test dataset args\n",
        "                model_geo.update_projection(args, gt_hcam, gt_pitch)\n",
        "\n",
        "                # Evaluate model\n",
        "                try:\n",
        "                    output_seg = model_seg(input, no_lane_exist=True)\n",
        "                    # output1 = F.softmax(output1, dim=1)\n",
        "                    output_seg = output_seg.softmax(dim=1)\n",
        "                    output_seg = output_seg / torch.max(torch.max(output_seg, dim=2, keepdim=True)[0], dim=3, keepdim=True)[0]\n",
        "                    output_seg = output_seg[:, 1:, :, :]\n",
        "                    output_geo, pred_hcam, pred_pitch = model_geo(output_seg)\n",
        "                except RuntimeError as e:\n",
        "                    print(\"Batch with idx {} skipped due to singular matrix\".format(idx.numpy()))\n",
        "                    print(e)\n",
        "                    continue\n",
        "\n",
        "                gt = gt.data.cpu().numpy()\n",
        "                output_geo = output_geo.data.cpu().numpy()\n",
        "                pred_pitch = pred_pitch.data.cpu().numpy().flatten()\n",
        "                pred_hcam = pred_hcam.data.cpu().numpy().flatten()\n",
        "\n",
        "                # unormalize lane outputs\n",
        "                num_el = input.size(0)\n",
        "                for j in range(num_el):\n",
        "                    unormalize_lane_anchor(gt[j], dataset)\n",
        "                    unormalize_lane_anchor(output_geo[j], dataset)\n",
        "\n",
        "                if vis:\n",
        "                    # Plot curves in two views\n",
        "                    vs_saver.save_result_new(dataset, args.vis_folder, epoch, i, idx,\n",
        "                                             input, gt, output_geo, pred_pitch, pred_hcam, evaluate=vis)\n",
        "\n",
        "                # visualize and write results\n",
        "                for j in range(num_el):\n",
        "                    im_id = idx[j]\n",
        "                    H_g2im, P_g2im, H_crop, H_im2ipm = dataset.transform_mats(idx[j])\n",
        "                    \"\"\"\n",
        "                        save results in test dataset format\n",
        "                    \"\"\"\n",
        "                    json_line = test_set_labels[im_id]\n",
        "                    lane_anchors = output_geo[j]\n",
        "                    # convert to json output format\n",
        "                    lanelines_pred, centerlines_pred, lanelines_prob, centerlines_prob =\\\n",
        "                        compute_3d_lanes_all_prob(lane_anchors, dataset.anchor_dim,\n",
        "                                                  dataset.anchor_x_steps, args.anchor_y_steps, pred_hcam[j])\n",
        "                    json_line[\"laneLines\"] = lanelines_pred\n",
        "                    json_line[\"centerLines\"] = centerlines_pred\n",
        "                    json_line[\"laneLines_prob\"] = lanelines_prob\n",
        "                    json_line[\"centerLines_prob\"] = centerlines_prob\n",
        "                    json.dump(json_line, jsonFile)\n",
        "                    jsonFile.write('\\n')\n",
        "\n",
        "        # evaluation at varying thresholds\n",
        "        eval_stats_pr = evaluator.bench_one_submit_varying_probs(lane_pred_file, test_gt_file)\n",
        "        max_f_prob = eval_stats_pr['max_F_prob_th']\n",
        "\n",
        "        # evaluate at the point with max F-measure. Additional eval of position error.\n",
        "        eval_stats = evaluator.bench_one_submit(lane_pred_file, test_gt_file, prob_th=max_f_prob)\n",
        "\n",
        "        print(\"Metrics: AP, F-score, x error (close), x error (far), z error (close), z error (far)\")\n",
        "        print(\n",
        "            \"Laneline:  {:.3}  {:.3}  {:.3}  {:.3}  {:.3}  {:.3}\".format(eval_stats_pr['laneline_AP'], eval_stats[0],\n",
        "                                                                         eval_stats[3], eval_stats[4],\n",
        "                                                                         eval_stats[5], eval_stats[6]))\n",
        "        print(\"Centerline:  {:.3}  {:.3}  {:.3}  {:.3}  {:.3}  {:.3}\".format(eval_stats_pr['centerline_AP'], eval_stats[7],\n",
        "                                                                             eval_stats[10], eval_stats[11],\n",
        "                                                                             eval_stats[12], eval_stats[13]))\n",
        "\n",
        "    return eval_stats\n",
        "\n",
        "'''\n",
        "if __name__ == '__main__':\n",
        "  print(\"hi\")\n",
        "  \n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "    args = define_args()\n",
        "    #args = parser.parse_args()\n",
        "\n",
        "    # manual settings\n",
        "    args.dataset_dir = '/content/drive/Shareddrives/colab/Apollo_Sim_3D_Lane_Release/'  # raw data dir\n",
        "    args.dataset_name = 'illus_chg'  # choose a data split 'standard' / 'rare_subset' / 'illus_chg'\n",
        "    args.mod = 'Gen_LaneNet_ext'  # model name\n",
        "    test_name = 'test'  # test set name\n",
        "    #pretrained_feat_model = 'pretrained/erfnet_model_sim3d.tar'\n",
        "    pretrained_feat_model='/content/drive/Shareddrives/colab/erfnet_model_sim3d.tar'\n",
        "    vis = False  # choose to save visualization result\n",
        "\n",
        "    # generate relative paths\n",
        "    args.data_dir = '/content/drive/Shareddrives/colab/data_splits/illus_chg'\n",
        "   # args.save_path = os.path.join(ops.join('data_splits', args.dataset_name), args.mod)\n",
        "    args.save_path ='/content/drive/Shareddrives/colab/data_splits/illus_chg/Gen_LaneNet_ext'\n",
        "    args.vis_folder = test_name + '_vis'\n",
        "    if vis:\n",
        "        mkdir_if_missing(os.path.join(args.save_path, 'example/' + args.vis_folder))\n",
        "    test_gt_file = ops.join(args.data_dir, test_name + '.json')\n",
        "    lane_pred_file = ops.join(args.save_path, test_name + '_pred_file.json')\n",
        "\n",
        "    # load configuration for certain dataset\n",
        "    sim3d_config(args)\n",
        "    args.y_ref = 5\n",
        "    # define evaluator\n",
        "    #evaluator = eval_3D_lane.LaneEval(args)\n",
        "    evaluator = LaneEval(args)\n",
        "    args.prob_th = 0.5\n",
        "\n",
        "    # Check GPU availability\n",
        "    if not args.no_cuda and not torch.cuda.is_available():\n",
        "        raise Exception(\"No gpu available for usage\")\n",
        "    torch.backends.cudnn.benchmark = args.cudnn\n",
        "\n",
        "    # Define network\n",
        "    #model_seg = erfnet.ERFNet(2)  # 2-class model\n",
        "    model_seg = ERFNet(2)\n",
        "    #model_geo = GeoNet3D_ext.Net(args)\n",
        "    model_geo = Net(args)\n",
        "    define_init_weights(model_geo, args.weight_init)\n",
        "\n",
        "    if not args.no_cuda:\n",
        "        # Load model on gpu before passing params to optimizer\n",
        "        model_seg = model_seg.cuda()\n",
        "        model_geo = model_geo.cuda()\n",
        "\n",
        "    # load segmentation model\n",
        "    checkpoint = torch.load(pretrained_feat_model)\n",
        "    model_seg = load_my_state_dict(model_seg, checkpoint['state_dict'])\n",
        "    model_seg.eval()  # do not back propagate to model1\n",
        "\n",
        "    # load geometry model\n",
        "    best_test_name = glob.glob(os.path.join(args.save_path, 'model_best*'))[0]\n",
        "    if os.path.isfile(best_test_name):\n",
        "        sys.stdout = Logger(os.path.join(args.save_path, 'Evaluate.txt'))\n",
        "        print(\"=> loading checkpoint '{}'\".format(best_test_name))\n",
        "        checkpoint = torch.load(best_test_name)\n",
        "        model_geo.load_state_dict(checkpoint['state_dict'])\n",
        "    else:\n",
        "        print(\"=> no checkpoint found at '{}'\".format(best_test_name))\n",
        "\n",
        "    # Data loader\n",
        "    test_dataset = LaneDataset(args.dataset_dir, test_gt_file, args)\n",
        "    # assign std of valid dataset to be consistent with train dataset\n",
        "    with open(ops.join(args.data_dir, 'geo_anchor_std.json')) as f:\n",
        "        anchor_std = json.load(f)\n",
        "    test_dataset.set_x_off_std(anchor_std['x_off_std'])\n",
        "    if not args.no_3d:\n",
        "        test_dataset.set_z_std(anchor_std['z_std'])\n",
        "    test_dataset.normalize_lane_label()\n",
        "    test_loader = get_loader(test_dataset, args)\n",
        "\n",
        "    # initialize visual saver\n",
        "    vs_saver = Visualizer(args, args.vis_folder)\n",
        "\n",
        "    mkdir_if_missing(os.path.join(args.save_path, 'example/' + args.vis_folder))\n",
        "    eval_stats = deploy(args, test_loader, test_dataset, model_seg, model_geo, vs_saver, test_gt_file, vis)\n",
        "'''\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nif __name__ == \\'__main__\\':\\n  print(\"hi\")\\n  \\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\\n\\n    args = define_args()\\n    #args = parser.parse_args()\\n\\n    # manual settings\\n    args.dataset_dir = \\'/content/drive/Shareddrives/colab/Apollo_Sim_3D_Lane_Release/\\'  # raw data dir\\n    args.dataset_name = \\'illus_chg\\'  # choose a data split \\'standard\\' / \\'rare_subset\\' / \\'illus_chg\\'\\n    args.mod = \\'Gen_LaneNet_ext\\'  # model name\\n    test_name = \\'test\\'  # test set name\\n    #pretrained_feat_model = \\'pretrained/erfnet_model_sim3d.tar\\'\\n    pretrained_feat_model=\\'/content/drive/Shareddrives/colab/erfnet_model_sim3d.tar\\'\\n    vis = False  # choose to save visualization result\\n\\n    # generate relative paths\\n    args.data_dir = \\'/content/drive/Shareddrives/colab/data_splits/illus_chg\\'\\n   # args.save_path = os.path.join(ops.join(\\'data_splits\\', args.dataset_name), args.mod)\\n    args.save_path =\\'/content/drive/Shareddrives/colab/data_splits/illus_chg/Gen_LaneNet_ext\\'\\n    args.vis_folder = test_name + \\'_vis\\'\\n    if vis:\\n        mkdir_if_missing(os.path.join(args.save_path, \\'example/\\' + args.vis_folder))\\n    test_gt_file = ops.join(args.data_dir, test_name + \\'.json\\')\\n    lane_pred_file = ops.join(args.save_path, test_name + \\'_pred_file.json\\')\\n\\n    # load configuration for certain dataset\\n    sim3d_config(args)\\n    args.y_ref = 5\\n    # define evaluator\\n    #evaluator = eval_3D_lane.LaneEval(args)\\n    evaluator = LaneEval(args)\\n    args.prob_th = 0.5\\n\\n    # Check GPU availability\\n    if not args.no_cuda and not torch.cuda.is_available():\\n        raise Exception(\"No gpu available for usage\")\\n    torch.backends.cudnn.benchmark = args.cudnn\\n\\n    # Define network\\n    #model_seg = erfnet.ERFNet(2)  # 2-class model\\n    model_seg = ERFNet(2)\\n    #model_geo = GeoNet3D_ext.Net(args)\\n    model_geo = Net(args)\\n    define_init_weights(model_geo, args.weight_init)\\n\\n    if not args.no_cuda:\\n        # Load model on gpu before passing params to optimizer\\n        model_seg = model_seg.cuda()\\n        model_geo = model_geo.cuda()\\n\\n    # load segmentation model\\n    checkpoint = torch.load(pretrained_feat_model)\\n    model_seg = load_my_state_dict(model_seg, checkpoint[\\'state_dict\\'])\\n    model_seg.eval()  # do not back propagate to model1\\n\\n    # load geometry model\\n    best_test_name = glob.glob(os.path.join(args.save_path, \\'model_best*\\'))[0]\\n    if os.path.isfile(best_test_name):\\n        sys.stdout = Logger(os.path.join(args.save_path, \\'Evaluate.txt\\'))\\n        print(\"=> loading checkpoint \\'{}\\'\".format(best_test_name))\\n        checkpoint = torch.load(best_test_name)\\n        model_geo.load_state_dict(checkpoint[\\'state_dict\\'])\\n    else:\\n        print(\"=> no checkpoint found at \\'{}\\'\".format(best_test_name))\\n\\n    # Data loader\\n    test_dataset = LaneDataset(args.dataset_dir, test_gt_file, args)\\n    # assign std of valid dataset to be consistent with train dataset\\n    with open(ops.join(args.data_dir, \\'geo_anchor_std.json\\')) as f:\\n        anchor_std = json.load(f)\\n    test_dataset.set_x_off_std(anchor_std[\\'x_off_std\\'])\\n    if not args.no_3d:\\n        test_dataset.set_z_std(anchor_std[\\'z_std\\'])\\n    test_dataset.normalize_lane_label()\\n    test_loader = get_loader(test_dataset, args)\\n\\n    # initialize visual saver\\n    vs_saver = Visualizer(args, args.vis_folder)\\n\\n    mkdir_if_missing(os.path.join(args.save_path, \\'example/\\' + args.vis_folder))\\n    eval_stats = deploy(args, test_loader, test_dataset, model_seg, model_geo, vs_saver, test_gt_file, vis)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ8OVNcYvzJM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c338aecb-92cb-430c-e29a-22563808f33d"
      },
      "source": [
        "! whereis datalab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datalab:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCsIv44zHsxe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "1316a3ca-5f1a-4f7b-f8a8-235bd0fd79ca"
      },
      "source": [
        "#Pytorch_Generalized_3D_Lane_Detection/tools/eval_3D_lane.py /\n",
        "\"\"\"\n",
        "Description: This code is to evaluate 3D lane detection. The optimal matching between ground-truth set and predicted\n",
        "set of lanes are sought via solving a min cost flow.\n",
        "Evaluation metrics includes:\n",
        "    Average Precision (AP)\n",
        "    Max F-scores\n",
        "    x error close (0 - 40 m)\n",
        "    x error far (0 - 100 m)\n",
        "    z error close (0 - 40 m)\n",
        "    z error far (0 - 100 m)\n",
        "Reference: \"Gen-LaneNet: Generalized and Scalable Approach for 3D Lane Detection\". Y. Guo. etal. 2020\n",
        "Author: Yuliang Guo (33yuliangguo@gmail.com)\n",
        "Date: March, 2020\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import os.path as ops\n",
        "import copy\n",
        "import math\n",
        "import ujson as json\n",
        "from scipy.interpolate import interp1d\n",
        "import matplotlib\n",
        "#from tools.utils import *\n",
        "#from tools.MinCostFlow import SolveMinCostFlow\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (35, 30)\n",
        "plt.rcParams.update({'font.size': 25})\n",
        "plt.rcParams.update({'font.weight': 'semibold'})\n",
        "\n",
        "color = [[0, 0, 255],  # red\n",
        "         [0, 255, 0],  # green\n",
        "         [255, 0, 255],  # purple\n",
        "         [255, 255, 0]]  # cyan\n",
        "\n",
        "vis_min_y = 5\n",
        "vis_max_y = 80\n",
        "\n",
        "\n",
        "class LaneEval(object):\n",
        "    def __init__(self, args):\n",
        "        self.dataset_dir = args.dataset_dir\n",
        "        self.K = args.K\n",
        "        self.no_centerline = args.no_centerline\n",
        "        self.resize_h = args.resize_h\n",
        "        self.resize_w = args.resize_w\n",
        "        self.H_crop = homography_crop_resize([args.org_h, args.org_w], args.crop_y, [args.resize_h, args.resize_w])\n",
        "\n",
        "        self.x_min = args.top_view_region[0, 0]\n",
        "        self.x_max = args.top_view_region[1, 0]\n",
        "        self.y_min = args.top_view_region[2, 1]\n",
        "        self.y_max = args.top_view_region[0, 1]\n",
        "        self.y_samples = np.linspace(self.y_min, self.y_max, num=100, endpoint=False)\n",
        "        # self.y_samples = np.linspace(min_y, max_y, num=100, endpoint=False)\n",
        "        self.dist_th = 1.5\n",
        "        self.ratio_th = 0.75\n",
        "        self.close_range = 40\n",
        "\n",
        "    def bench(self, pred_lanes, gt_lanes, gt_visibility, raw_file, gt_cam_height, gt_cam_pitch, vis, ax1, ax2):\n",
        "        \"\"\"\n",
        "            Matching predicted lanes and ground-truth lanes in their IPM projection, ignoring z attributes.\n",
        "            x error, y_error, and z error are all considered, although the matching does not rely on z\n",
        "            The input of prediction and ground-truth lanes are in ground coordinate, x-right, y-forward, z-up\n",
        "            The fundamental assumption is: 1. there are no two points from different lanes with identical x, y\n",
        "                                              but different z's\n",
        "                                           2. there are no two points from a single lane having identical x, y\n",
        "                                              but different z's\n",
        "            If the interest area is within the current drivable road, the above assumptions are almost always valid.\n",
        "        :param pred_lanes: N X 2 or N X 3 lists depending on 2D or 3D\n",
        "        :param gt_lanes: N X 2 or N X 3 lists depending on 2D or 3D\n",
        "        :param raw_file: file path rooted in dataset folder\n",
        "        :param gt_cam_height: camera height given in ground-truth data\n",
        "        :param gt_cam_pitch: camera pitch given in ground-truth data\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        # change this properly\n",
        "        close_range_idx = np.where(self.y_samples > self.close_range)[0][0]\n",
        "\n",
        "        r_lane, p_lane = 0., 0.\n",
        "        x_error_close = []\n",
        "        x_error_far = []\n",
        "        z_error_close = []\n",
        "        z_error_far = []\n",
        "\n",
        "        # only keep the visible portion\n",
        "        gt_lanes = [prune_3d_lane_by_visibility(np.array(gt_lane), np.array(gt_visibility[k])) for k, gt_lane in\n",
        "                    enumerate(gt_lanes)]\n",
        "        gt_lanes = [lane for lane in gt_lanes if lane.shape[0] > 1]\n",
        "        # only consider those gt lanes overlapping with sampling range\n",
        "        gt_lanes = [lane for lane in gt_lanes if lane[0, 1] < self.y_samples[-1] and lane[-1, 1] > self.y_samples[0]]\n",
        "        gt_lanes = [prune_3d_lane_by_range(np.array(gt_lane), 3 * self.x_min, 3 * self.x_max) for gt_lane in gt_lanes]\n",
        "        gt_lanes = [lane for lane in gt_lanes if lane.shape[0] > 1]\n",
        "        cnt_gt = len(gt_lanes)\n",
        "        cnt_pred = len(pred_lanes)\n",
        "\n",
        "        gt_visibility_mat = np.zeros((cnt_gt, 100))\n",
        "        pred_visibility_mat = np.zeros((cnt_pred, 100))\n",
        "        # resample gt and pred at y_samples\n",
        "        for i in range(cnt_gt):\n",
        "            min_y = np.min(np.array(gt_lanes[i])[:, 1])\n",
        "            max_y = np.max(np.array(gt_lanes[i])[:, 1])\n",
        "            x_values, z_values, visibility_vec = resample_laneline_in_y(np.array(gt_lanes[i]), self.y_samples,\n",
        "                                                                        out_vis=True)\n",
        "            gt_lanes[i] = np.vstack([x_values, z_values]).T\n",
        "            gt_visibility_mat[i, :] = np.logical_and(x_values >= self.x_min,\n",
        "                                                     np.logical_and(x_values <= self.x_max,\n",
        "                                                                    np.logical_and(self.y_samples >= min_y,\n",
        "                                                                                   self.y_samples <= max_y)))\n",
        "            gt_visibility_mat[i, :] = np.logical_and(gt_visibility_mat[i, :], visibility_vec)\n",
        "\n",
        "        for i in range(cnt_pred):\n",
        "            # # ATTENTION: ensure y mono increase before interpolation: but it can reduce size\n",
        "            # pred_lanes[i] = make_lane_y_mono_inc(np.array(pred_lanes[i]))\n",
        "            # pred_lane = prune_3d_lane_by_range(np.array(pred_lanes[i]), self.x_min, self.x_max)\n",
        "            min_y = np.min(np.array(pred_lanes[i])[:, 1])\n",
        "            max_y = np.max(np.array(pred_lanes[i])[:, 1])\n",
        "            x_values, z_values, visibility_vec = resample_laneline_in_y(np.array(pred_lanes[i]), self.y_samples,\n",
        "                                                                        out_vis=True)\n",
        "            pred_lanes[i] = np.vstack([x_values, z_values]).T\n",
        "            pred_visibility_mat[i, :] = np.logical_and(x_values >= self.x_min,\n",
        "                                                       np.logical_and(x_values <= self.x_max,\n",
        "                                                                      np.logical_and(self.y_samples >= min_y,\n",
        "                                                                                     self.y_samples <= max_y)))\n",
        "            pred_visibility_mat[i, :] = np.logical_and(pred_visibility_mat[i, :], visibility_vec)\n",
        "            # pred_visibility_mat[i, :] = np.logical_and(x_values >= self.x_min, x_values <= self.x_max)\n",
        "\n",
        "        adj_mat = np.zeros((cnt_gt, cnt_pred), dtype=np.int)\n",
        "        cost_mat = np.zeros((cnt_gt, cnt_pred), dtype=np.int)\n",
        "        cost_mat.fill(1000)\n",
        "        num_match_mat = np.zeros((cnt_gt, cnt_pred), dtype=np.float)\n",
        "        x_dist_mat_close = np.zeros((cnt_gt, cnt_pred), dtype=np.float)\n",
        "        x_dist_mat_close.fill(1000.)\n",
        "        x_dist_mat_far = np.zeros((cnt_gt, cnt_pred), dtype=np.float)\n",
        "        x_dist_mat_far.fill(1000.)\n",
        "        z_dist_mat_close = np.zeros((cnt_gt, cnt_pred), dtype=np.float)\n",
        "        z_dist_mat_close.fill(1000.)\n",
        "        z_dist_mat_far = np.zeros((cnt_gt, cnt_pred), dtype=np.float)\n",
        "        z_dist_mat_far.fill(1000.)\n",
        "        # compute curve to curve distance\n",
        "        for i in range(cnt_gt):\n",
        "            for j in range(cnt_pred):\n",
        "                x_dist = np.abs(gt_lanes[i][:, 0] - pred_lanes[j][:, 0])\n",
        "                z_dist = np.abs(gt_lanes[i][:, 1] - pred_lanes[j][:, 1])\n",
        "                euclidean_dist = np.sqrt(x_dist ** 2 + z_dist ** 2)\n",
        "\n",
        "                # apply visibility to penalize different partial matching accordingly\n",
        "                euclidean_dist[\n",
        "                    np.logical_or(gt_visibility_mat[i, :] < 0.5, pred_visibility_mat[j, :] < 0.5)] = self.dist_th\n",
        "\n",
        "                # if np.average(euclidean_dist) < 2*self.dist_th: # don't prune here to encourage finding perfect match\n",
        "                num_match_mat[i, j] = np.sum(euclidean_dist < self.dist_th)\n",
        "                adj_mat[i, j] = 1\n",
        "                # ATTENTION: use the sum as int type to meet the requirements of min cost flow optimization (int type)\n",
        "                # using num_match_mat as cost does not work?\n",
        "                cost_mat[i, j] = np.sum(euclidean_dist).astype(np.int)\n",
        "                # cost_mat[i, j] = num_match_mat[i, j]\n",
        "\n",
        "                # use the both visible portion to calculate distance error\n",
        "                both_visible_indices = np.logical_and(gt_visibility_mat[i, :] > 0.5, pred_visibility_mat[j, :] > 0.5)\n",
        "                if np.sum(both_visible_indices[:close_range_idx]) > 0:\n",
        "                    x_dist_mat_close[i, j] = np.sum(\n",
        "                        x_dist[:close_range_idx] * both_visible_indices[:close_range_idx]) / np.sum(\n",
        "                        both_visible_indices[:close_range_idx])\n",
        "                    z_dist_mat_close[i, j] = np.sum(\n",
        "                        z_dist[:close_range_idx] * both_visible_indices[:close_range_idx]) / np.sum(\n",
        "                        both_visible_indices[:close_range_idx])\n",
        "                else:\n",
        "                    x_dist_mat_close[i, j] = self.dist_th\n",
        "                    z_dist_mat_close[i, j] = self.dist_th\n",
        "\n",
        "                if np.sum(both_visible_indices[close_range_idx:]) > 0:\n",
        "                    x_dist_mat_far[i, j] = np.sum(\n",
        "                        x_dist[close_range_idx:] * both_visible_indices[close_range_idx:]) / np.sum(\n",
        "                        both_visible_indices[close_range_idx:])\n",
        "                    z_dist_mat_far[i, j] = np.sum(\n",
        "                        z_dist[close_range_idx:] * both_visible_indices[close_range_idx:]) / np.sum(\n",
        "                        both_visible_indices[close_range_idx:])\n",
        "                else:\n",
        "                    x_dist_mat_far[i, j] = self.dist_th\n",
        "                    z_dist_mat_far[i, j] = self.dist_th\n",
        "\n",
        "        # solve bipartite matching vis min cost flow solver\n",
        "        match_results = SolveMinCostFlow(adj_mat, cost_mat)\n",
        "        match_results = np.array(match_results)\n",
        "\n",
        "        # only a match with avg cost < self.dist_th is consider valid one\n",
        "        match_gt_ids = []\n",
        "        match_pred_ids = []\n",
        "        if match_results.shape[0] > 0:\n",
        "            for i in range(len(match_results)):\n",
        "                if match_results[i, 2] < self.dist_th * self.y_samples.shape[0]:\n",
        "                    gt_i = match_results[i, 0]\n",
        "                    pred_i = match_results[i, 1]\n",
        "                    # consider match when the matched points is above a ratio\n",
        "                    if num_match_mat[gt_i, pred_i] / np.sum(gt_visibility_mat[gt_i, :]) >= self.ratio_th:\n",
        "                        r_lane += 1\n",
        "                        match_gt_ids.append(gt_i)\n",
        "                    if num_match_mat[gt_i, pred_i] / np.sum(pred_visibility_mat[pred_i, :]) >= self.ratio_th:\n",
        "                        p_lane += 1\n",
        "                        match_pred_ids.append(pred_i)\n",
        "                    x_error_close.append(x_dist_mat_close[gt_i, pred_i])\n",
        "                    x_error_far.append(x_dist_mat_far[gt_i, pred_i])\n",
        "                    z_error_close.append(z_dist_mat_close[gt_i, pred_i])\n",
        "                    z_error_far.append(z_dist_mat_far[gt_i, pred_i])\n",
        "\n",
        "        # visualize lanelines and matching results both in image and 3D\n",
        "        if vis:\n",
        "            P_g2im = projection_g2im(gt_cam_pitch, gt_cam_height, self.K)\n",
        "            P_gt = np.matmul(self.H_crop, P_g2im)\n",
        "            img = cv2.imread(ops.join(self.dataset_dir, raw_file))\n",
        "            img = cv2.warpPerspective(img, self.H_crop, (self.resize_w, self.resize_h))\n",
        "            img = img.astype(np.float) / 255\n",
        "\n",
        "            for i in range(cnt_gt):\n",
        "                x_values = gt_lanes[i][:, 0]\n",
        "                z_values = gt_lanes[i][:, 1]\n",
        "                x_2d, y_2d = projective_transformation(P_gt, x_values, self.y_samples, z_values)\n",
        "                x_2d = x_2d.astype(np.int)\n",
        "                y_2d = y_2d.astype(np.int)\n",
        "\n",
        "                if i in match_gt_ids:\n",
        "                    color = [0, 0, 1]\n",
        "                else:\n",
        "                    color = [0, 1, 1]\n",
        "                for k in range(1, x_2d.shape[0]):\n",
        "                    # only draw the visible portion\n",
        "                    if gt_visibility_mat[i, k - 1] and gt_visibility_mat[i, k]:\n",
        "                        img = cv2.line(img, (x_2d[k - 1], y_2d[k - 1]), (x_2d[k], y_2d[k]), color[-1::-1], 3)\n",
        "                ax2.plot(x_values[np.where(gt_visibility_mat[i, :])],\n",
        "                         self.y_samples[np.where(gt_visibility_mat[i, :])],\n",
        "                         z_values[np.where(gt_visibility_mat[i, :])], color=color, linewidth=5)\n",
        "\n",
        "            for i in range(cnt_pred):\n",
        "                x_values = pred_lanes[i][:, 0]\n",
        "                z_values = pred_lanes[i][:, 1]\n",
        "                x_2d, y_2d = projective_transformation(P_gt, x_values, self.y_samples, z_values)\n",
        "                x_2d = x_2d.astype(np.int)\n",
        "                y_2d = y_2d.astype(np.int)\n",
        "\n",
        "                if i in match_pred_ids:\n",
        "                    color = [1, 0, 0]\n",
        "                else:\n",
        "                    color = [1, 0, 1]\n",
        "                for k in range(1, x_2d.shape[0]):\n",
        "                    # only draw the visible portion\n",
        "                    if pred_visibility_mat[i, k - 1] and pred_visibility_mat[i, k]:\n",
        "                        img = cv2.line(img, (x_2d[k - 1], y_2d[k - 1]), (x_2d[k], y_2d[k]), color[-1::-1], 2)\n",
        "                ax2.plot(x_values[np.where(pred_visibility_mat[i, :])],\n",
        "                         self.y_samples[np.where(pred_visibility_mat[i, :])],\n",
        "                         z_values[np.where(pred_visibility_mat[i, :])], color=color, linewidth=5)\n",
        "\n",
        "            cv2.putText(img, 'Recall: {:.3f}'.format(r_lane / (cnt_gt + 1e-6)),\n",
        "                        (5, 30), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.7, color=(0, 0, 1), thickness=2)\n",
        "            cv2.putText(img, 'Precision: {:.3f}'.format(p_lane / (cnt_pred + 1e-6)),\n",
        "                        (5, 60), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.7, color=(0, 0, 1), thickness=2)\n",
        "            ax1.imshow(img[:, :, [2, 1, 0]])\n",
        "\n",
        "        return r_lane, p_lane, cnt_gt, cnt_pred, x_error_close, x_error_far, z_error_close, z_error_far\n",
        "\n",
        "    # compare predicted set and ground-truth set using a fixed lane probability threshold\n",
        "    def bench_one_submit(self, pred_file, gt_file, prob_th=0.5, vis=False):\n",
        "        if vis:\n",
        "            save_path = pred_file[:pred_file.rfind('/')]\n",
        "            save_path += '/vis'\n",
        "            if vis and not os.path.exists(save_path):\n",
        "                try:\n",
        "                    os.makedirs(save_path)\n",
        "                except OSError as e:\n",
        "                    print(e.message)\n",
        "        # try:\n",
        "        pred_lines = open(pred_file).readlines()\n",
        "        json_pred = [json.loads(line) for line in pred_lines]\n",
        "        # except BaseException as e:\n",
        "        #     raise Exception('Fail to load json file of the prediction.')\n",
        "        json_gt = [json.loads(line) for line in open(gt_file).readlines()]\n",
        "        if len(json_gt) != len(json_pred):\n",
        "            raise Exception('We do not get the predictions of all the test tasks')\n",
        "        gts = {l['raw_file']: l for l in json_gt}\n",
        "\n",
        "        laneline_stats = []\n",
        "        laneline_x_error_close = []\n",
        "        laneline_x_error_far = []\n",
        "        laneline_z_error_close = []\n",
        "        laneline_z_error_far = []\n",
        "        centerline_stats = []\n",
        "        centerline_x_error_close = []\n",
        "        centerline_x_error_far = []\n",
        "        centerline_z_error_close = []\n",
        "        centerline_z_error_far = []\n",
        "        for i, pred in enumerate(json_pred):\n",
        "            if 'raw_file' not in pred or 'laneLines' not in pred:\n",
        "                raise Exception('raw_file or lanelines not in some predictions.')\n",
        "            raw_file = pred['raw_file']\n",
        "\n",
        "            # if raw_file != 'images/05/0000347.jpg':\n",
        "            #     continue\n",
        "            pred_lanelines = pred['laneLines']\n",
        "            pred_laneLines_prob = pred['laneLines_prob']\n",
        "            pred_lanelines = [pred_lanelines[ii] for ii in range(len(pred_laneLines_prob)) if\n",
        "                              pred_laneLines_prob[ii] > prob_th]\n",
        "\n",
        "            if raw_file not in gts:\n",
        "                raise Exception('Some raw_file from your predictions do not exist in the test tasks.')\n",
        "            gt = gts[raw_file]\n",
        "            gt_cam_height = gt['cam_height']\n",
        "            gt_cam_pitch = gt['cam_pitch']\n",
        "\n",
        "            if vis:\n",
        "                fig = plt.figure()\n",
        "                ax1 = fig.add_subplot(221)\n",
        "                ax2 = fig.add_subplot(222, projection='3d')\n",
        "                ax3 = fig.add_subplot(223)\n",
        "                ax4 = fig.add_subplot(224, projection='3d')\n",
        "            else:\n",
        "                ax1 = 0\n",
        "                ax2 = 0\n",
        "                ax3 = 0\n",
        "                ax4 = 0\n",
        "\n",
        "            # evaluate lanelines\n",
        "            gt_lanelines = gt['laneLines']\n",
        "            gt_visibility = gt['laneLines_visibility']\n",
        "            # N to N matching of lanelines\n",
        "            r_lane, p_lane, cnt_gt, cnt_pred, \\\n",
        "            x_error_close, x_error_far, \\\n",
        "            z_error_close, z_error_far = self.bench(pred_lanelines,\n",
        "                                                    gt_lanelines,\n",
        "                                                    gt_visibility,\n",
        "                                                    raw_file,\n",
        "                                                    gt_cam_height,\n",
        "                                                    gt_cam_pitch,\n",
        "                                                    vis, ax1, ax2)\n",
        "            laneline_stats.append(np.array([r_lane, p_lane, cnt_gt, cnt_pred]))\n",
        "            # consider x_error z_error only for the matched lanes\n",
        "            # if r_lane > 0 and p_lane > 0:\n",
        "            laneline_x_error_close.extend(x_error_close)\n",
        "            laneline_x_error_far.extend(x_error_far)\n",
        "            laneline_z_error_close.extend(z_error_close)\n",
        "            laneline_z_error_far.extend(z_error_far)\n",
        "\n",
        "            # evaluate centerlines\n",
        "            if not self.no_centerline:\n",
        "                pred_centerlines = pred['centerLines']\n",
        "                pred_centerlines_prob = pred['centerLines_prob']\n",
        "                pred_centerlines = [pred_centerlines[ii] for ii in range(len(pred_centerlines_prob)) if\n",
        "                                    pred_centerlines_prob[ii] > prob_th]\n",
        "\n",
        "                gt_centerlines = gt['centerLines']\n",
        "                gt_visibility = gt['centerLines_visibility']\n",
        "\n",
        "                # N to N matching of lanelines\n",
        "                r_lane, p_lane, cnt_gt, cnt_pred, \\\n",
        "                x_error_close, x_error_far, \\\n",
        "                z_error_close, z_error_far = self.bench(pred_centerlines,\n",
        "                                                        gt_centerlines,\n",
        "                                                        gt_visibility,\n",
        "                                                        raw_file,\n",
        "                                                        gt_cam_height,\n",
        "                                                        gt_cam_pitch,\n",
        "                                                        vis, ax3, ax4)\n",
        "                centerline_stats.append(np.array([r_lane, p_lane, cnt_gt, cnt_pred]))\n",
        "                # consider x_error z_error only for the matched lanes\n",
        "                # if r_lane > 0 and p_lane > 0:\n",
        "                centerline_x_error_close.extend(x_error_close)\n",
        "                centerline_x_error_far.extend(x_error_far)\n",
        "                centerline_z_error_close.extend(z_error_close)\n",
        "                centerline_z_error_far.extend(z_error_far)\n",
        "\n",
        "            if vis:\n",
        "                ax1.set_xticks([])\n",
        "                ax1.set_yticks([])\n",
        "                # ax2.set_xlabel('x axis')\n",
        "                # ax2.set_ylabel('y axis')\n",
        "                # ax2.set_zlabel('z axis')\n",
        "                bottom, top = ax2.get_zlim()\n",
        "                left, right = ax2.get_xlim()\n",
        "                ax2.set_zlim(min(bottom, -0.1), max(top, 0.1))\n",
        "                ax2.set_xlim(left, right)\n",
        "                ax2.set_ylim(vis_min_y, vis_max_y)\n",
        "                ax2.locator_params(nbins=5, axis='x')\n",
        "                ax2.locator_params(nbins=5, axis='z')\n",
        "                ax2.tick_params(pad=18)\n",
        "\n",
        "                ax3.set_xticks([])\n",
        "                ax3.set_yticks([])\n",
        "                # ax4.set_xlabel('x axis')\n",
        "                # ax4.set_ylabel('y axis')\n",
        "                # ax4.set_zlabel('z axis')\n",
        "                bottom, top = ax4.get_zlim()\n",
        "                left, right = ax4.get_xlim()\n",
        "                ax4.set_zlim(min(bottom, -0.1), max(top, 0.1))\n",
        "                ax4.set_xlim(left, right)\n",
        "                ax4.set_ylim(vis_min_y, vis_max_y)\n",
        "                ax4.locator_params(nbins=5, axis='x')\n",
        "                ax4.locator_params(nbins=5, axis='z')\n",
        "                ax4.tick_params(pad=18)\n",
        "\n",
        "                fig.subplots_adjust(wspace=0, hspace=0.01)\n",
        "                fig.savefig(ops.join(save_path, raw_file.replace(\"/\", \"_\")))\n",
        "                plt.close(fig)\n",
        "                print('processed sample: {}  {}'.format(i, raw_file))\n",
        "\n",
        "        output_stats = []\n",
        "        laneline_stats = np.array(laneline_stats)\n",
        "        laneline_x_error_close = np.array(laneline_x_error_close)\n",
        "        laneline_x_error_far = np.array(laneline_x_error_far)\n",
        "        laneline_z_error_close = np.array(laneline_z_error_close)\n",
        "        laneline_z_error_far = np.array(laneline_z_error_far)\n",
        "\n",
        "        R_lane = np.sum(laneline_stats[:, 0]) / (np.sum(laneline_stats[:, 2]) + 1e-6)\n",
        "        P_lane = np.sum(laneline_stats[:, 1]) / (np.sum(laneline_stats[:, 3]) + 1e-6)\n",
        "        F_lane = 2 * R_lane * P_lane / (R_lane + P_lane + 1e-6)\n",
        "        x_error_close_avg = np.average(laneline_x_error_close)\n",
        "        x_error_far_avg = np.average(laneline_x_error_far)\n",
        "        z_error_close_avg = np.average(laneline_z_error_close)\n",
        "        z_error_far_avg = np.average(laneline_z_error_far)\n",
        "\n",
        "        output_stats.append(F_lane)\n",
        "        output_stats.append(R_lane)\n",
        "        output_stats.append(P_lane)\n",
        "        output_stats.append(x_error_close_avg)\n",
        "        output_stats.append(x_error_far_avg)\n",
        "        output_stats.append(z_error_close_avg)\n",
        "        output_stats.append(z_error_far_avg)\n",
        "\n",
        "        if not self.no_centerline:\n",
        "            centerline_stats = np.array(centerline_stats)\n",
        "            centerline_x_error_close = np.array(centerline_x_error_close)\n",
        "            centerline_x_error_far = np.array(centerline_x_error_far)\n",
        "            centerline_z_error_close = np.array(centerline_z_error_close)\n",
        "            centerline_z_error_far = np.array(centerline_z_error_far)\n",
        "\n",
        "            R_lane = np.sum(centerline_stats[:, 0]) / (np.sum(centerline_stats[:, 2]) + 1e-6)\n",
        "            P_lane = np.sum(centerline_stats[:, 1]) / (np.sum(centerline_stats[:, 3]) + 1e-6)\n",
        "            F_lane = 2 * R_lane * P_lane / (R_lane + P_lane + 1e-6)\n",
        "            x_error_close_avg = np.average(centerline_x_error_close)\n",
        "            x_error_far_avg = np.average(centerline_x_error_far)\n",
        "            z_error_close_avg = np.average(centerline_z_error_close)\n",
        "            z_error_far_avg = np.average(centerline_z_error_far)\n",
        "\n",
        "            output_stats.append(F_lane)\n",
        "            output_stats.append(R_lane)\n",
        "            output_stats.append(P_lane)\n",
        "            output_stats.append(x_error_close_avg)\n",
        "            output_stats.append(x_error_far_avg)\n",
        "            output_stats.append(z_error_close_avg)\n",
        "            output_stats.append(z_error_far_avg)\n",
        "\n",
        "        return output_stats\n",
        "\n",
        "    def bench_PR(self, pred_lanes, gt_lanes, gt_visibility):\n",
        "        \"\"\"\n",
        "            Matching predicted lanes and ground-truth lanes in their IPM projection, ignoring z attributes.\n",
        "            x error, y_error, and z error are all considered, although the matching does not rely on z\n",
        "            The input of prediction and ground-truth lanes are in ground coordinate, x-right, y-forward, z-up\n",
        "            The fundamental assumption is: 1. there are no two points from different lanes with identical x, y\n",
        "                                              but different z's\n",
        "                                           2. there are no two points from a single lane having identical x, y\n",
        "                                              but different z's\n",
        "            If the interest area is within the current drivable road, the above assumptions are almost always valid.\n",
        "        :param pred_lanes: N X 2 or N X 3 lists depending on 2D or 3D\n",
        "        :param gt_lanes: N X 2 or N X 3 lists depending on 2D or 3D\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        r_lane, p_lane = 0., 0.\n",
        "\n",
        "        # only keep the visible portion\n",
        "        gt_lanes = [prune_3d_lane_by_visibility(np.array(gt_lane), np.array(gt_visibility[k])) for k, gt_lane in\n",
        "                    enumerate(gt_lanes)]\n",
        "        gt_lanes = [lane for lane in gt_lanes if lane.shape[0] > 1]\n",
        "        # only consider those gt lanes overlapping with sampling range\n",
        "        gt_lanes = [lane for lane in gt_lanes if lane[0, 1] < self.y_samples[-1] and lane[-1, 1] > self.y_samples[0]]\n",
        "        gt_lanes = [prune_3d_lane_by_range(np.array(gt_lane), 3 * self.x_min, 3 * self.x_max) for gt_lane in gt_lanes]\n",
        "        gt_lanes = [lane for lane in gt_lanes if lane.shape[0] > 1]\n",
        "        cnt_gt = len(gt_lanes)\n",
        "        cnt_pred = len(pred_lanes)\n",
        "\n",
        "        gt_visibility_mat = np.zeros((cnt_gt, 100))\n",
        "        pred_visibility_mat = np.zeros((cnt_pred, 100))\n",
        "        # resample gt and pred at y_samples\n",
        "        for i in range(cnt_gt):\n",
        "            min_y = np.min(np.array(gt_lanes[i])[:, 1])\n",
        "            max_y = np.max(np.array(gt_lanes[i])[:, 1])\n",
        "            x_values, z_values, visibility_vec = resample_laneline_in_y(np.array(gt_lanes[i]), self.y_samples,\n",
        "                                                                        out_vis=True)\n",
        "            gt_lanes[i] = np.vstack([x_values, z_values]).T\n",
        "            gt_visibility_mat[i, :] = np.logical_and(x_values >= self.x_min,\n",
        "                                                     np.logical_and(x_values <= self.x_max,\n",
        "                                                                    np.logical_and(self.y_samples >= min_y,\n",
        "                                                                                   self.y_samples <= max_y)))\n",
        "            gt_visibility_mat[i, :] = np.logical_and(gt_visibility_mat[i, :], visibility_vec)\n",
        "\n",
        "        for i in range(cnt_pred):\n",
        "            # # ATTENTION: ensure y mono increase before interpolation: but it can reduce size\n",
        "            # pred_lanes[i] = make_lane_y_mono_inc(np.array(pred_lanes[i]))\n",
        "            # pred_lane = prune_3d_lane_by_range(np.array(pred_lanes[i]), self.x_min, self.x_max)\n",
        "            min_y = np.min(np.array(pred_lanes[i])[:, 1])\n",
        "            max_y = np.max(np.array(pred_lanes[i])[:, 1])\n",
        "            x_values, z_values, visibility_vec = resample_laneline_in_y(np.array(pred_lanes[i]), self.y_samples,\n",
        "                                                                        out_vis=True)\n",
        "            pred_lanes[i] = np.vstack([x_values, z_values]).T\n",
        "            pred_visibility_mat[i, :] = np.logical_and(x_values >= self.x_min,\n",
        "                                                       np.logical_and(x_values <= self.x_max,\n",
        "                                                                      np.logical_and(self.y_samples >= min_y,\n",
        "                                                                                     self.y_samples <= max_y)))\n",
        "            pred_visibility_mat[i, :] = np.logical_and(pred_visibility_mat[i, :], visibility_vec)\n",
        "            # pred_visibility_mat[i, :] = np.logical_and(x_values >= self.x_min, x_values <= self.x_max)\n",
        "\n",
        "        adj_mat = np.zeros((cnt_gt, cnt_pred), dtype=np.int)\n",
        "        cost_mat = np.zeros((cnt_gt, cnt_pred), dtype=np.int)\n",
        "        cost_mat.fill(1000)\n",
        "        num_match_mat = np.zeros((cnt_gt, cnt_pred), dtype=np.float)\n",
        "        # compute curve to curve distance\n",
        "        for i in range(cnt_gt):\n",
        "            for j in range(cnt_pred):\n",
        "                x_dist = np.abs(gt_lanes[i][:, 0] - pred_lanes[j][:, 0])\n",
        "                z_dist = np.abs(gt_lanes[i][:, 1] - pred_lanes[j][:, 1])\n",
        "                euclidean_dist = np.sqrt(x_dist ** 2 + z_dist ** 2)\n",
        "\n",
        "                # apply visibility to penalize different partial matching accordingly\n",
        "                euclidean_dist[\n",
        "                    np.logical_or(gt_visibility_mat[i, :] < 0.5, pred_visibility_mat[j, :] < 0.5)] = self.dist_th\n",
        "\n",
        "                # if np.average(euclidean_dist) < 2*self.dist_th: # don't prune here to encourage finding perfect match\n",
        "                num_match_mat[i, j] = np.sum(euclidean_dist < self.dist_th)\n",
        "                adj_mat[i, j] = 1\n",
        "                # ATTENTION: use the sum as int type to meet the requirements of min cost flow optimization (int type)\n",
        "                # why using num_match_mat as cost does not work?\n",
        "                cost_mat[i, j] = np.sum(euclidean_dist).astype(np.int)\n",
        "                # cost_mat[i, j] = num_match_mat[i, j]\n",
        "\n",
        "        # solve bipartite matching vis min cost flow solver\n",
        "        match_results = SolveMinCostFlow(adj_mat, cost_mat)\n",
        "        match_results = np.array(match_results)\n",
        "\n",
        "        # only a match with avg cost < self.dist_th is consider valid one\n",
        "        match_gt_ids = []\n",
        "        match_pred_ids = []\n",
        "        if match_results.shape[0] > 0:\n",
        "            for i in range(len(match_results)):\n",
        "                if match_results[i, 2] < self.dist_th * self.y_samples.shape[0]:\n",
        "                    gt_i = match_results[i, 0]\n",
        "                    pred_i = match_results[i, 1]\n",
        "                    # consider match when the matched points is above a ratio\n",
        "                    if num_match_mat[gt_i, pred_i] / np.sum(gt_visibility_mat[gt_i, :]) >= self.ratio_th:\n",
        "                        r_lane += 1\n",
        "                        match_gt_ids.append(gt_i)\n",
        "                    if num_match_mat[gt_i, pred_i] / np.sum(pred_visibility_mat[pred_i, :]) >= self.ratio_th:\n",
        "                        p_lane += 1\n",
        "                        match_pred_ids.append(pred_i)\n",
        "\n",
        "        return r_lane, p_lane, cnt_gt, cnt_pred\n",
        "\n",
        "    # evaluate two dataset at varying lane probability threshold to calculate AP\n",
        "    def bench_one_submit_varying_probs(self, pred_file, gt_file, eval_out_file=None, eval_fig_file=None):\n",
        "        varying_th = np.linspace(0.05, 0.95, 19)\n",
        "        # try:\n",
        "        pred_lines = open(pred_file).readlines()\n",
        "        json_pred = [json.loads(line) for line in pred_lines]\n",
        "        # except BaseException as e:\n",
        "        #     raise Exception('Fail to load json file of the prediction.')\n",
        "        json_gt = [json.loads(line) for line in open(gt_file).readlines()]\n",
        "        if len(json_gt) != len(json_pred):\n",
        "            raise Exception('We do not get the predictions of all the test tasks')\n",
        "        gts = {l['raw_file']: l for l in json_gt}\n",
        "\n",
        "        laneline_r_all = []\n",
        "        laneline_p_all = []\n",
        "        laneline_gt_cnt_all = []\n",
        "        laneline_pred_cnt_all = []\n",
        "        centerline_r_all = []\n",
        "        centerline_p_all = []\n",
        "        centerline_gt_cnt_all = []\n",
        "        centerline_pred_cnt_all = []\n",
        "        for i, pred in enumerate(json_pred):\n",
        "            print('Evaluating sample {} / {}'.format(i, len(json_pred)))\n",
        "            if 'raw_file' not in pred or 'laneLines' not in pred:\n",
        "                raise Exception('raw_file or lanelines not in some predictions.')\n",
        "            raw_file = pred['raw_file']\n",
        "\n",
        "            pred_lanelines = pred['laneLines']\n",
        "            pred_laneLines_prob = pred['laneLines_prob']\n",
        "            if raw_file not in gts:\n",
        "                raise Exception('Some raw_file from your predictions do not exist in the test tasks.')\n",
        "            gt = gts[raw_file]\n",
        "            gt_cam_height = gt['cam_height']\n",
        "            gt_cam_pitch = gt['cam_pitch']\n",
        "\n",
        "            # evaluate lanelines\n",
        "            gt_lanelines = gt['laneLines']\n",
        "            gt_visibility = gt['laneLines_visibility']\n",
        "            r_lane_vec = []\n",
        "            p_lane_vec = []\n",
        "            cnt_gt_vec = []\n",
        "            cnt_pred_vec = []\n",
        "\n",
        "            for prob_th in varying_th:\n",
        "                pred_lanelines = [pred_lanelines[ii] for ii in range(len(pred_laneLines_prob)) if\n",
        "                                  pred_laneLines_prob[ii] > prob_th]\n",
        "                pred_laneLines_prob = [prob for prob in pred_laneLines_prob if prob > prob_th]\n",
        "                pred_lanelines_copy = copy.deepcopy(pred_lanelines)\n",
        "                # N to N matching of lanelines\n",
        "                r_lane, p_lane, cnt_gt, cnt_pred = self.bench_PR(pred_lanelines_copy,\n",
        "                                                                 gt_lanelines,\n",
        "                                                                 gt_visibility)\n",
        "                r_lane_vec.append(r_lane)\n",
        "                p_lane_vec.append(p_lane)\n",
        "                cnt_gt_vec.append(cnt_gt)\n",
        "                cnt_pred_vec.append(cnt_pred)\n",
        "\n",
        "            laneline_r_all.append(r_lane_vec)\n",
        "            laneline_p_all.append(p_lane_vec)\n",
        "            laneline_gt_cnt_all.append(cnt_gt_vec)\n",
        "            laneline_pred_cnt_all.append(cnt_pred_vec)\n",
        "\n",
        "            # evaluate centerlines\n",
        "            if not self.no_centerline:\n",
        "                pred_centerlines = pred['centerLines']\n",
        "                pred_centerLines_prob = pred['centerLines_prob']\n",
        "                gt_centerlines = gt['centerLines']\n",
        "                gt_visibility = gt['centerLines_visibility']\n",
        "                r_lane_vec = []\n",
        "                p_lane_vec = []\n",
        "                cnt_gt_vec = []\n",
        "                cnt_pred_vec = []\n",
        "\n",
        "                for prob_th in varying_th:\n",
        "                    pred_centerlines = [pred_centerlines[ii] for ii in range(len(pred_centerLines_prob)) if\n",
        "                                        pred_centerLines_prob[ii] > prob_th]\n",
        "                    pred_centerLines_prob = [prob for prob in pred_centerLines_prob if prob > prob_th]\n",
        "                    pred_centerlines_copy = copy.deepcopy(pred_centerlines)\n",
        "                    # N to N matching of lanelines\n",
        "                    r_lane, p_lane, cnt_gt, cnt_pred = self.bench_PR(pred_centerlines_copy,\n",
        "                                                                     gt_centerlines,\n",
        "                                                                     gt_visibility)\n",
        "                    r_lane_vec.append(r_lane)\n",
        "                    p_lane_vec.append(p_lane)\n",
        "                    cnt_gt_vec.append(cnt_gt)\n",
        "                    cnt_pred_vec.append(cnt_pred)\n",
        "                centerline_r_all.append(r_lane_vec)\n",
        "                centerline_p_all.append(p_lane_vec)\n",
        "                centerline_gt_cnt_all.append(cnt_gt_vec)\n",
        "                centerline_pred_cnt_all.append(cnt_pred_vec)\n",
        "\n",
        "        output_stats = []\n",
        "        # compute precision, recall\n",
        "        laneline_r_all = np.array(laneline_r_all)\n",
        "        laneline_p_all = np.array(laneline_p_all)\n",
        "        laneline_gt_cnt_all = np.array(laneline_gt_cnt_all)\n",
        "        laneline_pred_cnt_all = np.array(laneline_pred_cnt_all)\n",
        "\n",
        "        R_lane = np.sum(laneline_r_all, axis=0) / (np.sum(laneline_gt_cnt_all, axis=0) + 1e-6)\n",
        "        P_lane = np.sum(laneline_p_all, axis=0) / (np.sum(laneline_pred_cnt_all, axis=0) + 1e-6)\n",
        "        F_lane = 2 * R_lane * P_lane / (R_lane + P_lane + 1e-6)\n",
        "\n",
        "        output_stats.append(F_lane)\n",
        "        output_stats.append(R_lane)\n",
        "        output_stats.append(P_lane)\n",
        "\n",
        "        if not self.no_centerline:\n",
        "            centerline_r_all = np.array(centerline_r_all)\n",
        "            centerline_p_all = np.array(centerline_p_all)\n",
        "            centerline_gt_cnt_all = np.array(centerline_gt_cnt_all)\n",
        "            centerline_pred_cnt_all = np.array(centerline_pred_cnt_all)\n",
        "\n",
        "            R_lane = np.sum(centerline_r_all, axis=0) / (np.sum(centerline_gt_cnt_all, axis=0) + 1e-6)\n",
        "            P_lane = np.sum(centerline_p_all, axis=0) / (np.sum(centerline_pred_cnt_all, axis=0) + 1e-6)\n",
        "            F_lane = 2 * R_lane * P_lane / (R_lane + P_lane + 1e-6)\n",
        "\n",
        "            output_stats.append(F_lane)\n",
        "            output_stats.append(R_lane)\n",
        "            output_stats.append(P_lane)\n",
        "\n",
        "        # calculate metrics\n",
        "        laneline_F = output_stats[0]\n",
        "        laneline_F_max = np.max(laneline_F)\n",
        "        laneline_max_i = np.argmax(laneline_F)\n",
        "        laneline_R = output_stats[1]\n",
        "        laneline_P = output_stats[2]\n",
        "        centerline_F = output_stats[3]\n",
        "        centerline_F_max = centerline_F[laneline_max_i]\n",
        "        centerline_max_i = laneline_max_i\n",
        "        centerline_R = output_stats[4]\n",
        "        centerline_P = output_stats[5]\n",
        "\n",
        "        laneline_R = np.array([1.] + laneline_R.tolist() + [0.])\n",
        "        laneline_P = np.array([0.] + laneline_P.tolist() + [1.])\n",
        "        centerline_R = np.array([1.] + centerline_R.tolist() + [0.])\n",
        "        centerline_P = np.array([0.] + centerline_P.tolist() + [1.])\n",
        "        f_laneline = interp1d(laneline_R, laneline_P)\n",
        "        f_centerline = interp1d(centerline_R, centerline_P)\n",
        "        r_range = np.linspace(0.05, 0.95, 19)\n",
        "        laneline_AP = np.mean(f_laneline(r_range))\n",
        "        centerline_AP = np.mean(f_centerline(r_range))\n",
        "\n",
        "        if eval_fig_file is not None:\n",
        "            # plot PR curve\n",
        "            fig = plt.figure()\n",
        "            ax1 = fig.add_subplot(121)\n",
        "            ax2 = fig.add_subplot(122)\n",
        "            ax1.plot(laneline_R, laneline_P, '-s')\n",
        "            ax2.plot(centerline_R, centerline_P, '-s')\n",
        "\n",
        "            ax1.set_xlim(0, 1)\n",
        "            ax1.set_ylim(0, 1)\n",
        "            ax1.set_title('Lane Line')\n",
        "            ax1.set_xlabel('Recall')\n",
        "            ax1.set_ylabel('Precision')\n",
        "            ax1.set_aspect('equal')\n",
        "            ax1.legend('Max F-measure {:.3}'.format(laneline_F_max))\n",
        "\n",
        "            ax2.set_xlim(0, 1)\n",
        "            ax2.set_ylim(0, 1)\n",
        "            ax2.set_title('Center Line')\n",
        "            ax2.set_xlabel('Recall')\n",
        "            ax2.set_ylabel('Precision')\n",
        "            ax2.set_aspect('equal')\n",
        "            ax2.legend('Max F-measure {:.3}'.format(centerline_F_max))\n",
        "\n",
        "            # fig.subplots_adjust(wspace=0.1, hspace=0.01)\n",
        "            fig.savefig(eval_fig_file)\n",
        "            plt.close(fig)\n",
        "\n",
        "        # print(\"===> Evaluation on validation set: \\n\"\n",
        "        #       \"laneline max F-measure {:.3} at Recall {:.3}, Precision {:.3} \\n\"\n",
        "        #       \"laneline AP: {:.3}\\n\"\n",
        "        #       \"centerline max F-measure {:.3} at Recall {:.3}, Precision {:.3} \\n\"\n",
        "        #       \"centerline AP: {:.3} \\n\".format(laneline_F_max,\n",
        "        #                                        laneline_R[laneline_max_i + 1],\n",
        "        #                                        laneline_P[laneline_max_i + 1],\n",
        "        #                                        laneline_AP,\n",
        "        #                                        centerline_F_max,\n",
        "        #                                        centerline_R[centerline_max_i + 1],\n",
        "        #                                        centerline_P[centerline_max_i + 1],\n",
        "        #                                        centerline_AP))\n",
        "\n",
        "        json_out = {}\n",
        "        json_out['laneline_R'] = laneline_R[1:-1].astype(np.float32).tolist()\n",
        "        json_out['laneline_P'] = laneline_P[1:-1].astype(np.float32).tolist()\n",
        "        json_out['laneline_F_max'] = laneline_F_max\n",
        "        json_out['laneline_max_i'] = laneline_max_i.tolist()\n",
        "        json_out['laneline_AP'] = laneline_AP\n",
        "\n",
        "        json_out['centerline_R'] = centerline_R[1:-1].astype(np.float32).tolist()\n",
        "        json_out['centerline_P'] = centerline_P[1:-1].astype(np.float32).tolist()\n",
        "        json_out['centerline_F_max'] = centerline_F_max\n",
        "        json_out['centerline_max_i'] = centerline_max_i.tolist()\n",
        "        json_out['centerline_AP'] = centerline_AP\n",
        "\n",
        "        json_out['max_F_prob_th'] = varying_th[laneline_max_i]\n",
        "\n",
        "        if eval_out_file is not None:\n",
        "            with open(eval_out_file, 'w') as jsonFile:\n",
        "                jsonFile.write(json.dumps(json_out))\n",
        "                jsonFile.write('\\n')\n",
        "                jsonFile.close()\n",
        "        return json_out\n",
        "\n",
        "'''\n",
        "if __name__ == '__main__':\n",
        "    import sys\n",
        "    print(\"start\")\n",
        "    vis = True\n",
        "    args = define_args()\n",
        "    #args = parser.parse_args()\n",
        "\n",
        "    # two method are compared: '3D_LaneNet' and 'Gen_LaneNet'\n",
        "    method_name = 'Gen_LaneNet_ext'\n",
        "\n",
        "    # Three different splits of datasets: 'standard', 'rare_subsit', 'illus_chg'\n",
        "    data_split = 'illus_chg'\n",
        "\n",
        "    # location where the original dataset is saved. Image will be loaded in case of visualization\n",
        "    args.dataset_dir = '/content/drive/Shareddrives/colab/Apollo_Sim_3D_Lane_Release'\n",
        "\n",
        "    # load configuration for certain dataset\n",
        "    sim3d_config(args)\n",
        "\n",
        "    # auto-file in dependent paths\n",
        "    gt_file = '/content/drive/Shareddrives/colab/data_splits/' + data_split + '/test.json'\n",
        "    pred_folder = '/content/drive/Shareddrives/colab/data_splits/' + data_split + '/' + method_name\n",
        "    pred_file = pred_folder + '/test_pred_file.json'\n",
        "\n",
        "    # Initialize evaluator\n",
        "    evaluator = LaneEval(args)\n",
        "    print(\"here\")\n",
        "    # evaluation at varying thresholds\n",
        "    eval_stats_pr = evaluator.bench_one_submit_varying_probs(pred_file, gt_file)\n",
        "    max_f_prob = eval_stats_pr['max_F_prob_th']\n",
        "    print(\"done\")\n",
        "    # evaluate at the point with max F-measure. Additional eval of position error. Option to visualize matching result\n",
        "    eval_stats = evaluator.bench_one_submit(pred_file, gt_file, prob_th=max_f_prob, vis=vis)\n",
        "\n",
        "    print(\"Metrics: AP, F-score, x error (close), x error (far), z error (close), z error (far)\")\n",
        "    print(\n",
        "        \"Laneline:  {:.3}  {:.3}  {:.3}  {:.3}  {:.3}  {:.3}\".format(eval_stats_pr['laneline_AP'], eval_stats[0],\n",
        "                                                                     eval_stats[3], eval_stats[4],\n",
        "                                                                     eval_stats[5], eval_stats[6]))\n",
        "    print(\"Centerline:  {:.3}  {:.3}  {:.3}  {:.3}  {:.3}  {:.3}\".format(eval_stats_pr['centerline_AP'], eval_stats[7],\n",
        "                                                                         eval_stats[10], eval_stats[11],\n",
        "                                                                         eval_stats[12], eval_stats[13]))\n",
        "                                                                         '''"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nif __name__ == \\'__main__\\':\\n    import sys\\n    print(\"start\")\\n    vis = True\\n    args = define_args()\\n    #args = parser.parse_args()\\n\\n    # two method are compared: \\'3D_LaneNet\\' and \\'Gen_LaneNet\\'\\n    method_name = \\'Gen_LaneNet_ext\\'\\n\\n    # Three different splits of datasets: \\'standard\\', \\'rare_subsit\\', \\'illus_chg\\'\\n    data_split = \\'illus_chg\\'\\n\\n    # location where the original dataset is saved. Image will be loaded in case of visualization\\n    args.dataset_dir = \\'/content/drive/Shareddrives/colab/Apollo_Sim_3D_Lane_Release\\'\\n\\n    # load configuration for certain dataset\\n    sim3d_config(args)\\n\\n    # auto-file in dependent paths\\n    gt_file = \\'/content/drive/Shareddrives/colab/data_splits/\\' + data_split + \\'/test.json\\'\\n    pred_folder = \\'/content/drive/Shareddrives/colab/data_splits/\\' + data_split + \\'/\\' + method_name\\n    pred_file = pred_folder + \\'/test_pred_file.json\\'\\n\\n    # Initialize evaluator\\n    evaluator = LaneEval(args)\\n    print(\"here\")\\n    # evaluation at varying thresholds\\n    eval_stats_pr = evaluator.bench_one_submit_varying_probs(pred_file, gt_file)\\n    max_f_prob = eval_stats_pr[\\'max_F_prob_th\\']\\n    print(\"done\")\\n    # evaluate at the point with max F-measure. Additional eval of position error. Option to visualize matching result\\n    eval_stats = evaluator.bench_one_submit(pred_file, gt_file, prob_th=max_f_prob, vis=vis)\\n\\n    print(\"Metrics: AP, F-score, x error (close), x error (far), z error (close), z error (far)\")\\n    print(\\n        \"Laneline:  {:.3}  {:.3}  {:.3}  {:.3}  {:.3}  {:.3}\".format(eval_stats_pr[\\'laneline_AP\\'], eval_stats[0],\\n                                                                     eval_stats[3], eval_stats[4],\\n                                                                     eval_stats[5], eval_stats[6]))\\n    print(\"Centerline:  {:.3}  {:.3}  {:.3}  {:.3}  {:.3}  {:.3}\".format(eval_stats_pr[\\'centerline_AP\\'], eval_stats[7],\\n                                                                         eval_stats[10], eval_stats[11],\\n                                                                         eval_stats[12], eval_stats[13]))\\n                                                                         '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "Jx6Phr4U6Anf",
        "outputId": "1057422b-0435-4e9a-c090-6539d37f8e21"
      },
      "source": [
        "\"\"\"\n",
        "Description: Visualization code to draw predicted lane-lines and center-lines in three views: image, virtual top,\n",
        "             3D ego-car. Respectively, lane-lines are shown in the top row, center-lines are drawn in the bottom.\n",
        "Author: Yuliang Guo (33yuliangguo@gmail.com)\n",
        "Date: March, 2020\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import os.path as ops\n",
        "import math\n",
        "import ujson as json\n",
        "import matplotlib\n",
        "#from tools.utils import *\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (35, 30)\n",
        "plt.rcParams.update({'font.size': 25})\n",
        "plt.rcParams.update({'font.weight': 'semibold'})\n",
        "\n",
        "min_y = 0\n",
        "max_y = 80\n",
        "\n",
        "colors = [[1, 0, 0],  # red\n",
        "          [0, 1, 0],  # green\n",
        "          [0, 0, 1],  # blue\n",
        "          [1, 0, 1],  # purple\n",
        "          [0, 1, 1],  # cyan\n",
        "          [1, 0.7, 0]]  # orange\n",
        "\n",
        "\n",
        "class lane_visualizer(object):\n",
        "    def __init__(self, args):\n",
        "        self.dataset_dir = args.dataset_dir\n",
        "        self.K = args.K\n",
        "        self.no_centerline = args.no_centerline\n",
        "        \"\"\"\n",
        "            this visualizer use higher resolution than network input for better look\n",
        "        \"\"\"\n",
        "        self.resize_h = args.org_h\n",
        "        self.resize_w = args.org_w\n",
        "        # self.resize_h = args.resize_h\n",
        "        # self.resize_w = args.resize_w\n",
        "        self.ipm_w = 2*args.ipm_w\n",
        "        self.ipm_h = 2*args.ipm_h\n",
        "        self.H_crop = homography_crop_resize([args.org_h, args.org_w], args.crop_y, [self.resize_h, self.resize_w])\n",
        "        # transformation from ipm to ground region\n",
        "        self.H_ipm2g = cv2.getPerspectiveTransform(np.float32([[0, 0],\n",
        "                                                              [self.ipm_w-1, 0],\n",
        "                                                              [0, self.ipm_h-1],\n",
        "                                                              [self.ipm_w-1, self.ipm_h-1]]),\n",
        "                                                   np.float32(args.top_view_region))\n",
        "        self.H_g2ipm = np.linalg.inv(self.H_ipm2g)\n",
        "\n",
        "        self.x_min = args.top_view_region[0, 0]\n",
        "        self.x_max = args.top_view_region[1, 0]\n",
        "        # self.y_samples = np.linspace(args.anchor_y_steps[0], args.anchor_y_steps[-1], num=100, endpoint=False)\n",
        "        self.y_samples = np.linspace(min_y, max_y, num=100, endpoint=False)\n",
        "\n",
        "    def visualize_lanes(self, pred_lanes, raw_file, gt_cam_height, gt_cam_pitch, ax1, ax2, ax3):\n",
        "        P_g2im = projection_g2im(gt_cam_pitch, gt_cam_height, self.K)\n",
        "        # P_gt = P_g2im\n",
        "        P_gt = np.matmul(self.H_crop, P_g2im)\n",
        "        H_g2im = homograpthy_g2im(gt_cam_pitch, gt_cam_height, self.K)\n",
        "        H_im2ipm = np.linalg.inv(np.matmul(self.H_crop, np.matmul(H_g2im, self.H_ipm2g)))\n",
        "\n",
        "        img = cv2.imread(ops.join(self.dataset_dir, raw_file))\n",
        "        img = cv2.warpPerspective(img, self.H_crop, (self.resize_w, self.resize_h))\n",
        "        img = img.astype(np.float) / 255\n",
        "        im_ipm = cv2.warpPerspective(img, H_im2ipm, (self.ipm_w, self.ipm_h))\n",
        "        im_ipm = np.clip(im_ipm, 0, 1)\n",
        "\n",
        "        cnt_pred = len(pred_lanes)\n",
        "        pred_visibility_mat = np.zeros((cnt_pred, 100))\n",
        "        for i in range(cnt_pred):\n",
        "            # # ATTENTION: ensure y mono increase before interpolation: but it can reduce size\n",
        "            # pred_lanes[i] = make_lane_y_mono_inc(np.array(pred_lanes[i]))\n",
        "            # pred_lane = prune_3d_lane_by_range(np.array(pred_lanes[i]), self.x_min, self.x_max)\n",
        "            min_y = np.min(np.array(pred_lanes[i])[:, 1])\n",
        "            max_y = np.max(np.array(pred_lanes[i])[:, 1])\n",
        "            x_values, z_values, visibility_vec = resample_laneline_in_y(np.array(pred_lanes[i]), self.y_samples, out_vis=True)\n",
        "            pred_lanes[i] = np.vstack([x_values, z_values]).T\n",
        "            pred_visibility_mat[i, :] = np.logical_and(x_values >= self.x_min,\n",
        "                                                       np.logical_and(x_values <= self.x_max,\n",
        "                                                                      np.logical_and(self.y_samples >= min_y,\n",
        "                                                                                     self.y_samples <= max_y)))\n",
        "            pred_visibility_mat[i, :] = np.logical_and(pred_visibility_mat[i, :], visibility_vec)\n",
        "\n",
        "        # draw lanes in multiple color\n",
        "        for i in range(cnt_pred):\n",
        "            x_values = pred_lanes[i][:, 0]\n",
        "            z_values = pred_lanes[i][:, 1]\n",
        "            # if 'gflat' in pred_file or 'ext' in pred_file:\n",
        "            x_ipm_values, y_ipm_values = transform_lane_g2gflat(gt_cam_height, x_values, self.y_samples, z_values)\n",
        "            # remove those points with z_values > gt_cam_height, this is only for visualization on top-view\n",
        "            x_ipm_values = x_ipm_values[np.where(z_values < gt_cam_height)]\n",
        "            y_ipm_values = y_ipm_values[np.where(z_values < gt_cam_height)]\n",
        "            # else:  # mean to visualize original anchor's preparation\n",
        "            #     x_ipm_values = x_values\n",
        "            #     y_ipm_values = self.y_samples\n",
        "            x_ipm_values, y_ipm_values = homographic_transformation(self.H_g2ipm, x_ipm_values, y_ipm_values)\n",
        "            x_ipm_values = x_ipm_values.astype(np.int)\n",
        "            y_ipm_values = y_ipm_values.astype(np.int)\n",
        "            x_2d, y_2d = projective_transformation(P_gt, x_values, self.y_samples, z_values)\n",
        "            x_2d = x_2d.astype(np.int)\n",
        "            y_2d = y_2d.astype(np.int)\n",
        "\n",
        "            color = colors[np.mod(i, len(colors))]\n",
        "            # draw on image\n",
        "            for k in range(1, x_2d.shape[0]):\n",
        "                # only draw the visible portion\n",
        "                if pred_visibility_mat[i, k - 1] and pred_visibility_mat[i, k]:\n",
        "                    img = cv2.line(img, (x_2d[k - 1], y_2d[k - 1]), (x_2d[k], y_2d[k]), color[-1::-1], 10)\n",
        "\n",
        "            # draw on ipm\n",
        "            for k in range(1, x_ipm_values.shape[0]):\n",
        "                # only draw the visible portion\n",
        "                if pred_visibility_mat[i, k - 1] and pred_visibility_mat[i, k]:\n",
        "                    im_ipm = cv2.line(im_ipm, (x_ipm_values[k - 1], y_ipm_values[k - 1]), (x_ipm_values[k], y_ipm_values[k]), color[-1::-1], 3)\n",
        "\n",
        "            # draw in 3d\n",
        "            ax3.plot(x_values[np.where(pred_visibility_mat[i, :])],\n",
        "                     self.y_samples[np.where(pred_visibility_mat[i, :])],\n",
        "                     z_values[np.where(pred_visibility_mat[i, :])], color=color, linewidth=5)\n",
        "        ax1.imshow(img[:, :, [2, 1, 0]])\n",
        "        ax2.imshow(im_ipm[:, :, [2, 1, 0]])\n",
        "\n",
        "'''\n",
        "if __name__ == '__main__':\n",
        "    parser = define_args()\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # dataset_name: 'standard' / 'rare_subset' / 'illus_chg'\n",
        "    args.dataset_name = 'illus_chg'\n",
        "    args.dataset_dir = '/media/yuliangguo/DATA1/Datasets/Apollo_Sim_3D_Lane_Release/'\n",
        "\n",
        "    # model name: 'Gen_LaneNet_ext' / '3D_LaneNet'\n",
        "    model_name = 'Gen_LaneNet_ext'\n",
        "\n",
        "    # load configuration for certain dataset\n",
        "    sim3d_config(args)\n",
        "    args.top_view_region = np.array([[-10, max_y], [10, max_y], [-10, 3], [10, 3]])\n",
        "    vs = lane_visualizer(args)\n",
        "\n",
        "    pred_file = '../data_splits/' + args.dataset_name + '/' + model_name + '/test_pred_file.json'\n",
        "    gt_file = '../data_splits/' + args.dataset_name + '/test.json'\n",
        "\n",
        "    save_path = pred_file[:pred_file.rfind('/')]\n",
        "    save_path += '/example/test_vis_pred'\n",
        "    if not os.path.exists(save_path):\n",
        "        try:\n",
        "            os.makedirs(save_path)\n",
        "        except OSError as e:\n",
        "            print(e.message)\n",
        "\n",
        "    pred_lines = open(pred_file).readlines()\n",
        "    json_pred = [json.loads(line) for line in pred_lines]\n",
        "    # except BaseException as e:\n",
        "    #     raise Exception('Fail to load json file of the prediction.')\n",
        "    json_gt = [json.loads(line) for line in open(gt_file).readlines()]\n",
        "    if len(json_gt) != len(json_pred):\n",
        "        raise Exception('We do not get the predictions of all the test tasks')\n",
        "    gts = {l['raw_file']: l for l in json_gt}\n",
        "\n",
        "    for i, pred in enumerate(json_pred):\n",
        "        raw_file = pred['raw_file']\n",
        "\n",
        "        pred_lanelines = pred['laneLines']\n",
        "        pred_centerlines = pred['centerLines']\n",
        "\n",
        "        if raw_file not in gts:\n",
        "            continue\n",
        "        gt = gts[raw_file]\n",
        "        gt_cam_height = gt['cam_height']\n",
        "        gt_cam_pitch = gt['cam_pitch']\n",
        "\n",
        "        fig = plt.figure()\n",
        "        ax1 = fig.add_subplot(231)\n",
        "        ax2 = fig.add_subplot(232)\n",
        "        ax3 = fig.add_subplot(233, projection='3d')\n",
        "        ax4 = fig.add_subplot(234)\n",
        "        ax5 = fig.add_subplot(235)\n",
        "        ax6 = fig.add_subplot(236, projection='3d')\n",
        "\n",
        "        # draw lanes\n",
        "        vs.visualize_lanes(pred_lanelines, raw_file, gt_cam_height, gt_cam_pitch, ax1, ax2, ax3)\n",
        "        vs.visualize_lanes(pred_centerlines, raw_file, gt_cam_height, gt_cam_pitch, ax4, ax5, ax6)\n",
        "        ax1.set_xticks([])\n",
        "        ax1.set_yticks([])\n",
        "        ax2.set_xticks([])\n",
        "        ax2.set_yticks([])\n",
        "        # ax2.set_xlabel('x axis')\n",
        "        # ax2.set_ylabel('y axis')\n",
        "        # ax2.set_zlabel('z axis')\n",
        "        bottom, top = ax3.get_zlim()\n",
        "        left, right = ax3.get_xlim()\n",
        "        ax3.set_zlim(min(bottom, -0.1), max(top, 0.1))\n",
        "        ax3.set_xlim(left, right)\n",
        "        ax3.set_ylim(min_y, max_y)\n",
        "        ax3.locator_params(nbins=5, axis='x')\n",
        "        ax3.locator_params(nbins=5, axis='z')\n",
        "        ax3.tick_params(pad=18)\n",
        "\n",
        "        ax4.set_xticks([])\n",
        "        ax4.set_yticks([])\n",
        "        ax5.set_xticks([])\n",
        "        ax5.set_yticks([])\n",
        "        # ax4.set_xlabel('x axis')\n",
        "        # ax4.set_ylabel('y axis')\n",
        "        # ax4.set_zlabel('z axis')\n",
        "        bottom, top = ax6.get_zlim()\n",
        "        left, right = ax6.get_xlim()\n",
        "        ax6.set_zlim(min(bottom, -0.1), max(top, 0.1))\n",
        "        ax6.set_xlim(left, right)\n",
        "        ax6.set_ylim(min_y, max_y)\n",
        "        ax6.locator_params(nbins=5, axis='x')\n",
        "        ax6.locator_params(nbins=5, axis='z')\n",
        "        ax6.tick_params(pad=18)\n",
        "\n",
        "        fig.subplots_adjust(wspace=0, hspace=0.01)\n",
        "        fig.savefig(ops.join(save_path, raw_file.replace(\"/\", \"_\")))\n",
        "        plt.close(fig)\n",
        "        print('processed sample: {}  {}'.format(i, raw_file))\n",
        "        '''"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nif __name__ == \\'__main__\\':\\n    parser = define_args()\\n    args = parser.parse_args()\\n\\n    # dataset_name: \\'standard\\' / \\'rare_subset\\' / \\'illus_chg\\'\\n    args.dataset_name = \\'illus_chg\\'\\n    args.dataset_dir = \\'/media/yuliangguo/DATA1/Datasets/Apollo_Sim_3D_Lane_Release/\\'\\n\\n    # model name: \\'Gen_LaneNet_ext\\' / \\'3D_LaneNet\\'\\n    model_name = \\'Gen_LaneNet_ext\\'\\n\\n    # load configuration for certain dataset\\n    sim3d_config(args)\\n    args.top_view_region = np.array([[-10, max_y], [10, max_y], [-10, 3], [10, 3]])\\n    vs = lane_visualizer(args)\\n\\n    pred_file = \\'../data_splits/\\' + args.dataset_name + \\'/\\' + model_name + \\'/test_pred_file.json\\'\\n    gt_file = \\'../data_splits/\\' + args.dataset_name + \\'/test.json\\'\\n\\n    save_path = pred_file[:pred_file.rfind(\\'/\\')]\\n    save_path += \\'/example/test_vis_pred\\'\\n    if not os.path.exists(save_path):\\n        try:\\n            os.makedirs(save_path)\\n        except OSError as e:\\n            print(e.message)\\n\\n    pred_lines = open(pred_file).readlines()\\n    json_pred = [json.loads(line) for line in pred_lines]\\n    # except BaseException as e:\\n    #     raise Exception(\\'Fail to load json file of the prediction.\\')\\n    json_gt = [json.loads(line) for line in open(gt_file).readlines()]\\n    if len(json_gt) != len(json_pred):\\n        raise Exception(\\'We do not get the predictions of all the test tasks\\')\\n    gts = {l[\\'raw_file\\']: l for l in json_gt}\\n\\n    for i, pred in enumerate(json_pred):\\n        raw_file = pred[\\'raw_file\\']\\n\\n        pred_lanelines = pred[\\'laneLines\\']\\n        pred_centerlines = pred[\\'centerLines\\']\\n\\n        if raw_file not in gts:\\n            continue\\n        gt = gts[raw_file]\\n        gt_cam_height = gt[\\'cam_height\\']\\n        gt_cam_pitch = gt[\\'cam_pitch\\']\\n\\n        fig = plt.figure()\\n        ax1 = fig.add_subplot(231)\\n        ax2 = fig.add_subplot(232)\\n        ax3 = fig.add_subplot(233, projection=\\'3d\\')\\n        ax4 = fig.add_subplot(234)\\n        ax5 = fig.add_subplot(235)\\n        ax6 = fig.add_subplot(236, projection=\\'3d\\')\\n\\n        # draw lanes\\n        vs.visualize_lanes(pred_lanelines, raw_file, gt_cam_height, gt_cam_pitch, ax1, ax2, ax3)\\n        vs.visualize_lanes(pred_centerlines, raw_file, gt_cam_height, gt_cam_pitch, ax4, ax5, ax6)\\n        ax1.set_xticks([])\\n        ax1.set_yticks([])\\n        ax2.set_xticks([])\\n        ax2.set_yticks([])\\n        # ax2.set_xlabel(\\'x axis\\')\\n        # ax2.set_ylabel(\\'y axis\\')\\n        # ax2.set_zlabel(\\'z axis\\')\\n        bottom, top = ax3.get_zlim()\\n        left, right = ax3.get_xlim()\\n        ax3.set_zlim(min(bottom, -0.1), max(top, 0.1))\\n        ax3.set_xlim(left, right)\\n        ax3.set_ylim(min_y, max_y)\\n        ax3.locator_params(nbins=5, axis=\\'x\\')\\n        ax3.locator_params(nbins=5, axis=\\'z\\')\\n        ax3.tick_params(pad=18)\\n\\n        ax4.set_xticks([])\\n        ax4.set_yticks([])\\n        ax5.set_xticks([])\\n        ax5.set_yticks([])\\n        # ax4.set_xlabel(\\'x axis\\')\\n        # ax4.set_ylabel(\\'y axis\\')\\n        # ax4.set_zlabel(\\'z axis\\')\\n        bottom, top = ax6.get_zlim()\\n        left, right = ax6.get_xlim()\\n        ax6.set_zlim(min(bottom, -0.1), max(top, 0.1))\\n        ax6.set_xlim(left, right)\\n        ax6.set_ylim(min_y, max_y)\\n        ax6.locator_params(nbins=5, axis=\\'x\\')\\n        ax6.locator_params(nbins=5, axis=\\'z\\')\\n        ax6.tick_params(pad=18)\\n\\n        fig.subplots_adjust(wspace=0, hspace=0.01)\\n        fig.savefig(ops.join(save_path, raw_file.replace(\"/\", \"_\")))\\n        plt.close(fig)\\n        print(\\'processed sample: {}  {}\\'.format(i, raw_file))\\n        '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErRKKfVL8d_z",
        "outputId": "9df6f0ae-b6e0-4297-dc54-fe02c2bd4632"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "A demo for Gen-LaneNet with new anchor extension. It predicts 3D lanes from a single image.\n",
        "Author: Yuliang Guo (33yuliangguo@gmail.com)\n",
        "Date: March, 2020\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import torchvision.transforms.functional as Q\n",
        "#from dataloader.Load_Data_3DLane_ext import *\n",
        "#from networks import GeoNet3D_ext, erfnet\n",
        "#from tools.utils import *\n",
        "#from tools.visualize_pred import lane_visualizer\n",
        "\n",
        "\n",
        "def unormalize_lane_anchor(anchor, num_y_steps, anchor_dim, x_off_std, z_std, num_types=3):\n",
        "    for i in range(num_types):\n",
        "        anchor[:, i*anchor_dim:i*anchor_dim + num_y_steps] = \\\n",
        "            np.multiply(anchor[:, i*anchor_dim: i*anchor_dim + num_y_steps], x_off_std)\n",
        "        anchor[:, i*anchor_dim + num_y_steps: i*anchor_dim + 2*num_y_steps] = \\\n",
        "            np.multiply(anchor[:, i*anchor_dim + num_y_steps: i*anchor_dim + 2*num_y_steps], z_std)\n",
        "\n",
        "\n",
        "def load_my_state_dict(model, state_dict):  # custom function to load model when not all dict elements\n",
        "    own_state = model.state_dict()\n",
        "    ckpt_name = []\n",
        "    cnt = 0\n",
        "    for name, param in state_dict.items():\n",
        "        if name[7:] not in list(own_state.keys()) or 'output_conv' in name:\n",
        "            ckpt_name.append(name)\n",
        "            # continue\n",
        "        own_state[name[7:]].copy_(param)\n",
        "        cnt += 1\n",
        "    print('#reused param: {}'.format(cnt))\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "    args = define_args()\n",
        "    #args = parser.parse_args()\n",
        "\n",
        "    # manual settings\n",
        "    image_file = '/content/drive/Shareddrives/colab/sample_image.jpg'\n",
        "    cam_file = '/content/drive/Shareddrives/colab/sample_image_cam.json'\n",
        "    args.mod = 'Gen_LaneNet_ext'  # model name\n",
        "    pretrained_feat_model = '/content/drive/Shareddrives/colab/erfnet_model_sim3d.tar'\n",
        "    trained_geo_model = '/content/drive/Shareddrives/colab/gen_lanenet_geo_model.tar'\n",
        "    anchor_std_file = '/content/drive/Shareddrives/colab/geo_anchor_std.json'\n",
        "\n",
        "    # load configuration for the model\n",
        "    sim3d_config(args)\n",
        "    args.y_ref = 5\n",
        "    args.batch_size = 1\n",
        "    anchor_y_steps = args.anchor_y_steps\n",
        "    num_y_steps = len(anchor_y_steps)\n",
        "    anchor_dim = 3 * num_y_steps + 1\n",
        "    x_min = args.top_view_region[0, 0]\n",
        "    x_max = args.top_view_region[1, 0]\n",
        "    anchor_x_steps = np.linspace(x_min, x_max, np.int(args.ipm_w / 8), endpoint=True)\n",
        "\n",
        "    # Check GPU availability\n",
        "    if not args.no_cuda and not torch.cuda.is_available():\n",
        "        raise Exception(\"No gpu available for usage\")\n",
        "    torch.backends.cudnn.benchmark = args.cudnn\n",
        "\n",
        "    # Define network\n",
        "    model_seg = ERFNet(2)  # 2-class model\n",
        "    model_geo = Net(args)\n",
        "    define_init_weights(model_geo, args.weight_init)\n",
        "\n",
        "    if not args.no_cuda:\n",
        "        # Load model on gpu before passing params to optimizer\n",
        "        model_seg = model_seg.cuda()\n",
        "        model_geo = model_geo.cuda()\n",
        "\n",
        "    # load segmentation model\n",
        "    checkpoint = torch.load(pretrained_feat_model)\n",
        "    model_seg = load_my_state_dict(model_seg, checkpoint['state_dict'])\n",
        "    model_seg.eval()  # do not back propagate to model1\n",
        "\n",
        "    # load geometry model\n",
        "    if os.path.isfile(trained_geo_model):\n",
        "        print(\"=> loading checkpoint '{}'\".format(trained_geo_model))\n",
        "        checkpoint = torch.load(trained_geo_model)\n",
        "        model_geo.load_state_dict(checkpoint['state_dict'])\n",
        "        model_geo.eval()\n",
        "    else:\n",
        "        print(\"=> no checkpoint found at '{}'\".format(trained_geo_model))\n",
        "\n",
        "    # load anchor std saved from training\n",
        "    with open(anchor_std_file) as f:\n",
        "        anchor_std = json.load(f)\n",
        "    x_off_std = np.array(anchor_std['x_off_std'])\n",
        "    z_std = np.array(anchor_std['z_std'])\n",
        "\n",
        "    #  load image\n",
        "    with open(image_file, 'rb') as f:\n",
        "        image = (Image.open(f).convert('RGB'))\n",
        "    # image preprocess\n",
        "    w, h = image.size\n",
        "    image = Q.crop(image, args.crop_y, 0, args.org_h - args.crop_y, w)\n",
        "    image = Q.resize(image, size=(args.resize_h, args.resize_w), interpolation=Image.BILINEAR)\n",
        "    image = transforms.ToTensor()(image).float()\n",
        "    image = transforms.Normalize(args.vgg_mean, args.vgg_std)(image)\n",
        "    image.unsqueeze_(0)\n",
        "    image = torch.cat(list(torch.split(image, 1, dim=0)) * args.batch_size)\n",
        "\n",
        "    if not args.no_cuda:\n",
        "        image = image.cuda()\n",
        "    # image = image.contiguous()\n",
        "    # image = torch.autograd.Variable(image)\n",
        "\n",
        "    # update camera setting os the model\n",
        "    with open(cam_file) as f:\n",
        "        cam_params = json.load(f)\n",
        "    gt_pitch = torch.tensor([cam_params['cameraPitch']], dtype=torch.float32)\n",
        "    gt_hcam = torch.tensor([cam_params['cameraHeight']], dtype=torch.float32)\n",
        "    model_geo.update_projection(args, gt_hcam, gt_pitch)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # deploy model\n",
        "        try:\n",
        "            output_seg = model_seg(image, no_lane_exist=True)\n",
        "            # output1 = F.softmax(output1, dim=1)\n",
        "            output_seg = output_seg.softmax(dim=1)\n",
        "            output_seg = output_seg / torch.max(torch.max(output_seg, dim=2, keepdim=True)[0], dim=3, keepdim=True)[0]\n",
        "            output_seg = output_seg[:, 1:, :, :]\n",
        "            output_geo, pred_hcam, pred_pitch = model_geo(output_seg)\n",
        "        except RuntimeError as e:\n",
        "            print(e)\n",
        "\n",
        "    output_geo = output_geo[0].data.cpu().numpy()\n",
        "\n",
        "    # unormalize lane outputs\n",
        "    unormalize_lane_anchor(output_geo, num_y_steps, anchor_dim, x_off_std, z_std, num_types=3)\n",
        "\n",
        "    # compute 3D lanes from network output, geometric transformation is involved\n",
        "    lanelines_pred, centerlines_pred, lanelines_prob, centerlines_prob = \\\n",
        "        compute_3d_lanes_all_prob(output_geo, anchor_dim, anchor_x_steps, anchor_y_steps, cam_params['cameraHeight'])\n",
        "\n",
        "    # visualize predicted lanes\n",
        "    # args.top_view_region = np.array([[-10, 80], [10, 80], [-10, 3], [10, 3]])\n",
        "    vs = lane_visualizer(args)\n",
        "    vs.dataset_dir = './'\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax1 = fig.add_subplot(231)\n",
        "    ax2 = fig.add_subplot(232)\n",
        "    ax3 = fig.add_subplot(233, projection='3d')\n",
        "    ax4 = fig.add_subplot(234)\n",
        "    ax5 = fig.add_subplot(235)\n",
        "    ax6 = fig.add_subplot(236, projection='3d')\n",
        "\n",
        "    # draw lanes\n",
        "    vs.visualize_lanes(lanelines_pred, image_file, cam_params['cameraHeight'], cam_params['cameraPitch'], ax1, ax2, ax3)\n",
        "    vs.visualize_lanes(centerlines_pred, image_file, cam_params['cameraHeight'], cam_params['cameraPitch'], ax4, ax5, ax6)\n",
        "    ax1.set_xticks([])\n",
        "    ax1.set_yticks([])\n",
        "    ax2.set_xticks([])\n",
        "    ax2.set_yticks([])\n",
        "    bottom, top = ax3.get_zlim()\n",
        "    left, right = ax3.get_xlim()\n",
        "    ax3.set_zlim(min(bottom, -0.1), max(top, 0.1))\n",
        "    ax3.set_xlim(left, right)\n",
        "    ax3.set_ylim(0, 80)\n",
        "    ax3.locator_params(nbins=5, axis='x')\n",
        "    ax3.locator_params(nbins=5, axis='z')\n",
        "    ax3.tick_params(pad=18)\n",
        "\n",
        "    ax4.set_xticks([])\n",
        "    ax4.set_yticks([])\n",
        "    ax5.set_xticks([])\n",
        "    ax5.set_yticks([])\n",
        "\n",
        "    bottom, top = ax6.get_zlim()\n",
        "    left, right = ax6.get_xlim()\n",
        "    ax6.set_zlim(min(bottom, -0.1), max(top, 0.1))\n",
        "    ax6.set_xlim(left, right)\n",
        "    ax6.set_ylim(0, 80)\n",
        "    ax6.locator_params(nbins=5, axis='x')\n",
        "    ax6.locator_params(nbins=5, axis='z')\n",
        "    ax6.tick_params(pad=18)\n",
        "\n",
        "    fig.subplots_adjust(wspace=0, hspace=0.01)\n",
        "    fig.savefig('test.png')\n",
        "    plt.close(fig)\n",
        "    print(\"done!\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Init weights in network with [normal]\n",
            "#reused param: 357\n",
            "=> loading checkpoint '/content/drive/Shareddrives/colab/gen_lanenet_geo_model.tar'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:387: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3982: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "done!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}